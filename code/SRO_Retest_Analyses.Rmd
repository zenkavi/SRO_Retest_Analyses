---
title: 'Self Regulation Ontology Retest Analyses'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/workspace_scripts/SRO_Retest_Analyses_Workspace.R')
```

In total `r sum(workers$Retest_worker)` participants were invited, `r nrow(worker_counts)` began the battery, `r sum(worker_counts$task_count >= 62)` completed the battery and 150 provided data that passed qc for both time points. Our target sample size was determined in advance of data collection and data collection continued until this number of participants with data that survived qc was reached.

Data collection took place on average `r round(mean(as.numeric(comp_dates$days_btw)))` number of days after the completion of the initial battery with a range of `r round(range(as.numeric(comp_dates$days_btw)))[1]` to `r round(range(as.numeric(comp_dates$days_btw)))[2]` days.

The variables included in this report are:  
- meaningful variables (includes only hdddm parameters)  
- EZ diffusion parameters  
- Raw RT and Accuracy measures  
- Variables found in the literature (for comparison)  

For reference here are the variables that are **not** included in the analyses of the remainder of this report because they were not of theoretical interest in factor structure analyses of this data. These include drift diffusion and other model parameters for specific conditions within a task; survey variables that are not part of the dependant variables for that survey in the literature and demographics (these are saved for prediction analyses).

```{r}
test_data2 <- read.csv(paste0(retest_data_path, 't1_data/variables_exhaustive.csv'))

df <- data.frame(names(test_data2)[which(names(test_data2) %in% names(test_data) == FALSE)])
names(df) = c('vars')

df %>%
  filter(vars != "X")
```

```{r}
rm(test_data2, df)
```

# Results

## Demographics

```{r}
summary(test_demog %>%
          select(Sex, Age))

summary(retest_demog %>%
          select(Sex, Age))
```

## Literature review

Here's what our literature review looks like

```{r}
lit_review
```

```{r message=FALSE}
tmp = lit_review[duplicated(lit_review$reference)==FALSE,]
```

The literature review included `r nrow(tmp)` papers, `r sum(tmp$sample_size)` participants and `r nrow(lit_review)` data points on reliability.

```{r}
rm(tmp)
```

```{r echo=FALSE, out.width='100%'}
fig_name = 'Lit_Review_Plot_t.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

An interactive version of this plot could be find [here](https://zenkavi.github.io/SRO_Retest_Analyses/output/reports/Interactive_Figures.html)

Takeaways from this review are:   
- Survey measures have been reported to higher reliability compared to task measures  
- Survey measures have less variability in the reported reliabiltiy estimates compared to task measures   

## Data quality checks

### Demographics reliability

Point estimates of reliability for the demographic variabels.

```{r}
demog_boot_df %>%
  group_by(dv) %>%
  summarise(median_icc = quantile(icc, probs=0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975)) %>%
  arrange(-median_icc)
```

Point estimates of reliability for demog items

```{r}
demog_rel_df = make_rel_df(test_demog, t2_df = retest_demog, metrics=c('icc'))

demog_rel_df %>%
  select(dv, icc) %>%
  arrange(-icc)
```

Plotting point estimates of reliability for demographic items coloring those that have a time limit darker in the histogram.

```{r warning=FALSE, message=FALSE}
timed_demogs <- c("DoctorVisitsLastMonth","DaysHalfLastMonth","Worthless","DaysPhysicalHealthFeelings","Depressed","EverythingIsEffort","Hopeless","RestlessFidgety","Nervous","DaysLostLastMonth","Last30DaysUsual")

demog_rel_df %>%
  mutate(timed = ifelse(dv %in% timed_demogs, 1, 0)) %>%
  ggplot(aes(icc, fill = factor(timed)))+
  geom_histogram(position = "identity", alpha = 0.7, color = NA)+
  xlab("ICC")+
  ylab("Count")+
  scale_fill_manual(values = c("grey", "grey25"))+
  theme(legend.position = "none")
```

```{r echo=FALSE}
rm(test_demog, retest_demog, demog_rel_df, timed_demogs)
```

### Effect of delays between the two measurements

Instead of the summary metric like the retest reliability estimate for each measure we can check whether the difference score distribution for each measure depends on the delay between the two measurements. Since the difference score distribution is at subject level we can check whether the order of subjects in this distribution depends on their order in the distribution of days between completing the two tests.  

To check this we make data frame with difference between two scores for each measure for each subject. Since the scores for different measures are on different scales for comparability the difference scores are normalized (ie demeaned and dividied by the sd of the difference score distribution.) Note that the variance of the difference score distribution accounts for the variance in both time points by summing them. Normalization equates the means of each difference score distribution to 0 which would mask any meaningful change between the two time points but the analysis here does not interpret the mean of the difference score distributions but is interested in its relation to the days between completion. We check if the variables show systematic differences between the two points later.  

Here we check if the difference is larger the longer the delay.  

```{r warning=FALSE}
numeric_cols = get_numeric_cols(test_data)

t1_2_difference = data.frame()

for(i in 1:length(numeric_cols)){
  tmp = match_t1_t2(numeric_cols[i],format='wide')
  tmp = tmp %>%
  mutate(difference = scale(`2` - `1`))
  t1_2_difference = rbind(t1_2_difference, tmp)
}

t1_2_difference$difference = as.data.frame(t1_2_difference$difference)$V1

t1_2_difference = t1_2_difference %>% separate(dv, c("task", "dv2"), sep="\\.", remove=FALSE)
```

```{r echo=FALSE}
rm(tmp, i, numeric_cols)
```

Add completion dates to this data frame.

```{r warning=FALSE}
t1_2_difference = merge(t1_2_difference, task_comp_times[,c('sub_id', 'task','days_btw')], by=c('sub_id', 'task'))
```

```{r echo=FALSE}
rm(task_comp_times)
```

What does the distribution of differences look like: The distribution of differences between two time points for each measure

```{r warning=FALSE, message=FALSE}
t1_2_difference %>%
  ggplot(aes(difference, alpha=dv))+
  geom_histogram(position='identity')+
  theme(legend.position = 'none')
```

How do the difference score distributions look like with respect to the days between completion?

```{r}
t1_2_difference %>%
  ggplot()+
  geom_smooth(aes(as.numeric(days_btw), abs(difference), group=factor(dv)), method='lm', se=FALSE, color = 'grey', alpha = 0.5)+
  geom_smooth(aes(as.numeric(days_btw), abs(difference)), method='lm', color = "black", se=FALSE)+
  theme(legend.title = element_blank())+
  xlab('Days between completion')+
  ylab('Scaled difference score')
```

To test if the slope of the black is significant we would run a mixed effects model with a fixed effect for days between completion, random slope for each dv depending on the days between and random intercept for each dv.  

Before I was using subjects as a random effect but days between the two time points for each measure depends on subj id. What varies randomly is which dv we are looking for its distribution of differences in relation to the days between the time points. So I changed the model to have fixed effect for the days between, a random slope for (dependent variables can be differentially sensitive to th effect of days between) and a random intercept for dependent variable.  

Significant fixed effect suggests that on average the longer the delay the smaller the difference.  

```{r}
summary(lmerTest::lmer(abs(difference) ~ scale(days_btw)+(scale(days_btw) | dv), data=t1_2_difference))

# summary(tmp <- MCMCglmm(abs(difference) ~ scale(days_btw), random = ~us(scale(days_btw)):dv, data=t1_2_difference, verbose=FALSE))
```

But if I just run one mixed effects model then we don't get a sense of the simple effects (how many of the variables this effect of the days between is significant and in which direction). I can run it separately for each dv to see if all difference score distributions are affected the same way depending on the days between completion. But since we are running so many tests we need to correct for multiple comparisons. How many of these tests are significant FDR correcting? None.

```{r}
get_delay_effect = function(df){
  mod = lm(abs(difference) ~ scale(days_btw), data = df)
  out = data.frame(estimate=coef(summary(mod))["scale(days_btw)","Estimate"], pval=coef(summary(mod))["scale(days_btw)","Pr(>|t|)"])
  return(out)
}

days_effect  = t1_2_difference %>%
  group_by(dv) %>%
  do(get_delay_effect(.))

#Correct p-values for multiple comparisons
sum(p.adjust(days_effect$pval, method = "fdr") < 0.05)
```

Conclusion: The average difference between the scores of a measure doesn't change with the increased delay.

### Overlapping survey questions

Some surveys have overlapping questions. Do these correlate within and across sessions?

First determine the overlapping questions.

```{r}
duplicate_items
```

```{r warning=FALSE, message=FALSE}
#surveys to read in
extract_items = c('worker',unique(with(duplicate_items, c(item1_ID, item2_ID))))

#correlations to compute:
#item1_t1 - item2_t1,
#item1_t2 - item2_t2,
#item1_t1 - item2_t2,
#item1_t2 - item2_t1

duplicate_items_data_t1 = duplicate_items_data_t1 %>%
  filter(worker %in% duplicate_items_data_t2$worker) %>%
  select(extract_items)

duplicate_items_data_t2=duplicate_items_data_t2 %>%
  filter(worker %in% duplicate_items_data_t1$worker) %>%
  select(extract_items)

duplicate_items = duplicate_items %>%
  mutate(t1_t1_cor = NA,
         t2_t2_cor = NA,
         t1_t2_cor = NA,
         t2_t1_cor = NA,
         t1_t1_polycor = NA,
         t2_t2_polycor = NA,
         t1_t2_polycor = NA,
         t2_t1_polycor = NA)

for(i in 1:nrow(duplicate_items)){
  duplicate_items$t1_t1_cor[i] = abs(cor(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))

  duplicate_items$t2_t2_cor[i] = abs(cor(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))

  duplicate_items$t1_t2_cor[i] = abs(cor(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))

  duplicate_items$t2_t1_cor[i] = abs(cor(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))

  duplicate_items$t1_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t1_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])
}

for(i in 1:nrow(duplicate_items)){
  duplicate_items$t1_t1_cor[i] = abs(cor(scale(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])])))

  duplicate_items$t2_t2_cor[i] = abs(cor(scale(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])])))

  duplicate_items$t1_t2_cor[i] = abs(cor(scale(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])])))

  duplicate_items$t2_t1_cor[i] = abs(cor(scale(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])])))

  duplicate_items$t1_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t1_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])
}

duplicate_items %>%
  arrange(-t1_t1_polycor, -t2_t2_polycor)
```

```{r}
# summary(duplicate_items$t1_t1_cor)
# summary(duplicate_items$t2_t2_cor)
# summary(duplicate_items$t1_t2_cor)
# summary(duplicate_items$t2_t1_cor)

summary(duplicate_items$t1_t1_polycor)
summary(duplicate_items$t2_t2_polycor)
# summary(duplicate_items$t1_t2_polycor)
# summary(duplicate_items$t2_t1_polycor)
```

```{r}
tmp = duplicate_items %>%
  select(similarity, t1_t1_polycor, t2_t2_polycor) %>%
  gather(key, value, -similarity)

tmp %>%
  ggplot(aes(similarity, value, col=key))+
  geom_smooth(method="lm", alpha = 0.15)+
  ylab("Polychoric correlation")+
  xlab("Levenshtein distance")+
  scale_color_discrete(breaks = c("t1_t1_polycor", "t2_t2_polycor"),
                       labels = c("T1 correlations", "T2 correlations"),
                       name = element_blank())+
  theme_bw()
```


```{r}
summary(lm(value~key*scale(similarity),tmp))
```

```{r eval=FALSE, echo=FALSE}
summary(MCMCglmm(value ~ key*scale(similarity), data=tmp, nitt = 50000))
```

```{r echo=FALSE}
rm(tmp, comb, t1_2_difference, duplicate_items, duplicate_items_data_t1, duplicate_items_data_t2, extract_items)
```

## Comparison to prior literature

Summarize bootstrapped results and merge to lit review data

Here's what our data looks like: (583 data points for 171 measures)

```{r}
rel_comp
```

### Lit review results

Distribution of reliabilities, sample sizes and delays in the literature

```{r}
rel_comp %>%
  select(dv, task, retest_reliability, sample_size, days) %>%
  filter(days < 3600) %>%
  gather(key, value, -dv, -task) %>%
  ggplot(aes(value, fill=task))+
  geom_density(alpha=0.5, position='identity')+
  facet_wrap(~key, scales='free')+
  theme(legend.title = element_blank())
```

```{r}
summary(rel_comp$sample_size[rel_comp$task == "survey"])
summary(rel_comp$sample_size[rel_comp$task == "task"])

rel_comp %>%
  group_by(task) %>%
  summarise(mean_sample_size = mean(sample_size),
            sem_sample_size = sem(sample_size)) %>%
  ggplot(aes(task,mean_sample_size,col=task))+
  geom_point(size=5)+
  geom_errorbar(aes(ymin=mean_sample_size-sem_sample_size, ymax=mean_sample_size+sem_sample_size), width = 0, size=2)+
  xlab("")+
  ylab("Sample size")+
  theme(legend.position = 'none',
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))
```

The literature has smaller sized samples for task measures compared to survey measures that report retest reliability.

```{r}
# summary(lm(sample_size ~ task,rel_comp))
summary(m1 <- MCMCglmm(sample_size ~ task, data=rel_comp, verbose=FALSE))
```

What predicts retest reliability in the literature?
Task, sample size, days

```{r}
mod1 = lmer(retest_reliability ~ task + (1|dv), rel_comp)
mod2 = lmer(retest_reliability ~ task + sample_size + (1|dv), rel_comp)

anova(mod1, mod2)
```

```{r}
mod3 = lmer(retest_reliability ~ task + sample_size + days+ (1|dv), rel_comp)

anova(mod2, mod3)
```

```{r}
#summary(mod2)
summary(MCMCglmm(retest_reliability ~ task + sample_size, random = ~ dv, data = rel_comp, verbose=FALSE))
```

```{r}
rel_comp %>%
  group_by(task) %>%
  summarise(mean_rr = mean(retest_reliability))
```

Are the residuals of this model heteroscedastic? Yes the residuals do not depend on the predictor.

```{r}
tmp = data.frame(rel_comp$sample_size, resid(mod2)) %>%
  rename(sample_size = rel_comp.sample_size, resid = resid.mod2.)

tmp %>%
  ggplot(aes(sample_size, resid))+
  geom_point()+
  geom_smooth(method = "lm")
```


```{r}
summary(lm(resid ~ sample_size, tmp))
```

Tasks have significantly lower reliability and reliability decreases with increasing sample size.

```{r}
rel_comp %>%
  ggplot(aes(sample_size, retest_reliability, color=task))+
  geom_smooth(method='lm')+
  # geom_point(alpha = 0.2)+
  theme(legend.title = element_blank(),
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14))+
  xlab("Sample Size")+
  ylab("Retest reliability")+
  ylim(-0.15, 1)
```

Highlighting the difference between survey and task reliability for comparability to our results.

```{r}
rel_comp %>%
  ggplot(aes(task, retest_reliability, fill = task))+
  geom_boxplot()+
  theme(legend.position = 'none',
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))+
  xlab("")+
  ylab("Mean ICC")+
  ylim(-0.4,1)
```

Note: We checked whether our results diverge most from studies with smaller sample sizes. Square difference between our mean estimate and the reliability from the literature decreases exponentially with sample size. The smaller the sample size in the literature the more the reliability estimate differs from our results. But this was a weak result because most of the studies in the literature have smaller sample sizes and you see both small and large deviations for these studies (these were not significant either).

### Correlation with our results

Correlation between our mean estimates from bootstrapped samples and the literature review for task variables

```{r eval=FALSE}
n_df = rel_comp %>%
  group_by(dv) %>%
  tally()

lit_emp_cor = function(){

  boot_comp = data.frame()

  for(i in 1:length(unique(rel_comp$dv)) ){
    cur_dv = unique(rel_comp$dv)[i]
    n = n_df$n[n_df$dv == cur_dv]
    sample_df = boot_df %>% filter(dv == cur_dv)
    tmp = sample_n(sample_df, n)
    boot_comp = rbind(boot_comp, tmp)
  }  

  rm(cur_dv, n, sample_df, tmp)

  #check if cbind is ok
  # sum(boot_comp$dv == rel_comp$dv)
  #cbinding pearson because that is the most common metric in the lit
  rel_comp = cbind(rel_comp, boot_comp$pearson)
  #rename new column
  names(rel_comp)[which(names(rel_comp) == "boot_comp$pearson")] = "pearson"

  out = data.frame(task = NA, survey = NA)

  out$task = with(rel_comp %>% filter(task == "task"), cor(pearson, retest_reliability))

  out$survey = with(rel_comp %>% filter(task == "survey"), cor(pearson, retest_reliability))

  rel_comp = rel_comp[,-16]

  return(out)
}

lit_emp_cor_out = plyr::rdply(100, lit_emp_cor)

write.csv(lit_emp_cor_out,'/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/lit_emp_cor_out.csv')
```


```{r echo=FALSE}
lit_emp_cor_out = read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/lit_emp_cor_out.csv')

summary(lit_emp_cor_out)
```

### Noise ceiling

Model comparisons building model to predict the reliabilities in the literature from a sample from our results versus a sample form the literature.

sample one row per measure out of lit review
r^2 of retest_reliability ~ sampled_reliability vs.
r^2 of retest_reliability ~ mean_icc

coef of retest_reliability ~ sampled_reliability vs.
coef of retest_reliability ~ mean_icc

```{r eval=FALSE}
comp_lit_pred <- function(df){

  sample_from_dv <- function(df){
    if(nrow(df)>1){
      row_num = sample(1:nrow(df),1)
      sample_row = df[row_num,]
      df = df[-row_num,]
      df$lit_predictor = sample_row$retest_reliability
    }
    return(df)
  }

  sampled_df = df %>%
    group_by(dv) %>%
    do(sample_from_dv(.)) %>%
    na.omit()

  if(length(unique(sampled_df$task))>1){
    mod_lit = lm(retest_reliability ~ lit_predictor+scale(sample_size)+task, data=sampled_df)
    mod_boot = lm(retest_reliability ~ mean_pearson+scale(sample_size)+task, data=sampled_df)
  }
  else{
    mod_lit = lm(retest_reliability ~ lit_predictor+scale(sample_size), data=sampled_df)
    mod_boot = lm(retest_reliability ~ mean_pearson+scale(sample_size), data=sampled_df)
  }


  out = data.frame(r2_lit = summary(mod_lit)$adj.r.squared,
                   r2_boot = summary(mod_boot)$adj.r.squared,
                   m_lit = coef(summary(mod_lit))["lit_predictor","Estimate"],
                   m_boot = coef(summary(mod_boot))["mean_pearson","Estimate"])

  return(out)
}

comp_lit_pred_out = plyr::rdply(1000, comp_lit_pred(rel_comp))

write.csv(comp_lit_pred_out,'/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/comp_lit_pred_out.csv')
```


```{r echo=FALSE}
comp_lit_pred_out = read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/comp_lit_pred_out.csv')
```

```{r echo=FALSE, out.width='100%'}
fig_name = 'LitAndBoot_Noise_Ceiling.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

```{r}
with(comp_lit_pred_out, t.test(r2_lit, r2_boot, paired=T))
```

```{r}
tmp = comp_lit_pred_out %>%
  select(r2_lit, r2_boot) %>%
  gather(key, value)

summary(MCMCglmm(value ~ key, data = tmp, verbose=F))
```

We checked if survey and task measures differed in how close our results were to the literature (i.e. Are you better in estimating the literature using our data for surveys vs tasks?). We found an interaction, such that: The literature does a better job in predicting itself for tasks compared to surveys. On the other hand, our data is better in predicting the literature estimates for the surveys than it is in predicting tasks.  

One might wonder: Why are the reliability estimates from surveys worse compared to tasks in predicting the literature? Because they are less variable. In this procedure having high variance in what is being predicted (i.e. the literature reliability estimates) is better.  

We also conducted LOOCV on this data and checked what it looked like if we estimate the left one out using the remaining ones for the noise ceiling model? The procedure followed these steps:
- Sample one row of literature,   
- fit model of sample size+task on remaining data,   
- use that model to predict the reliability estimates,   
- compare those to the left out reliability estimate from the literature versus the average reliability estimate from our data  

We found that the (squared) difference is slightly larger when predicting the left-out value from our data versus predicting left out value from literature. The squared differences were larger for predicting the literature compared to predicting our data for surveys while for tasks the squared differences were larger when predicting our data compared to predicting the literature. (This fits with other results too: our results differ from the literature more for the survey measures).  

We do not present these results in further detail as they yield only converging evidence as the analyses above.

### Measure level comparison

Literature vs. our data on a  measure basis

```{r echo=FALSE, out.width='100%'}
fig_name = 'LitVsEmp_Measure_Plot.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

```{r}
rel_comp %>%
  group_by(dv, task) %>%
  summarise(mean_lit = mean(retest_reliability),
            mean_emp = unique(mean_pearson)) %>%
  ggplot(aes(mean_emp, mean_lit, col=task, shape=task))+
  geom_smooth(method="lm")+
  geom_point()+
  xlim(-0.25, 1)+
  ylim(-0.25, 1)+
  ylab("Average Literature Reliability Estimate")+
  xlab("Average Empirical Reliability Estimate")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
        plot.caption = element_text(family = "Times", lineheight = 2, hjust = 0))+
  labs(caption = str_wrap("FIGURE 2: Correlation between mean reliability estimates for each measure found in the literature with the mean reliability from our data.", width = 120))
```

```{r echo=FALSE}
rm(tmp, rel_comp, mod1, mod2, mod3, i, lit_emp_cor_out, comp_lit_pred, comp_lit_pred_out)
```

## Relationship between reliability metrics (point estimates)

Though we are primarily reporting ICC's as our metric of reliability the results don't change depending on the metric chosen. Here we plot point estimates of three different reliability metrics against each other (ICC, Pearson, Spearman). The bootstrapped version is essentially the same but the plots are busier due to more datapoints.

```{r}
rel_df = make_rel_df(t1_df = test_data, t2_df = retest_data, metrics = c('spearman', 'icc', 'pearson', 'var_breakdown', 'partial_eta', 'sem'))

rel_df$task = 'task'
rel_df[grep('survey', rel_df$dv), 'task'] = 'survey'
rel_df[grep('holt', rel_df$dv), 'task'] = "task"
rel_df = rel_df %>%
  select(dv, task, spearman, icc, pearson, partial_eta, sem, var_subs, var_ind, var_resid)
```

We calculated `r as.numeric(table(rel_df$task)[1])` measures for surveys and `r as.numeric(table(rel_df$task)[2])` measures for cognitive tasks.  

```{r echo=FALSE, out.width='100%'}
fig_name = 'Metric_Scatterplots.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

As the scatter plots depict the correlations between different types reliability metrics were very high.

```{r}
cor(rel_df[,c('spearman', 'icc', 'pearson')])
```

Note: Some variables have <0 ICC's. This would be the case if the $MS_{error}$>$MS_{between}$. Data for these variables have no relationship between the two time points.

## Summary of all measure reliabilities

Summarized bootstrapped reliabilities

```{r message=FALSE, warning=FALSE}
boot_df %>%
  group_by(dv) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            spearman_median = quantile(spearman, probs = 0.5),
            spearman_2.5 = quantile(spearman, probs = 0.025),
            spearman_97.5 = quantile(spearman, probs = 0.975)) %>%
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5', 'spearman_median', 'spearman_2.5', 'spearman_97.5'), digits=3)
```


```{r echo=FALSE, out.width='100%'}
fig_name = 'Boot_Both_w_trialinfo.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

Variable level summary of bootstrapped reliabilities.

```{r echo=FALSE, out.width='100%'}
fig_name = 'Bootstrap_Raw_Var_Plot.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

Example of the overlaying procedure.

```{r echo=FALSE, out.width='100%'}
fig_name = 'Bootstrap_Example_Plot_t.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

## Survey vs Tasks

Comparison of survey measures to cognitive task measures in the bootstrapped results. Multilevel model with random intercepts for each measure and fixed effect of survey versus cognitive measure.

```{r}
boot_df = boot_df %>%
    mutate(task = ifelse(grepl("survey",dv), "survey","task"),
           task = ifelse(grepl("holt",dv), "task", task))

boot_df %>%
  group_by(task) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            spearman_median = quantile(spearman, probs = 0.5),
            spearman_2.5 = quantile(spearman, probs = 0.025),
            spearman_97.5 = quantile(spearman, probs = 0.975),
            num_vars = n()/1000) %>%
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5', 'spearman_median', 'spearman_2.5', 'spearman_97.5'), digits=3)
```


```{r eval=FALSE}
summary(icc_by_task_model)
```

```{r echo=FALSE}
rm(icc_by_task_model)
```

### Variance breakdown

The quantiative explanation for the difference in reliability estimates between surveys and tasks, as recently detailed by Hedge et al. (2017), lies in the difference in sources of variance between these measures. Specifically, the ICC is calculated as the ratio of variance between subjects variance to all sources of variance. Thus, measures with high between subjects variance would have high test-retest reliability. Intuitively, measures with high between subjects variance are also better suited for individual difference analyses as they would capture the differences between the subjects in a sample.

Here we first plot the percentage of variance explained by the three sources of variance for the point estimates of measure reliabilities. The plot only includes raw measures (no DDM parameters) and the measures are ranked by percentage of between subject variability for each task/survey (i.e. the best to worst individual difference measure for each task/survey). Then we compare statistically whether the percentage of variance explained by these sources differ between tasks and surveys.

```{r echo=FALSE, out.width='100%'}
fig_name = 'Variance_Breakdown_Plot.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

Comparing types of variance for survey vs task measures: Survey measures have higher between subject variability  

Note: This analysis includes DDM variables too.

Running separate models for different sources of variance because interactive model with variance type*task seemed too complicated.

First we find that task measures have a smaller percentage of their overall variance explained by variability between subjects compared to survey measures.

```{r}
tmp = boot_df %>%
  select(dv, task, var_subs_pct, var_ind_pct, var_resid_pct)

summary(var_subs_pct_by_task_model)
```

We also find that a significantly larger percentage of their variance is explained by between session variability. Larger between session variability suggests systematic differences between the two sessions. Such systematic effects can be due to e.g. learning effects as explored later.

```{r}
summary(var_ind_pct_by_task_model)
```

```{r}
summary(var_resid_pct_by_task_model)
```

```{r}
rm(var_subs_pct_by_task_model, var_ind_pct_by_task_model, var_resid_pct_by_task_model)
```

```{r}
tmp_save = tmp%>%
  gather(key, value, -dv, -task) %>%
  group_by(task, key) %>%
  summarise(median = median(value),
            sd = sd(value)) %>%
  mutate(key = ifelse(key == 'var_ind_pct', 'Between session variance', ifelse(key == 'var_subs_pct', 'Between subjects variance', ifelse(key == 'var_resid_pct', 'Residual variance',NA)))) %>%
  rename(Median = median, SD = sd) %>%
  arrange(task, key)

tmp_save
```

Summarizing for clearer presentation. This graph is currently using the bootstrapped reliabilities and is therefore messier than if just using the point estimates.

```{r echo=FALSE, out.width='100%'}
fig_name = 'Variance_Breakdown_DotPlot.jpeg'

knitr::include_graphics(paste0(fig_path, fig_name))
```

## Systematic effects between time points

### Anova time effects

The type of ICC we have chosen does not take within subject (between session/systematic) variance in to account. This is why Weir recommends checking whether there is a significant change based on time and examining the SEMs. These systematic effects could be meaningful and important to account for for some measures (e.g. task measures that show learning effects).

Had we chosen another kind of ICC taking this source of variance into account (e.g. 2,1 or 2,k) they could have suggested that tasks have lower reliability.

Doing a simple t-test on the difference score alone would not be a very rigourous way of testing whether any change is meaningful because two distributions from both time points with error would be compared to each other. Fortunately there are ways to take the error for both measurements in to account.

To check whether a measure shows systematic differences between the two time points in a meaningful number of bootstrapped samples we can: check if the effect of time is significant in each bootstrapped sample and filter variables that have more than 5% of the boostrapped samples showing significant time effects.

Another way might be to compute confidence intervals using SEMs as described in the second half of Weir (2005) and check what percent of participants have scores that fall out of this range.

First we ask: Which variables have significant time effects in more than 5% of the bootstrapped samples?

23/74 survey measures
133/372 task measures

```{r}
boot_df %>%
  select(dv, p_time, task) %>%
  mutate(time_effect_sig = ifelse(p_time<0.05,1,0)) %>%
  group_by(dv)%>%
  summarise(pct_sig_time_effect = sum(time_effect_sig)/10,
            task = unique(task))%>%
  filter(pct_sig_time_effect>5) %>%
  arrange(task,-pct_sig_time_effect) %>%
  ungroup()%>%
  group_by(task) %>%
  summarise(count=n()) %>%
  mutate(total_num_vars = c(74, 372),
         prop = count/total_num_vars) %>%
  ggplot(aes(task, prop, fill=task))+
  geom_bar(stat="identity", alpha=0.5, width=0.5)+
  theme(legend.position = "none",
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))+
  ylab("Proportiono of bootstrap samples")+
  xlab("")+
  ylim(0,1)

ggsave('PropTimeEffects.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 3, height = 5, units = "in", limitsize = FALSE, dpi = 300)

```

### SEM CI calculation

Step 1: T = Grand mean + ICC * (Subject score - Grand mean)
Step 2: SEP = SD of both measurements * sqrt(1-ICC^2)

Here I calculate the proportion of subjects that move more than one standard error of prediction (SEP) in t2 compared to t1 for each measure.

This is odd to think about because the larger the ICC of a measure the smaller the SEP. So very small differences between the two time points can be categorized as 'meaningful' based on the tiny SEP.

What you are interested in is not necessarily whether individuals change at all between the two time points though. You want to know if this change is systematic in one direction.

Here I calculate the proportion of people showing 'meaningful' changes in one direction or the other. To integrate both of these direction I subtracted one from the other and filtered the variables that have more than 5% of the participants showing meaningful change in one direction over the other (so if a variable has 10 participants showing difference in one and 10 in the other direction this would cancel out but a variables with 15 people showing a positive and 5 negative change would remain).


```{r}
get_ind_ci = function(dv_var){
  matched = match_t1_t2(dv_var)
  grand_mean = mean(matched$score)
  grand_sd = sd(matched$score)
  dv_icc = get_icc(dv_var)
  sep = grand_sd * sqrt(1-(dv_icc^2))
  matched = matched %>%
    spread(time, score) %>%
    rename("t1"="1", "t2"="2") %>%
    mutate(true_score = grand_mean+(dv_icc*(t1-grand_mean)),
           ci_up = true_score+(1.96*sep),
           ci_low = true_score-(1.96*sep),
           t2_above_ci = ifelse(t2>ci_up,1,0),
           t2_below_ci = ifelse(t2<ci_low,1, 0))
    return(matched)
}

get_prop_out_ci = function(dv_var){
  get_ind_ci(dv_var) %>%
    summarise(prop_above_ci = sum(t2_above_ci)/n(),
              prop_below_ci = sum(t2_below_ci/n()))
}

#Create df of point estimate reliabilities
ind_ci_df <- data.frame(prop_above_ci = rep(NA, length(numeric_cols)),
                        prop_below_ci = rep(NA, length(numeric_cols)))

row.names(ind_ci_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
    ind_ci_df[numeric_cols[i], 'prop_above_ci'] <- get_prop_out_ci(numeric_cols[i])$prop_above_ci
    ind_ci_df[numeric_cols[i], 'prop_below_ci'] <- get_prop_out_ci(numeric_cols[i])$prop_below_ci
}

ind_ci_df$dv = row.names(ind_ci_df)
row.names(ind_ci_df) = seq(1:nrow(ind_ci_df))
ind_ci_df$task = 'task'
ind_ci_df[grep('survey', ind_ci_df$dv), 'task'] = 'survey'
ind_ci_df[grep('holt', ind_ci_df$dv), 'task'] = "task"
ind_ci_df = ind_ci_df %>%
  select(dv, task, prop_above_ci, prop_below_ci)

mean_diff_df = ind_ci_df %>%
  mutate(prop_one_direction = prop_above_ci-prop_below_ci) %>%
  filter(abs(prop_one_direction)>0.05) %>%
  arrange(-abs(prop_one_direction))

mean_diff_df
```

This doesn't make it easier to interpret whether it is a performance improvement or decline. In fact 'performance' isn't even necessarily the correct term here.

## Task Reliabilities

Here we summarize the results on a task level to make it more digestable and easier to make contact with the literature.  

We reduce the list of task measures to a list of one per task by averaging only the measures that are extracted and used from these tasks in the literature. We call these the 'meaningful measures.'

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2) %>%
  group_by(task_name) %>%
  summarise(median_icc = median(icc),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_measures = n()/1000,
            mean_num_trials = round(mean(num_all_trials)))%>%
  arrange(-median_icc)

tmp %>%
  datatable() %>%
  formatRound(columns=c('median_icc','icc_2.5','icc_97.5'), digits=3)
```

### Number of trials

Does number of items in a task have a significant effect on the average ICC of meaningful measures for all trials from a task? No.

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2)

summary(icc_by_num_trials_model)
```

```{r warning=FALSE, message=FALSE}
tmp %>%
  ggplot(aes(num_all_trials, icc))+
  geom_point()+
  geom_smooth(method="lm")+
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")
```

#### Trial number dependence intrameasure

The above analysis was looking at the effect of number of trials across tasks. But some tasks might be bad for individual difference measurement regardless of how many trials there are in them whereas for others fewer trials might be yielding a sufficiently reliable measure.

For tasks for which dependent variables are estimated using many trials one can ask: Does the same measure get less reliable if fewer trials are used to estimate its reliability?

This won't make sense for all tasks. For example to estimate a risk aversion parameter you need all trials for Holt and Laury. For Kirby and Bickel you have specific conditions looking at fewer trials. The Cognitive Reflection Task might be more appropriate to analyze each item seaprately. The writing task does not have trial numbers. For all others it might be interesting to investigate.

These kinds of analyses are too task-specific and in-depth for a paper that is trying to give a global sense of the differences between self-regulation measures in their suitablity for individual difference analyses based on their stability across time. Such analyses would provide a detailed examination of how to extract the most reliable/best individual difference measure from tasks with a set of mediocre variables to begin with. Though we do not provide such a comprehensive analysis of this sort in this paper we provide a single example of this approach and hope the open access we provide to the data spurs further work.

For this example we look at the retest reliability of dependent measures from the threebytwo with >400 trials. Here is a graph of how the point estimates of the retest reliability changes for each of the dependent measures using different numbers of trials to estimate them.

Post process dv's from cluster to calculate reliabilities
```{r}
t1_dvs = t1_dvs %>% select(-X)
t2_dvs = t2_dvs %>% select(-X)
```

```{r eval=FALSE}
hr_merge = merge(t1_dvs, t2_dvs, by = c("worker_id", "level_1"))

hr_merge = hr_merge %>%
  gather(key, value, -worker_id, -level_1) %>%
  separate(key, c("dv", "time"), sep="\\.") %>%
  mutate(time = ifelse(time == "x", 1, 2))%>%
  rename(sub_id = worker_id,
         breaks = level_1)

t1_dvs = hr_merge %>%
  filter(time == 1) %>%
  select(-time) %>%
  spread(dv, value)

t2_dvs = hr_merge %>%
  filter(time == 2) %>%
  select(-time) %>%
  spread(dv, value)

# calculate point estimates for reliability of each of the variables for each break
# get_icc for each break of tmp_t1_dvs and tmp_t2_dvs

trial_num_rel_df = data.frame(breaks=rep(NA, length(unique(t1_dvs$breaks))),
                              acc_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              avg_rt_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              conceptual_responses_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              fail_to_maintain_set_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              learning_rate_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              learning_to_learn_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              nonperseverative_errors_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              perseverative_errors_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              perseverative_responses_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              total_errors_icc=rep(NA, length(unique(t1_dvs$breaks))))

for(i in 1:length(unique(t1_dvs$breaks))){
  cur_break = unique(t1_dvs$breaks)[i]
  tmp_t1_dvs = t1_dvs %>% filter(breaks == cur_break)
  tmp_t2_dvs = t2_dvs %>% filter(breaks == cur_break)
  trial_num_rel_df$breaks[i] = cur_break
  trial_num_rel_df$acc_icc[i] = get_icc("acc", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$avg_rt_icc[i] = get_icc("avg_rt", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$conceptual_responses_icc[i] = get_icc("conceptual_responses", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$fail_to_maintain_set_icc[i] = get_icc("fail_to_maintain_set", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$learning_rate_icc[i] = get_icc("learning_rate", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$learning_to_learn_icc[i] = get_icc("learning_to_learn", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$nonperseverative_errors_icc[i] = get_icc("nonperseverative_errors", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$perseverative_errors_icc[i] = get_icc("perseverative_errors", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$perseverative_responses_icc[i] = get_icc("perseverative_responses", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$total_errors_icc[i] = get_icc("total_errors", tmp_t1_dvs, tmp_t2_dvs)
}
rm(i, cur_break, tmp_t1_dvs, tmp_t2_dvs)

trial_num_rel_df$breaks = as.numeric(trial_num_rel_df$breaks)

# write.csv(trial_num_rel_df, paste0(retest_data_path, 'trial_num_rel_df_shift.csv'))
```

Takeaways from this graph:
- Reliability estimates stabilize after about an eigthth of the trials (note that this might not be true for other model parameter estimates; here we are only looking at raw response time and accuracy measures)
- Only three measures that are not contrast measures have reliabilities >0
- All the other measures are basically completely unreliable (including the two 'meaningful variables' in the literature: the cue switch cost RT's)

```{r warning=FALSE, message=FALSE}
trial_num_rel_df %>%
  gather(key, value, -breaks) %>%
  filter(key %in% c("avg_rt_icc", "learning_to_learn_icc", "nonperseverative_errors_icc")) %>%
  ggplot(aes((breaks+1)*10, value, shape=key))+
  geom_point()+
  geom_line(alpha = 0.5)+
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
                             plot.caption = element_text(family = "Times", lineheight = 2, hjust = 0)) +
  scale_shape_manual(values = c(0:9),
                     breaks = c("avg_rt_icc","learning_to_learn_icc", "nonperseverative_errors_icc"),
                     labels = c("Average Response Time", "Learning to Learn",  "Nonperseverative Errors")) +
  labs(caption = str_wrap("FIGURE 6: Change in point estimates of ICC for three dependent measures selected from the Shift task using increasing numbers of trials to estimate them. These measures show the different types of relationships reliability estimates have based on the trial numbers used to estimate them.", width = 180))

ggsave('Shift_Task_Intrameasure_Trialnum_Dependendence.tiff', device = "tiff", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures", width = 10, height = 5, units = "in", dpi = 300)
```

### Raw vs DDM

Checking DDM results in the bootstrapped estimates. Variables using all trials are significantly more reliable compared to difference scores. Raw measures don't differ from DDM parameters. Which DDM is better depends on whether all trials are used.

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1,
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv')

tmp_save = tmp %>%
  drop_na() %>% #try removing this in final release
  group_by(contrast, raw_fit, rt_acc) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_vars = n()/1000)

tmp_save %>%
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5'), digits=3)
```

Comparing contrast vs non-contrast: overall has higher reliability than difference.

```{r}
# summary(lmerTest::lmer(icc ~ contrast + (1|dv) ,tmp))
summary(MCMCglmm(icc ~ contrast, random = ~ dv, data=tmp))
```

Comparing raw vs ddm in overall estimates: EZ is significantly better than HDDM and comparable to raw estimates.

```{r}
summary(icc_by_rawfit_noncon_model)
```

Comparing raw vs ddm in difference scores: EZ is significantly worse than HDDM and comparable to raw estimates.

```{r}
summary(icc_by_rawfit_con_model)
```

```{r}
ddm_boot_plot = tmp %>%
  group_by(dv) %>%
  summarise(mean_icc = mean(icc),
            raw_fit = unique(raw_fit),
            contrast = unique(contrast),
            rt_acc = unique(rt_acc)) %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), mean_icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~factor(contrast, levels=c("non-contrast", "contrast"), labels=c("Non-contrast", "Contrast")))+
  theme_bw()+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
        legend.text = element_text(size=16),
        strip.text = element_text(size=16),
        axis.text = element_text(size = 16),
        text = element_text(size=16))+
  guides(fill = guide_legend(ncol = 2, byrow=F))+
  scale_fill_brewer(palette="Greys",
                    breaks=c( "Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))

mylegend<-g_legend(ddm_boot_plot)

grob_name <- names(mylegend$grobs)[1]

#manually fix the legend
#move non-decision down
#key
mylegend$grobs[grob_name][[1]]$layout[11,c(1:4)] <- c(4,8,4,8)
mylegend$grobs[grob_name][[1]]$layout[12,c(1:4)] <- c(4,8,4,8)
#text
mylegend$grobs[grob_name][[1]]$layout[17,c(1:4)] <- c(4,10,4,10)
#move threshold down
#key
mylegend$grobs[grob_name][[1]]$layout[9,c(1:4)] <- c(3,8,3,8)
mylegend$grobs[grob_name][[1]]$layout[10,c(1:4)] <- c(3,8,3,8)
#text
mylegend$grobs[grob_name][[1]]$layout[16,c(1:4)] <- c(3,10,3,10)
#move drift rate right and up
#key
mylegend$grobs[grob_name][[1]]$layout[7,c(1:4)] <- c(2,8,2,8)
mylegend$grobs[grob_name][[1]]$layout[8,c(1:4)] <- c(2,8,2,8)
#text
mylegend$grobs[grob_name][[1]]$layout[15,c(1:4)] <- c(2,10,2,10)

mycaption = textGrob(label = str_wrap("FIGURE 7: Average bootstrapped reliability estimates per measure comparing raw measures and model parameters as well as contrast and non-contrast measures for task measures.", width = 130), gp = gpar(fontfamily="Times", lineheight = 2, fontsize = 18), hjust = 0, vjust = 0.5, x = unit(0, "npc"))

ddm_boot_legend = arrangeGrob(ddm_boot_plot +theme(legend.position="none"),
             mylegend, mycaption, nrow=3, heights=c(10, 1, 1))

ggsave('Bootstrap_DDM_Comp_Legend.tiff', plot = ddm_boot_legend, device = "tiff", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 14, height = 10, units = "in", limitsize = FALSE, dpi=300)
```

```{r}
tmp %>%
  group_by(contrast) %>%
  summarise(mean_icc = mean(icc),
            sd_icc = sd(icc))
```

```{r}
tmp2 = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1,
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(rel_df[,c("dv", "icc")], by = 'dv')

ddm_point_plot = tmp2 %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~factor(contrast, levels=c("non-contrast", "contrast"), labels=c("Non-contrast", "Contrast")))+
  theme_bw()+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
        legend.text = element_text(size=16),
        strip.text = element_text(size=16),
        axis.text = element_text(size = 16),
        text = element_text(size=16))+
  guides(fill = guide_legend(ncol = 2))+
  scale_fill_brewer(palette="Greys",
                    breaks=c( "Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))+
  ylim(-0.4, 1)

mylegend<-g_legend(ddm_point_plot)

grob_name <- names(mylegend$grobs)[1]

#manually fix the legend
#move non-decision down
#key
mylegend$grobs[grob_name][[1]]$layout[11,c(1:4)] <- c(4,8,4,8)
mylegend$grobs[grob_name][[1]]$layout[12,c(1:4)] <- c(4,8,4,8)
#text
mylegend$grobs[grob_name][[1]]$layout[17,c(1:4)] <- c(4,10,4,10)
#move threshold down
#key
mylegend$grobs[grob_name][[1]]$layout[9,c(1:4)] <- c(3,8,3,8)
mylegend$grobs[grob_name][[1]]$layout[10,c(1:4)] <- c(3,8,3,8)
#text
mylegend$grobs[grob_name][[1]]$layout[16,c(1:4)] <- c(3,10,3,10)
#move drift rate right and up
#key
mylegend$grobs[grob_name][[1]]$layout[7,c(1:4)] <- c(2,8,2,8)
mylegend$grobs[grob_name][[1]]$layout[8,c(1:4)] <- c(2,8,2,8)
#text
mylegend$grobs[grob_name][[1]]$layout[15,c(1:4)] <- c(2,10,2,10)

ddm_point_legend = arrangeGrob(ddm_point_plot +theme(legend.position="none"),
             mylegend, nrow=2, heights=c(10, 1))


ggsave('PointEst_DDM_Comp.jpg', plot=ddm_point_legend, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 15, height = 7, units = "in", limitsize = FALSE)
```

## Survey reliabilities

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'survey') %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2)

tmp_save = tmp %>%
  group_by(task_name) %>%
  summarise(median_icc = median(icc),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_measures = n()/1000,
            mean_num_trials = round(mean(num_all_trials)))%>%
  arrange(-median_icc)

tmp_save %>%
  datatable() %>%
  formatRound(columns=c('median_icc', 'icc_2.5','icc_97.5'), digits=3)
```

```{r}
tmp_save = tmp_save%>%
  mutate(task_name = gsub("_", " ", task_name),
         task_name = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", task_name, perl=TRUE),
         task_name = gsub("Survey", "", task_name))

names(tmp_save) = gsub("_", " ", names(tmp_save))
names(tmp_save) = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", names(tmp_save), perl=TRUE)

sjt.df(tmp_save %>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/survey_rel_table.doc")
```

### Number of items

Does number of items in a survey have a significant effect on the average ICC of survey measures? No.

```{r warning=FALSE, message=FALSE}
summary(lmerTest::lmer(icc ~ num_all_trials + (1|dv), data = tmp))
```

```{r warning=FALSE, message=FALSE}
tmp %>%
  ggplot(aes(num_all_trials, icc))+
  geom_point()+
  geom_smooth(method="lm")+
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")
```
