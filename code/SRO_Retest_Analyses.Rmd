---
title: 'Self Regulation Ontology Retest Analyses'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/SRO_Retest_Analyses_Helper_Functions.R')

theme_set(theme_bw())

test_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/'

retest_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-29-2018/'
```

# Introduction

The psychological construct of self-regulation, or related concepts of impulsivity, self-control, inhibition and others correlate with problematic real-world behaviors such as disordered eating (Mobbs et al., 2010, Verbeken et al., 2009, Nederkoorn et al. 2006a, Nederkoorn et al., 2006b), gambling (Lawrence et al., 2009, Fuentes et al., 2006, Alessi and Petry, 2003), drug addiction (Coffey et al., 2003, de Wit, Crean and John, 2000, Sher, Bartholow and Wood, 2000, Kirby, Petry and Bickel, 1999) or bad financial decisions (Meier and Sprenger, 2015, Meier and Sprenger, 2013, Meier and Sprenger 2012). In these lines of work measures of self-regulation are used as behavioral assays for individual difference analyses. An underlying assumption of treating behavioral measures as reflective of person-specific characteristics is that the measures of self-regulation are stable across time. In other words, that they have high test-retest reliability (or high between- and low within-subject variability). This assumption is not extensively and equally tested in the literature for all measures of self-regulation.    
Self-regulation measures can be grouped in two broad categories: Surveys and cognitive tasks. While the former typically goes through some form of psychometric testing often filtering out items with low test-retest reliability this is less frequently true for the latter. Though some empirical results on test-retest reliabilities of certain cognitive task measures are reported in pockets of the literature an exhaustive summary or systematic elimination of tasks or measures based on stability across time does not seem to exist. Test-retest reliability is, however, crucial for individual difference analyses (Hedge, Powell and Sumner, 2017).   
To answer the question of reliability of self-regulation measures comprehensively we created a large battery consisting of both cognitive task and survey measures. In this paper we make two contributions: First we provide a review of these measures along with their reported reliabilities when available. Then we report results of a large study we conducted where we collected retest reliability data using all of these measures. This effort not only fills in a notable gap in the literature in clarifying which measures of self-regulation are stable across time but also provides guidance on factors to pay attention to when constructing new tasks for individual difference measures.  

# Methods

## Data collection

This sample consists of data for 150 subjects of the original sample of 522 that has completed the initial battery of 37 cognitive tasks, 23 surveys and 3 surveys on demographics. Details of the original sample as well as quality control (qc) procedures are described elsewhere (Eisenberg et al., 2017). Invited participants were chosen randomly and only subsets of them were invited for a given batch (instead of opening the battery to all qualified subjects) with the intention to avoid a potential oversampling and bias towards "high self regulators".

```{r warning=FALSE, message=FALSE}
workers = read.csv(paste0(retest_data_path,'Local/User_717570_workers.csv'))
workers = workers %>% 
  group_by(Worker.ID) %>%
  mutate(Retest_worker=ifelse(sum(CURRENT.RetestWorker,CURRENT.RetestWorkerB2,CURRENT.RetestWorkerB3,CURRENT.RetestWorkerB4,CURRENT.RetestWorkerB5,na.rm=T)>0,1,0)) %>%
  ungroup()

worker_counts <- fromJSON(paste0(retest_data_path,'/Local/retest_worker_counts.json'))

worker_counts = as.data.frame(unlist(worker_counts))
names(worker_counts) = "task_count"
```

In total `r sum(workers$Retest_worker)` participants were invited, `r nrow(worker_counts)` began the battery, `r sum(worker_counts$task_count >= 62)` completed the battery and 150 provided data that passed qc for both time points. Our target sample size was determined in advance of data collection and data collection continued until this number of participants with data that survived qc was reached.

```{r warning=FALSE, message=FALSE}
disc_comp_date = read.csv(paste0(retest_data_path,'Local/discovery_completion_dates.csv'), header=FALSE)
val_comp_date = read.csv(paste0(retest_data_path,'Local/validation_completion_dates.csv'), header=FALSE)
test_comp_date = rbind(disc_comp_date, val_comp_date)
rm(disc_comp_date, val_comp_date)
retest_comp_date = read.csv(paste0(retest_data_path,'Local/retest_completion_dates.csv'), header=FALSE)
comp_dates = merge(retest_comp_date, test_comp_date, by="V1")
names(comp_dates) <- c("sub_id", "retest_comp", "test_comp")
comp_dates$retest_comp = as.Date(comp_dates$retest_comp)
comp_dates$test_comp = as.Date(comp_dates$test_comp)
comp_dates$days_btw = with(comp_dates, retest_comp-test_comp)
```

Data collection took place on average `r round(mean(as.numeric(comp_dates$days_btw)))` number of days after the completion of the initial battery with a range of `r round(range(as.numeric(comp_dates$days_btw)))[1]` to `r round(range(as.numeric(comp_dates$days_btw)))[2]` days.

```{r}
rm(test_comp_date, retest_comp_date, comp_dates)
```

## Demographics

```{r}
test_demog <- read.csv(paste0(retest_data_path, '/t1_data/demographic_health.csv'))

retest_demog <- read.csv(paste0(retest_data_path, 'demographic_health.csv'))

retest_demog = retest_demog[retest_demog$X %in% test_demog$X,]

names(test_demog)[which(names(test_demog) == 'X')] <-'sub_id'
names(retest_demog)[which(names(retest_demog) == 'X')] <-'sub_id'

summary(test_demog %>%
          select(Sex, Age))

summary(retest_demog %>%
          select(Sex, Age))
```

## Literature

One of the major contributions of this project is a comprehensive literature review of the retest reliabilities of the surveys and tasks that were used. We reviewed the literature on a measure (as opposed to task level) paying attention to differences in sample size, the delay between the two measurements as well as the statistic that was used to assess reliabilities (e.g. Spearman vs. Pearson correlations). Here we present a table and a visualization summarizing our findings. References mentioned in the table below can be found [here](https://docs.google.com/spreadsheets/d/1hcED4_rWSGSE8Td0aqT0kWqlLCBAzRNKBXj3tSram58/edit?usp=sharing).  

```{r}
lit_review <- read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/lit_review_figure.csv')

lit_review
```

```{r message=FALSE}
lit_review = lit_review %>%
  separate(dv, c("task_group", "var"), sep="\\.",remove=FALSE,extra="merge") %>%
  mutate(task_group = factor(task_group, levels = task_group[order(task)]),
         type = as.character(type)) %>%
  mutate(task_group = gsub("_", " ", task_group),
         var = gsub("_", " ", var)) %>%
  arrange(task_group, raw_fit, var) %>%
  mutate(task_group = gsub("survey", "", task_group),
         task_group = gsub("task", "", task_group),
         task_group = str_to_title(task_group)) %>%
  mutate(task_group = ifelse(task_group == "Psychological Refractory Period Two Choices", "Psychological Refractory Period", ifelse(task_group == "Angling Risk Always Sunny", "Angling Risk", ifelse(task_group == "Two Stage", "Two Step", ifelse(task_group == "Threebytwo", "Task Switching", ifelse(task_group == "Adaptive N Back", "Adaptive N-back", ifelse(task_group == "Go Nogo", "Go/No-go",  ifelse(task_group == "Ravens", "Raven's", task_group)))))))) %>%
  mutate(task_group = ifelse(task_group == "Bis Bas ", "BIS-BAS", ifelse(task_group == "Bis11 ", "BIS-11", ifelse(task_group == "Dospert Eb ", "DOSPERT EB", ifelse(task_group == "Dospert Rp ", "DOSPOERT RP", ifelse(task_group == "Dospert Rt ", "DOSPERT RT", ifelse(task_group == "Erq ", "ERQ", ifelse(task_group == "Upps Impulsivity ", "UPPS-P", task_group))))))))  %>%
  select(-measure_description)


tmp = lit_review[duplicated(lit_review$reference)==FALSE,]

nrow(tmp)
sum(tmp$sample_size)
nrow(lit_review)
```

```{r}
rm(tmp)
```

Measure level plot

```{r warning=FALSE, message=FALSE}
lit_review = lit_review %>%
  select(-reference)
```

```{r warning= FALSE, message =FALSE, echo=FALSE}
p1_legend = lit_review %>%
  filter(task == 'task') %>%
ggplot(aes(y = var, x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape=type))+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80"),
        legend.position = 'bottom') + 
  xlab("")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 16, 17, 3))

p1 = lit_review %>%
  filter(task == 'task') %>%
ggplot(aes(y = var, x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape = type), color='#00BFC4')+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80"),
        legend.position = 'bottom') + 
  xlab("")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 16, 17, 3))

p2 = lit_review %>%
  filter(task == 'survey') %>%
ggplot(aes(y = var, x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape = type), color = '#F8766D')+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80")) + 
  xlab("")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 17, 3))

mylegend<-g_legend(p1_legend)

p3 <- arrangeGrob(arrangeGrob(p1 +theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(10, 1))

ggsave('Lit_Review_Plot.jpg', plot = p3, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 24, height = 20, units = "in", dpi=100)
rm(p1, p2, p3, p1_legend, mylegend)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/Lit_Review_Plot.jpg')
```

Because this plot is difficult to digest we summarize it on a task level to give a general sense of the main takeaways. This plot naturally disregards much of the fine grained information.

```{r}
p1_t_legend <- lit_review %>%
  filter(task == 'task') %>%
ggplot(aes(y = factor(task_group, levels=rev(unique(task_group))), x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape = type), color='black')+
  theme(axis.text.y = element_text(size=30),
        legend.position = 'bottom',
        axis.text.x = element_text(size=20),
        axis.title.x = element_text(size=20), 
        legend.text = element_text(size=12), 
        legend.key.width = unit(0.75, "inches"), 
        legend.title = element_text(size=20),
        legend.spacing.x = unit(0.5, "inches")) + 
  guides(size = guide_legend(override.aes = list(size=c(9,18,28))),
         shape = guide_legend(override.aes = list(size=18)))+
  xlab("Retest Reliability")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_y_discrete(labels = function(x) str_wrap(x, width = 10))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 16, 17, 3), name="Type")+
  scale_size_continuous(name = "Sample Size")+
  geom_vline(xintercept = 0, color = "red", size = 1)

p1_t <- lit_review %>%
  filter(task == 'task') %>%
ggplot(aes(y = factor(task_group, levels=rev(unique(task_group))), x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape = type), color='#00BFC4')+
  theme(axis.text.y = element_text(size=30),
        legend.position = 'none',
        axis.text.x = element_text(size=23),
        axis.title.x = element_text(size=30)) + 
  xlab("Retest Reliability")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_y_discrete(labels = function(x) str_wrap(x, width = 15))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 16, 17, 3))+
  scale_size_continuous(range = c(5, 35))+
  geom_vline(xintercept = 0, color = "red", size = 1)

p2_t <- lit_review %>%
  filter(task == 'survey') %>%
ggplot(aes(y = factor(task_group, levels=rev(unique(task_group))), x = retest_reliability)) + 
  geom_point(aes(size=sample_size, shape = type), color='#F8766D')+
  theme(axis.text.y = element_text(size=30),
        legend.position = 'none',
        axis.text.x = element_text(size=23),
        axis.title.x = element_text(size=30)) + 
  xlab("Retest Reliability")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_y_discrete(labels = function(x) str_wrap(x, width = 15))+
  scale_shape_manual(breaks = sort(lit_review$type), values = c(15, 16, 17, 3))+
  scale_size_continuous(range = c(5, 35))+
  geom_vline(xintercept = 0, color = "red", size = 1)

mylegend<-g_legend(p1_t_legend)

p3_t <- arrangeGrob(arrangeGrob(p1_t, p2_t, nrow=1), mylegend, nrow=2,heights=c(10, 1))

ggsave('Lit_Review_Plot_t.jpg', plot = p3_t, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 27, height = 48, units = "in", limitsize = FALSE, dpi = 72)
rm(p1_t, p2_t, p3_t, mylegend, p1_t_legend)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/Lit_Review_Plot_t.jpg')
```

An interactive version of this plot could be find [here](https://zenkavi.github.io/SRO_Retest_Analyses/output/reports/Lit_Review_Figure.html)

Takeaways from this review are:   
- Survey measures have been reported to higher reliability compared to task measures  
- Survey measures have less variability in the reported reliabiltiy estimates compared to task measures   

## Loading datasets

The variables included in this report are:  
- meaningful variables (includes only hdddm parameters)  
- EZ diffusion parameters  
- Raw RT and Accuracy measures  
- Variables found in the literature (for comparison)  

```{r echo=FALSE}
#Get variables of interest from Ian's release
tmp1 <- read.csv(paste0(test_data_path,'meaningful_variables.csv'))
tmp2 <- read.csv(paste0(test_data_path,'meaningful_variables_noDDM.csv'))
tmp3 <- read.csv(paste0(test_data_path,'meaningful_variables_EZ.csv'))
retest_report_vars = c(names(tmp1), names(tmp2), names(tmp3))
retest_report_vars = unique(retest_report_vars)
lit_rev_vars = as.character(unique(lit_review$dv)[which(unique(lit_review$dv) %in% retest_report_vars == FALSE)])
retest_report_vars = c(retest_report_vars, lit_rev_vars)
rm(tmp1, tmp2, tmp3, lit_rev_vars)
```

### Load time 1 data
```{r}
test_data <- read.csv(paste0(retest_data_path,'t1_data/variables_exhaustive.csv'))

test_data <- test_data[,names(test_data) %in% retest_report_vars]

test_data$X <- as.character(test_data$X)
names(test_data)[which(names(test_data) == 'X')] <-'sub_id' 
```

For reference here are the variables that are **not** included in the analyses of the remainder of this report because they were not of theoretical interest in factor structure analyses of this data so far. These include drift diffusion and other model parameters for specific conditions within a task; survey variables that are not part of the dependant variables for that survey in the literature and demographics (these are saved for prediction analyses).

```{r}
test_data2 <- read.csv(paste0(retest_data_path, 't1_data/variables_exhaustive.csv'))

df <- data.frame(names(test_data2)[which(names(test_data2) %in% names(test_data) == FALSE)])
names(df) = c('vars')

df
```

```{r echo=FALSE}
rm(test_data2, df)
```

### Load time 2 data 
```{r}
retest_data <- read.csv(paste0(retest_data_path,'variables_exhaustive.csv'))

retest_data <- retest_data[,names(retest_data) %in% retest_report_vars]

retest_data$X <- as.character(retest_data$X)
names(retest_data)[which(names(retest_data) == 'X')] <-'sub_id' 
retest_data = retest_data[retest_data$sub_id %in% test_data$sub_id,]
```

### Replace HDDM parameters in t1 data

Since HDDM parameters depend on the sample on which they are fit we refit the model on t1 data for the subjects that have t2 data. Here we replace the HDDM parameters in the current t1 dataset with these refitted values. 

```{r}
hddm_refits <- read.csv(paste0(retest_data_path,'t1_data/hddm_refits_exhaustive.csv'))

hddm_refits = hddm_refits[,names(hddm_refits) %in% retest_report_vars]

hddm_refits$X <- as.character(hddm_refits$X)
names(hddm_refits)[which(names(hddm_refits) == 'X')] <-'sub_id' 

#For later comparison of whether fitting the DDM parameters on full or retest sample makes a big difference
test_data_full_sample_hddm <- test_data

#drop hddm columns from test_data
test_data = cbind(test_data$sub_id, test_data[,names(test_data) %in% names(hddm_refits) == FALSE])

#fix naming before merging
names(test_data)[which(names(test_data) == 'test_data$sub_id')] <-'sub_id'

#merge hddm refits to test data
test_data = merge(test_data, hddm_refits, by="sub_id")
```

# Results

## Data quality checks

### Demographics reliability

Point estimates of reliability for the demographic variabels.

```{r}
process_boot_df = function(df){
  df = df %>%
  drop_na() %>%
  mutate(dv = as.character(dv),
         icc = as.numeric(as.character(icc)),
         spearman = as.numeric(as.character(spearman)),
         pearson = as.numeric(as.character(pearson)),
         eta_sq = as.numeric(as.character(eta_sq)),
         sem = as.numeric(as.character(sem)),
         partial_eta_sq = as.numeric(as.character(partial_eta_sq)),
         omega_sq = as.numeric(as.character(omega_sq)),
         var_subs = as.numeric(as.character(var_subs)),
         var_ind = as.numeric(as.character(var_ind)),
         var_resid = as.numeric(as.character(var_resid)),
         F_time = as.numeric(as.character(F_time)),
         p_time = as.numeric(as.character(p_time)),
         df_time = as.numeric(as.character(df_time)),
         df_resid = as.numeric(as.character(df_resid)))
  return(df)} 

demog_boot_df <- read.csv(gzfile(paste0(retest_data_path,'demog_boot_merged.csv.gz')))

demog_boot_df = process_boot_df(demog_boot_df)
```

```{r}
tmp = demog_boot_df %>%
  group_by(dv) %>%
  summarise(median_icc = quantile(icc, probs=0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975)) %>%
  arrange(-median_icc)
tmp
```

```{r}
sjt.df(tmp%>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/demog_rel_table.doc")
```

Point estimates of reliability for demog items

```{r}
numeric_cols = c()

for(i in 1:length(names(test_demog))){
  if(is.numeric(test_demog[,i])){
    numeric_cols <- c(numeric_cols, names(test_demog)[i])
  }
}

demog_rel_df <- data.frame(icc = rep(NA, length(numeric_cols)))

row.names(demog_rel_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  demog_rel_df[numeric_cols[i], 'icc'] <- get_icc(numeric_cols[i], t1_df = test_demog, t2_df = retest_demog)
}

demog_rel_df = demog_rel_df %>%
  mutate(var = row.names(.)) %>%
  select(var, icc) %>%
  arrange(-icc)
```

Checking low reliability demog vars: Mostly NA's

```{r}
test_demog$HowOftenHazardousCannabis
retest_demog$HowOftenHazardousCannabis

test_demog$CaffieneOtherSourcesDayMG
retest_demog$CaffieneOtherSourcesDayMG

test_demog$SpouseParentsComplainDrugUse
retest_demog$SpouseParentsComplainDrugUse

test_demog$OtherDebtAmount
retest_demog$OtherDebtAmount
```

Plotting point estimates of reliability for demographic items coloring those that have a time limit darker in the histogram.

```{r warning=FALSE, message=FALSE}
timed_demogs <- c("DoctorVisitsLastMonth","DaysHalfLastMonth","Worthless","DaysPhysicalHealthFeelings","Depressed","EverythingIsEffort","Hopeless","RestlessFidgety","Nervous","DaysLostLastMonth","Last30DaysUsual")

demog_rel_df %>%
  mutate(timed = ifelse(var %in% timed_demogs, 1, 0)) %>%
  ggplot(aes(icc, fill = factor(timed)))+
  geom_histogram(position = "identity", alpha = 0.7, color = NA)+
  xlab("ICC")+
  ylab("Count")+
  scale_fill_manual(values = c("grey", "grey25"))+
  theme(legend.position = "none")

ggsave('DemogRelDist.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 6, height = 5, units = "in", dpi=500)
```

```{r echo=FALSE}
rm(test_demog, retest_demog, demog_rel_df, numeric_cols)
```

### Effect of delays between the two measurements

Due to our data collection strategy we did not strictly control for the delay between the two measurements as standard psychometrics studies measuring retest reliability might. 

An individual difference measure would preferably remain stable regardless of the delay between multiple measurements.

Since we only had two measurements we could not test directly whether a measure becomes less reliable depending on the delay between the two time points (The average number of days between two measurements would be the same for all measures since reliability is a measure level metric while days between completion a subject level one).

So if you regress the average difference for each measure on average delay between two measurements you are regressing a vector with varying numbers on a vector of same values, like a t test asking if the mean of the varying column, in this case the average difference score, is different than the unique value in the single value column, i.e. the average delay between two time points (Note that this should be true in theory but in practice since for each measure there might subjects for whom the dv could not be calculated there might be >1 unique values for the average delay between the two time points). This analysis is not meaningful. 

Instead of the summary metric like the retest reliability estimate for each measure we can check whether the difference score distribution for each measure depends on the delay between the two measurements. Since the difference score distribution is at subject level we can check whether the order of subjects in this distribution depends on their order in the distribution of days between completing the two tests.

Make data frame with difference between two scores for each measure for each subject. Since the scores for different measures are on different scales for comparability the difference scores are normalized (ie demeaned and dividied by the sd of the difference score distribution.) Note that the variance of the difference score distribution accounts for the variance in both time points by summing them. Normalization equates the means of each difference score distribution to 0 which would mask any meaningful change between the two time points but the analysis here does not interpret the mean of the difference score distributions but is interested in its relation to the days between completion. We check if the variables show systematic differences between the two points later.

Here we check if the difference is larger the longer the delay

```{r warning=FALSE}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i]) & names(test_data)[i] %in% names(retest_data)){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

t1_2_difference = data.frame()

for(i in 1:length(numeric_cols)){
  tmp = match_t1_t2(numeric_cols[i],format='wide')
  tmp = tmp %>% 
  mutate(difference = scale(`2` - `1`))
  t1_2_difference = rbind(t1_2_difference, tmp)
}

t1_2_difference$difference = as.data.frame(t1_2_difference$difference)$V1

t1_2_difference = t1_2_difference %>% separate(dv, c("task", "dv2"), sep="\\.", remove=FALSE)
```

```{r echo=FALSE}
rm(tmp, i)
```

Add completion dates to this data frame.

```{r}
retest_task_comp_times = read.csv(paste0(retest_data_path, 'Local/retest_task_completion_times.csv'))
test_task_comp_times = read.csv(paste0(retest_data_path, 'Local/test_task_completion_times.csv'))
task_comp_times = merge(retest_task_comp_times, test_task_comp_times, by=c('worker_id','task'))
rm(retest_task_comp_times, test_task_comp_times)
task_comp_times = task_comp_times %>%
  select(-X.x, -X.y) %>%
  mutate(finish_day.x = as.Date(finish_day.x),
         finish_day.y = as.Date(finish_day.y),
         days_btw = finish_day.x-finish_day.y) %>%
  rename(sub_id=worker_id)
```

```{r warning=FALSE}
t1_2_difference = merge(t1_2_difference, task_comp_times[,c('sub_id', 'task','days_btw')], by=c('sub_id', 'task'))
```

```{r echo=FALSE}
rm(task_comp_times)
```

What does the distribution of differences look like: The distribution of differences between two time points for each measure 

```{r warning=FALSE, message=FALSE}
t1_2_difference %>%
  ggplot(aes(difference, alpha=dv))+
  geom_histogram(position='identity')+
  theme(legend.position = 'none')
```

How do the difference score distributions look like with respect to the days between completion?

```{r}
t1_2_difference %>%
  ggplot()+
  geom_smooth(aes(as.numeric(days_btw), abs(difference), group=factor(dv)), method='lm', se=FALSE, color = 'grey', alpha = 0.5)+
  geom_smooth(aes(as.numeric(days_btw), abs(difference)), method='lm', color = "black", se=FALSE)+
  theme(legend.title = element_blank())+
  xlab('Days between completion')+
  ylab('Scaled difference score')

ggsave('DaysBtwEffect.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 6, height = 4, units = "in", limitsize = FALSE, dpi = 500)
```

To test if the slope of the black is significant we would run a mixed effects model with a fixed effect for days between completion, random slope for each dv depending on the days between and random intercept for each dv.

Before I was using subjects as a random effect but days between the two time points for each measure depends on subj id. What varies randomly is which dv we are looking for its distribution of differences in relation to the days between the time points. So I changed the model to have fixed effect for the days between, a random slope for (dependent variables can be differentially sensitive to th effect of days between) and a random intercept for dependent variable.

Significant fixed effect suggests that on average the longer the delay the smaller the difference.

```{r}
#summary(lmerTest::lmer(abs(difference) ~ scale(days_btw)+(scale(days_btw) | dv), data=t1_2_difference)) 

summary(tmp <- MCMCglmm(abs(difference) ~ scale(days_btw), random = ~us(scale(days_btw)):dv, data=t1_2_difference)) 
```

But if I just run one mixed effects model then we don't get a sense of the simple effects (how many of the variables this effect of the days between is significant and in which direction). I can run it separately for each dv to see if all difference score distributions are affected the same way depending on the days between completion. But since we are running so many tests we need to correct for multiple comparisons. How many of these tests are significant FDR correcting? None.

```{r}
get_delay_effect = function(df){
  mod = lm(abs(difference) ~ scale(days_btw), data = df)
  out = data.frame(estimate=coef(summary(mod))["scale(days_btw)","Estimate"], pval=coef(summary(mod))["scale(days_btw)","Pr(>|t|)"])
  return(out)
}

days_effect  = t1_2_difference %>%
  group_by(dv) %>%
  do(get_delay_effect(.))

#Correct p-values for multiple comparisons
sum(p.adjust(days_effect$pval, method = "fdr") < 0.05)
```

Conclusion: The average difference between the scores of a measure doesn't change with the increased delay. 

### Overlapping survey questions

Some surveys have overlapping questions. Do these correlate within and across sessions?

First determine the overlapping questions.

```{r}
tmp = read.csv(gzfile(paste0(retest_data_path, 'items.csv.gz')))
tmp = tmp %>% 
  filter(worker == 's005') %>% 
  select(item_ID, item_text) %>% 
  mutate(item_text = trimws(as.character(item_text))) %>%
  unite(item, c("item_ID", "item_text"), sep = "___")

comb = as.data.frame(t(combn(unique(tmp$item),2)))

duplicate_items = comb %>% 
  filter(grepl('dospert', V1)==FALSE) %>%
  filter(grepl('selection_optimization', V1)==FALSE) %>%
  filter(grepl('sensation_seeking', V1)==FALSE) %>%
  separate(V1, c("item1_ID", "item1_text"), sep="___") %>%
  separate(V2, c("item2_ID", "item2_text"), sep="___") %>%
  mutate(similarity = levenshteinSim(item1_text, item2_text)) %>%
  filter(similarity>0.8) %>%
  select(similarity, item1_ID, item2_ID, item1_text, item2_text) %>%
  arrange(-similarity)

duplicate_items
```

```{r warning=FALSE, message=FALSE}
#surveys to read in
extract_items = c('worker',unique(with(duplicate_items, c(item1_ID, item2_ID))))

#correlations to compute:
#item1_t1 - item2_t1, 
#item1_t2 - item2_t2, 
#item1_t1 - item2_t2, 
#item1_t2 - item2_t1

duplicate_items_data_t1 = read.csv(paste0(test_data_path, 'subject_x_items.csv'))
duplicate_items_data_t2 = read.csv(paste0(retest_data_path, 'subject_x_items.csv'))

duplicate_items_data_t1 = duplicate_items_data_t1 %>%
  filter(worker %in% duplicate_items_data_t2$worker) %>%
  select(extract_items)

duplicate_items_data_t2=duplicate_items_data_t2 %>%
  filter(worker %in% duplicate_items_data_t1$worker) %>%
  select(extract_items)

duplicate_items = duplicate_items %>%
  mutate(t1_t1_cor = NA,
         t2_t2_cor = NA,
         t1_t2_cor = NA,
         t2_t1_cor = NA,
         t1_t1_polycor = NA,
         t2_t2_polycor = NA,
         t1_t2_polycor = NA,
         t2_t1_polycor = NA)

for(i in 1:nrow(duplicate_items)){
  duplicate_items$t1_t1_cor[i] = abs(cor(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))
  
  duplicate_items$t2_t2_cor[i] = abs(cor(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))
  
  duplicate_items$t1_t2_cor[i] = abs(cor(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))
  
  duplicate_items$t2_t1_cor[i] = abs(cor(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                                  duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))
  
  duplicate_items$t1_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t1_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])
}

for(i in 1:nrow(duplicate_items)){
  duplicate_items$t1_t1_cor[i] = abs(cor(scale(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])])))
  
  duplicate_items$t2_t2_cor[i] = abs(cor(scale(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])])))
  
  duplicate_items$t1_t2_cor[i] = abs(cor(scale(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])])))
  
  duplicate_items$t2_t1_cor[i] = abs(cor(scale(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])]),
                                  scale(duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])])))
  
  duplicate_items$t1_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t1_t2_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t1[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t2[,c(duplicate_items$item2_ID[i])]))$rho[2])

  duplicate_items$t2_t1_polycor[i] = abs(polychoric(data.frame(duplicate_items_data_t2[,c(duplicate_items$item1_ID[i])],
                      duplicate_items_data_t1[,c(duplicate_items$item2_ID[i])]))$rho[2])
}

duplicate_items %>%
  arrange(-t1_t1_polycor, -t2_t2_polycor)
```

```{r}
# summary(duplicate_items$t1_t1_cor)
# summary(duplicate_items$t2_t2_cor)
# summary(duplicate_items$t1_t2_cor)
# summary(duplicate_items$t2_t1_cor)

summary(duplicate_items$t1_t1_polycor)
summary(duplicate_items$t2_t2_polycor)
# summary(duplicate_items$t1_t2_polycor)
# summary(duplicate_items$t2_t1_polycor)
```

```{r}
tmp = duplicate_items %>%
  select(similarity, t1_t1_polycor, t2_t2_polycor) %>%
  gather(key, value, -similarity) 


# tmp = duplicate_items %>%
#   select(similarity, t1_t1_polycor, t1_t2_polycor, t2_t2_polycor, t2_t1_polycor) %>%
#   gather(key, value, -similarity) 


tmp %>%
  ggplot(aes(similarity, value, col=key))+
  geom_smooth(method="lm", alpha = 0.15)+
  ylab("Polychoric correlation")+
  xlab("Levenshtein distance")+
  scale_color_discrete(breaks = c("t1_t1_polycor", "t2_t2_polycor"),
                       labels = c("T1 correlations", "T2 correlations"),
                       name = element_blank())+
  theme_bw()

ggsave('OverlappingItemSimilarity.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 6, height = 4, units = "in", limitsize = FALSE, dpi = 500)
```


```{r}
summary(lm(value~key*scale(similarity),tmp))
```

```{r}
summary(MCMCglmm(value ~ key*scale(similarity), data=tmp, nitt = 50000)) 
```


Time 1: item 1 - item 2
Time 1 item 1 - time 2 item 2
Time 2 item 1 - time 1 item 2
Time 2: item 1 - item 2

```{r warning=FALSE, message=FALSE}
tmp = duplicate_items_data_t1 %>%
  gather(key, value, -worker)

tmp2 = duplicate_items_data_t2 %>%
  gather(key, value, -worker)

tmp = duplicate_items %>%
  select(item1_ID, item2_ID) %>%
  left_join(tmp, by = c("item1_ID" = "key")) %>%
  left_join(tmp, by = c("item2_ID" = "key", "worker" = "worker")) %>%
  rename(item1_time1 = value.x, item2_time1 = value.y) %>%
  left_join(tmp2, by = c("item1_ID" = "key", "worker" = "worker")) %>%
  left_join(tmp2, by = c("item2_ID" = "key", "worker" = "worker")) %>%
  rename(item1_time2 = value.x, item2_time2 = value.y) %>%
  group_by(item1_ID) %>%
  mutate(item1_time1 = scale(item1_time1),
         item1_time2 = scale(item1_time2),
         item2_time1 = scale(item2_time1),
         item2_time2 = scale(item2_time2),
         item1_time1 = ifelse(item1_ID == "mpq_control_survey.13", item1_time1*-1, item1_time1),
         item1_time2 = ifelse(item1_ID == "mpq_control_survey.13", item1_time2*-1, item1_time2)) %>%
  ungroup()

p1 = tmp %>%
  ggplot(aes(item1_time1, item2_time1, col=item1_ID))+
  geom_smooth (alpha=0.3, size=0, span=0.5, method = "lm")+
  stat_smooth (geom="line", alpha=1, size=1, span=0.5, method= "lm")+
  theme(legend.position = "none")+
  geom_abline(slope=1, intercept=0, size = 2, linetype = "dashed")+
  xlab("Item 1 T1")+
  ylab("Item 2 T1")
 
p2 = tmp %>%
  ggplot(aes(item1_time1, item2_time2, col=item1_ID))+
  geom_smooth (alpha=0.3, size=0, span=0.5, method = "lm")+
  stat_smooth (geom="line", alpha=1, size=1, span=0.5, method= "lm")+
  theme(legend.position = "none")+
  geom_abline(slope=1, intercept=0, size = 2, linetype = "dashed")+
  xlab("Item 1 T1")+
  ylab("Item 2 T2")

p3 = tmp %>%
  ggplot(aes(item1_time2, item2_time1, col=item1_ID))+
  geom_smooth (alpha=0.3, size=0, span=0.5, method = "lm")+
  stat_smooth (geom="line", alpha=1, size=1, span=0.5, method= "lm")+
  theme(legend.position = "none")+
  geom_abline(slope=1, intercept=0, size = 2, linetype = "dashed")+
  xlab("Item 1 T2")+
  ylab("Item 2 T1")

p4 = tmp %>%
  ggplot(aes(item1_time2, item2_time2, col=item1_ID))+
  geom_smooth (alpha=0.3, size=0, span=0.5, method = "lm")+
  stat_smooth (geom="line", alpha=1, size=1, span=0.5, method= "lm")+
  theme(legend.position = "none")+
  geom_abline(slope=1, intercept=0, size = 2, linetype = "dashed")+
  xlab("Item 1 T2")+
  ylab("Item 2 T2")

p5 = arrangeGrob(p1, p2, p3, p4, ncol=2)

ggsave('DataCheckOverlappingItems.jpg', p5, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 8, height = 5, units = "in", limitsize = FALSE, dpi = 500)

rm(tmp, tmp2, p1, p2, p3, p4, p5)
```

```{r echo=FALSE}
rm(tmp, comb, t1_2_difference, duplicate_items, duplicate_items_data_t1, duplicate_items_data_t2, extract_items)
```

## Comparison to prior literature

Read in and process bootstrapped results.

```{r}
boot_df <- read.csv(gzfile(paste0(retest_data_path,'bootstrap_merged.csv.gz')))

boot_df = process_boot_df(boot_df)

boot_df = boot_df[boot_df$dv %in% retest_report_vars,]

# Check if you have all variables bootstrapped
# retest_report_vars[which(retest_report_vars %in% boot_df$dv==FALSE)]

# Boot df contains hddm parameters fit on the full sample in the t1 data
# refits_bootstrap_merged.csv.gz contains bootstrapped reliabilities 

refit_boot_df = read.csv(gzfile(paste0(retest_data_path,'refits_bootstrap_merged.csv.gz')))

refit_boot_df = process_boot_df(refit_boot_df)

fullfit_boot_df = boot_df[as.character(boot_df$dv) %in% unique(as.character(refit_boot_df$dv)),]

boot_df = boot_df[!as.character(boot_df$dv) %in% unique(as.character(refit_boot_df$dv)),]

boot_df = rbind(boot_df, refit_boot_df)

rm(refit_boot_df)
```

Summarize bootstrapped results and merge to lit review data

```{r message=FALSE}
var_boot_df = boot_df %>%
  group_by(dv) %>%
  summarise(mean_icc = mean(icc),
            mean_pearson = mean(pearson))

rel_comp = lit_review %>%
  left_join(var_boot_df, by = 'dv')
```

Here's what our data looks like: (583 data points for 171 measures)

```{r}
rel_comp
```

### Lit review results

Distribution of reliabilities, sample sizes and delays

```{r}
rel_comp %>% 
  select(dv, task, retest_reliability, sample_size, days) %>%
  filter(days < 3600) %>%
  gather(key, value, -dv, -task) %>%
  ggplot(aes(value, fill=task))+
  geom_density(alpha=0.5, position='identity')+
  facet_wrap(~key, scales='free')+
  theme(legend.title = element_blank())
```

```{r}
summary(rel_comp$sample_size[rel_comp$task == "survey"])
summary(rel_comp$sample_size[rel_comp$task == "task"])

rel_comp %>%
  group_by(task) %>%
  summarise(mean_sample_size = mean(sample_size),
            sem_sample_size = sem(sample_size)) %>%
  ggplot(aes(task,mean_sample_size,col=task))+
  geom_point(size=5)+
  geom_errorbar(aes(ymin=mean_sample_size-sem_sample_size, ymax=mean_sample_size+sem_sample_size), width = 0, size=2)+
  xlab("")+
  ylab("Sample size")+
  theme(legend.position = 'none',
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))

ggsave('LitReviewSampleSize.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 3, height = 5, units = "in", limitsize = FALSE, dpi = 500)
```

The literature has smaller sized samples for task measures compared to survey measures that report retest reliability.

```{r}
# summary(lm(sample_size ~ task,rel_comp))
summary(m1 <- MCMCglmm(sample_size ~ task, data=rel_comp))
```

What predicts retest reliability in the literature?
Task, sample size, days

```{r}
mod1 = lmer(retest_reliability ~ task + (1|dv), rel_comp)
mod2 = lmer(retest_reliability ~ task + sample_size + (1|dv), rel_comp)

anova(mod1, mod2)
```

```{r}
mod3 = lmer(retest_reliability ~ task + sample_size + days+ (1|dv), rel_comp)

anova(mod2, mod3)
```

```{r}
#summary(mod2)
summary(MCMCglmm(retest_reliability ~ task + sample_size, random = ~ dv, data = rel_comp))
```

```{r}
rel_comp %>% 
  group_by(task) %>%
  summarise(mean_rr = mean(retest_reliability))
```

Are the residuals of this model heteroscedastic? Yes the residuals do not depend on the predictor.

```{r}
tmp = data.frame(rel_comp$sample_size, resid(mod2)) %>%
  rename(sample_size = rel_comp.sample_size, resid = resid.mod2.)

tmp %>%
  ggplot(aes(sample_size, resid))+
  geom_point()+
  geom_smooth(method = "lm")
```


```{r}
summary(lm(resid ~ sample_size, tmp))
```

Tasks have significantly lower reliability and reliability decreases with increasing sample size.

```{r}
rel_comp %>% 
  ggplot(aes(sample_size, retest_reliability, color=task))+
  geom_smooth(method='lm')+
  # geom_point(alpha = 0.2)+
  theme(legend.title = element_blank(),
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14))+
  xlab("Sample Size")+
  ylab("Retest reliability")+
  ylim(-0.15, 1)

ggsave('LitReviewRelBySampleSize.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 8, height = 5, units = "in", limitsize = FALSE, dpi = 500)
```

Highlighting the difference between survey and task reliability for comparability to our results.

```{r}
rel_comp %>% 
  ggplot(aes(task, retest_reliability, fill = task))+
  geom_boxplot()+
  theme(legend.position = 'none',
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))+
  xlab("")+
  ylab("Mean ICC")+
  ylim(-0.4,1)

ggsave('LitTaskVsSurvey.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 3, height = 5, units = "in", limitsize = FALSE, dpi = 500)
```

Note: We checked whether our results diverge most from studies with smaller sample sizes. Square difference between our mean estimate and the reliability from the literature decreases exponentially with sample size. The smaller the sample size in the literature the more the reliability estimate differs from our results. But this was a weak result because most of the studies in the literature have smaller sample sizes and you see both small and large deviations for these studies (these were not significant either).

### Correlation with our results

Correlation between our mean estimates from bootstrapped samples and the literature review for task variables

```{r}
n_df = rel_comp %>% 
  group_by(dv) %>%
  tally()

lit_emp_cor = function(){
  
  boot_comp = data.frame()
  
  for(i in 1:length(unique(rel_comp$dv)) ){
    cur_dv = unique(rel_comp$dv)[i]
    n = n_df$n[n_df$dv == cur_dv]
    sample_df = boot_df %>% filter(dv == cur_dv)
    tmp = sample_n(sample_df, n)
    boot_comp = rbind(boot_comp, tmp)
  }  
  
  rm(cur_dv, n, sample_df, tmp)
  
  #check if cbind is ok
  # sum(boot_comp$dv == rel_comp$dv)
  #cbinding pearson because that is the most common metric in the lit
  rel_comp = cbind(rel_comp, boot_comp$pearson)
  #rename new column
  names(rel_comp)[which(names(rel_comp) == "boot_comp$pearson")] = "pearson"
  
  out = data.frame(task = NA, survey = NA)
  
  out$task = with(rel_comp %>% filter(task == "task"), cor(pearson, retest_reliability))
  
  out$survey = with(rel_comp %>% filter(task == "survey"), cor(pearson, retest_reliability))
  
  rel_comp = rel_comp[,-16]
  
  return(out)
}

#lit_emp_cor_out = plyr::rdply(100, lit_emp_cor)

#write.csv(lit_emp_cor_out,'/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/lit_emp_cor_out.csv')

lit_emp_cor_out = read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/lit_emp_cor_out.csv')

summary(lit_emp_cor_out)

```

### Noise ceiling

Model comparisons building model to predict the reliabilities in the literature from a sample from our results versus a sample form the literature.

sample one row per measure out of lit review
r^2 of retest_reliability ~ sampled_reliability vs. 
r^2 of retest_reliability ~ mean_icc

coef of retest_reliability ~ sampled_reliability vs. 
coef of retest_reliability ~ mean_icc

```{r}
comp_lit_pred <- function(df){
  
  sample_from_dv <- function(df){
    if(nrow(df)>1){
      row_num = sample(1:nrow(df),1)
      sample_row = df[row_num,]
      df = df[-row_num,]
      df$lit_predictor = sample_row$retest_reliability
    }
    return(df)
  }
  
  sampled_df = df %>%
    group_by(dv) %>%
    do(sample_from_dv(.)) %>%
    na.omit()
  
  if(length(unique(sampled_df$task))>1){
    mod_lit = lm(retest_reliability ~ lit_predictor+scale(sample_size)+task, data=sampled_df)
    mod_boot = lm(retest_reliability ~ mean_pearson+scale(sample_size)+task, data=sampled_df)
  }
  else{
    mod_lit = lm(retest_reliability ~ lit_predictor+scale(sample_size), data=sampled_df)
    mod_boot = lm(retest_reliability ~ mean_pearson+scale(sample_size), data=sampled_df)
  }
  
  
  out = data.frame(r2_lit = summary(mod_lit)$adj.r.squared,
                   r2_boot = summary(mod_boot)$adj.r.squared,
                   m_lit = coef(summary(mod_lit))["lit_predictor","Estimate"],
                   m_boot = coef(summary(mod_boot))["mean_pearson","Estimate"])
  
  return(out)
}

comp_lit_pred_out = plyr::rdply(1000, comp_lit_pred(rel_comp))
```


```{r}
tmp = comp_lit_pred_out %>%
  select(-.n, -m_lit, -m_boot) %>%
  gather(key, value) %>%
  separate(key, c("stat", "sample"), sep = "_")
 
# tmp$stat = as.factor(tmp$stat)
# levels(tmp$stat) <- c("coefficient", expression(r^"2"))

tmp %>%  
  ggplot(aes(value, fill=sample))+
  geom_density(alpha = 0.5, position='identity', color=NA)+
  # facet_grid(.~stat, scales='free', labeller = label_parsed)+
  scale_fill_manual(breaks=c("boot","lit"),
                       labels=c("Empirical", "Literature"),
                      name="Predictor",
                      values = c("gray15", "gray75"))+
  xlab('Proportion of Variance in Literature Explained')+
  ylab('Density')+
  xlim(0,1)+
  ylim(0,40)+
  theme(axis.title.x  = element_text(size=12),
        axis.text.x  = element_text(size=12),
        axis.text.y  = element_text(size=12),
        axis.title.y  = element_text(size=12),
        legend.title  = element_text(size=12),
        legend.text  = element_text(size=12))

ggsave('LitAndBoot_Noise_Ceiling.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 6, height = 4, units = "in", limitsize = FALSE, dpi = 500)
```


```{r}
with(comp_lit_pred_out, t.test(r2_lit, r2_boot, paired=T))
```

```{r}
tmp = comp_lit_pred_out %>%
  select(r2_lit, r2_boot) %>%
  gather(key, value)

summary(MCMCglmm(value ~ key, data = tmp))
```

Are you at noise ceiling for suveys vs. tasks? Are you better in estimating the literature using our data for surveys vs tasks?

```{r}
comp_lit_pred_out_task = plyr::rdply(1000, comp_lit_pred(rel_comp %>% filter(task=="task")))
comp_lit_pred_out_task$task = "task"
comp_lit_pred_out_survey = plyr::rdply(1000, comp_lit_pred(rel_comp %>% filter(task=="survey")))
comp_lit_pred_out_survey$task = "survey"
comp_lit_pred_out_both = rbind(comp_lit_pred_out_task, comp_lit_pred_out_survey)

comp_lit_pred_out_both %>%
  select(-.n, -m_lit, -m_boot) %>%
  gather(key, value, -task) %>%
  separate(key, c("stat", "sample"), sep = "_") %>%
  ggplot(aes(value, fill=sample, alpha=task, color=task))+
  geom_density(position='identity', size=2)+
  # facet_grid(~task)+
  scale_fill_manual(breaks=c("boot","lit"),
                       labels=c("Empirical", "Literature"),
                      name="Predictor",
                      values = c("purple", "orange"))+
  scale_alpha_manual(breaks=c("survey", "task"),
                     values = c(0.4, 0.7),
                     name="Measure type")+
  scale_color_discrete(name="Measure type")+
  xlab('Variance Explained')+
  ylab('Density')
```

Are the differences in the distributions significant?
There is a significant interaction, such that:

```{r}
tmp = comp_lit_pred_out_both %>%
  select(-.n, -m_lit, -m_boot) %>%
  gather(key, value, -task)

summary(lm(value ~ key*task, tmp))
```

The literature does a better job in predicting itself for tasks compared to surveys (comparing the two orange distributions). 

```{r}
with(comp_lit_pred_out_both, t.test(r2_lit ~ task))
```

Our data is better in predicting the literature estimates for the surveys than it is in predicting tasks (comparing the difference between orange and purple for red and blue outlines).

```{r}
with(comp_lit_pred_out_both, t.test(r2_lit-r2_boot ~ task))
```

Why are the reliability estimates from surveys worse compared to tasks in predicting the literature? Because they are less variable. In this procedure having high variance in what is being predicted (i.e. the literature reliability estimates) is better.

Rank by variance: empirical estimates for surveys < literature estimates for surveys < empirical estimates for tasks < literature estimates for tasks

```{r}
rel_comp %>%
  select(task, retest_reliability, mean_pearson) %>%
  gather(key, value, -task) %>%
  group_by(key, task) %>%
  summarise(variance = var(value)) %>%
  ungroup()%>%
  mutate(key = ifelse(key == "mean_pearson", "emp", "lit")) %>%
  arrange(variance)
```

### LOOCV predicting lit vs our data

What does it look like if you estimate the left one out using the remaining ones for the noise ceiling model?
Sample one row of literature, fit model of sample size+task on remaining data, use that model to predict the reliability estimates, compare those to the left out reliability estimate from the literature versus the average reliability estimate from our data

```{r}
pred_sample <- function(df){
  
  get_sample_row <- function(df){
    if(nrow(df)>1){
      row_num = sample(1:nrow(df),1)
      sample_row = df[row_num,]
    }
    else{
      sample_row = df
    }
    return(sample_row)
  }
  
  test_df = df %>%
    group_by(dv) %>%
    do(get_sample_row(.))
  
  train_df = anti_join(df, test_df)
  
  if(length(unique(train_df$task))>1){
    mod = lm(retest_reliability ~ scale(sample_size)+task, data=train_df)
  }
  else{
    mod = lm(retest_reliability ~ scale(sample_size), data=train_df)
  }
  
  test_df$pred = predict(mod, test_df)
  
  # out = data.frame(res_lit = sum(test_df$retest_reliability - pred)^2,
  #                  res_boot = sum(test_df$mean_pearson - pred)^2)
  
  return(test_df)
}
```

Why did I have to run it separately for the function before but can run it on all data once here? Before the output was model fits for all measures. Here the output is measure level prediction.

```{r message=FALSE}
pred_sample_out = plyr::rdply(1000, pred_sample(rel_comp))
```

Plot difference between prediction and value of either the lit estimate or the average estimate from our data

```{r}
tmp = pred_sample_out %>%
  select(task,pred, retest_reliability, mean_pearson) %>%
  gather(key, value, -task, -pred) %>%
  mutate(sq_diff = (pred - value)^2)

tmp%>%  
  ggplot(aes(sq_diff, fill=key))+
  geom_density(position = "identity", color=NA, alpha=0.5)+
  scale_fill_manual(breaks=c("mean_pearson","retest_reliability"),
                       labels=c("Empirical", "Literature"),
                      name="Predicting",
                      values = c("purple", "orange"))
```

Are the means of these distributions different? Yes: the difference is larger when predicting the left-out value from our data versus predicting left out value from literature. The difference seems pretty small though.

```{r}
with(tmp, t.test(sq_diff ~ key))
```

Plot difference between prediction and value broken down by measure type.

```{r}
tmp%>%
  ggplot(aes(sq_diff, fill=key, color=task, alpha=task))+
  geom_density(position = "identity", size=2)+
  scale_fill_manual(breaks=c("mean_pearson","retest_reliability"),
                       labels=c("Empirical", "Literature"),
                      name="Predicting",
                      values = c("purple", "orange"))+
  scale_alpha_manual(breaks=c("survey", "task"),
                     values = c(0.4, 0.7),
                     name="Measure type")+
  scale_color_discrete(name="Measure type")
```

Can we capture this interaction in a single model? Yes. The significant interaction suggests that the squared difference is larger for predicting the literature compared to predicting our data for surveys while for tasks the squared difference is larger when predicting our data compared to predicting the literature. (This fits with other results too: our results differ from the literature more for the survey measures)

```{r}
aggregate(sq_diff ~ key*task,tmp, FUN=mean)
```

```{r}
summary(lm(sq_diff ~ key*task,tmp))
```

Data versus model prediction plot: The predictions are not veriable so they don't really capture the variability neither in the literature nor in our data very well.

```{r}
tmp %>%
  ggplot(aes(value, pred, color=task))+
  geom_point()+
  facet_wrap(~key)
```

Distributions of sum squared residual: Residuals are larger for tasks compared to surveys (i.e. predictions are worse when predicting task variables - possibly because their variability is higher?). For surveys the predictions of the literature are better than predictions of our data while the opposite is true for task measures.

Does this make sense with the results above? Figure out how to think of the noise ceiling analyses and LOOCV results together.

```{r}
pred_sample_out %>%
  mutate(sq_diff_lit = (pred - retest_reliability)^2,
         sq_diff_emp = (pred - mean_icc)^2) %>%
  group_by(.n, task) %>%
  summarise(sum_sq_diff_lit = sum(sq_diff_lit),
            sum_sq_diff_emp = sum(sq_diff_emp)) %>%
  # select(-'.n') %>%
  gather(key, value, -task, -.n) %>%
  ggplot(aes(value, fill=key, color=task, alpha=task))+
  geom_density(size=1)+
  scale_fill_manual(breaks=c("sum_sq_diff_emp","sum_sq_diff_lit"),
                       labels=c("Empirical", "Literature"),
                      name="Predicting",
                      values = c("purple", "orange"))+
  scale_alpha_manual(breaks=c("survey", "task"),
                     values = c(0.4, 0.7),
                     name="Measure type")+
  scale_color_discrete(name="Measure type")
```

### Measure level comparison

Literature vs. our data on a  measure basis

```{r warning=FALSE, message=FALSE}
p1 = rel_comp %>%
  filter(task == 'task') %>%
ggplot(aes(x = var, y = retest_reliability)) + 
  geom_boxplot(fill='#00BFC4', position="identity")+
  geom_point(aes(x = var, y = mean_pearson),color="purple", fill="purple", size=3, shape=23)+
  coord_flip()+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80"),
        legend.position = 'bottom') + 
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))

p2 = rel_comp %>%
  filter(task == 'survey') %>%
ggplot(aes(x = var, y = retest_reliability)) + 
  geom_boxplot(fill='#F8766D', position="identity")+
  geom_point(aes(x = var, y = mean_pearson),color="purple", fill="purple", size=3, shape=23)+
  coord_flip()+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80"),
        legend.position = 'bottom') + 
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))

p3 <- arrangeGrob(p1, p2,nrow=1)

ggsave('LitVsEmp_Measure_Plot.jpg', plot = p3, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 24, height = 20, units = "in", dpi=100)
rm(p1, p2, p3)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/LitVsEmp_Measure_Plot.jpg')
```

```{r}
rel_comp %>%
  group_by(dv, task) %>%
  summarise(mean_lit = mean(retest_reliability),
            mean_emp = unique(mean_pearson)) %>%
  ggplot(aes(mean_emp, mean_lit, col=task, shape=task))+
  geom_smooth(method="lm")+
  geom_point()+
  xlim(-0.25, 1)+
  ylim(-0.25, 1)+
  ylab("Average Literature Reliability Estimate")+
  xlab("Average Empirical Reliability Estimate")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')

ggsave('LitAndEmpAveCor.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 6, height = 4, units = "in", limitsize = FALSE, dpi = 500)
```

```{r echo=FALSE}
rm(tmp, rel_comp, mod1, mod2, mod3, n_df, i, lit_emp_cor, lit_emp_cor_out, comp_lit_pred, comp_lit_pred_out)
```

## Relationship between reliability metrics (point estimates)

Based on [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) ICC(3,k) does not take in to account within subject differences between two time points (i.e. the fixed effect of time/systematic error). Thus, it is well approximated by Pearson's r and subject to similar criticisms. [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) suggests reporting at least this systematic error effect size if one chooses to report with ICC(3,k). Based on his conclusions here I report:  
- ICC(3,k): As Dave clarified this ranges from 1 to -1/(number of repeated measures -1) so in our case this range would be [-1, 1]; larger values would mean that the two scores of a subject for a given measure are more similar to each other than they are to scores of other people  
- "ICC is reflective of the ability of a test to differentiate between different individuals"
- partial $\eta^2$ for time ($SS_{time}/SS_{within}$): effect size of time   
- SEM ($\sqrt(MS_{error})$): standard error of measurement; the smaller the better. It 
"quantifies the precision of individual scores on a test" and is not dependent on the sample in the way ICC is since it doesn't depend on between subject reliability (at least in this formulation) but is unit-dependent.

We calculated `r as.numeric(table(rel_df$task)[1])` measures for surveys and `r as.numeric(table(rel_df$task)[2])` measures for cognitive tasks.  

Though we are primarily reporting ICC's as our metric of reliability the results don't change depending on the metric chosen. Here we plot point estimates of three different reliability metrics against each other (ICC, Pearson, Spearman). The bootstrapped version is essentially the same but the plots are busier due to more datapoints.


```{r}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i]) & names(test_data)[i] %in% names(retest_data)){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}
#Create df of point estimate reliabilities
rel_df <- data.frame(spearman = rep(NA, length(numeric_cols)),
                     icc = rep(NA, length(numeric_cols)),
                     pearson = rep(NA, length(numeric_cols)),
                     partial_eta_sq = rep(NA, length(numeric_cols)),
                     sem = rep(NA, length(numeric_cols)),
                     var_subs = rep(NA, length(numeric_cols)),
                     var_ind = rep(NA, length(numeric_cols)),
                     var_resid = rep(NA, length(numeric_cols)))

row.names(rel_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  rel_df[numeric_cols[i], 'spearman'] <- get_spearman(numeric_cols[i]) 
  rel_df[numeric_cols[i], 'icc'] <- get_icc(numeric_cols[i])
  rel_df[numeric_cols[i], 'pearson'] <- get_pearson(numeric_cols[i])
  rel_df[numeric_cols[i], 'partial_eta_sq'] <- get_partial_eta(numeric_cols[i])
  rel_df[numeric_cols[i], 'sem'] <- get_sem(numeric_cols[i])
  rel_df[numeric_cols[i], 'var_subs'] <- get_var_breakdown(numeric_cols[i])$subs
  rel_df[numeric_cols[i], 'var_ind'] <- get_var_breakdown(numeric_cols[i])$ind
  rel_df[numeric_cols[i], 'var_resid'] <- get_var_breakdown(numeric_cols[i])$resid
}

rel_df$dv = row.names(rel_df)
row.names(rel_df) = seq(1:nrow(rel_df))
rel_df$task = 'task'
rel_df[grep('survey', rel_df$dv), 'task'] = 'survey'
rel_df[grep('holt', rel_df$dv), 'task'] = "task"
rel_df = rel_df %>%
  select(dv, task, spearman, icc, pearson, partial_eta_sq, sem, var_subs, var_ind, var_resid)
# row.names(rel_df) = NULL

```

```{r}
table(rel_df$task)
```

```{r warning=FALSE, message=FALSE}
p1 = rel_df %>%
  ggplot(aes(spearman, icc, col=task))+
  geom_point()+
  theme_bw()+
  theme(legend.title = element_blank(),
        legend.position = "none")+
  geom_abline(intercept = 0, slope=1)

p2 = rel_df %>%
  ggplot(aes(pearson, icc, col=task))+
  geom_point()+
  theme_bw()+
  theme(legend.title = element_blank(),
        legend.position = "none")+
  geom_abline(intercept = 0, slope=1)

p3 = rel_df %>%
  ggplot(aes(pearson, spearman, col=task))+
  geom_point()+
  theme_bw()+
  theme(legend.title = element_blank(),
        legend.position = "none")+
  geom_abline(intercept = 0, slope=1)

grid.arrange(p1, p2, p3, nrow=1)

ggsave('Metric_Scatterplots.jpg', plot = grid.arrange(p1, p2, p3, nrow=1), device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 12, height = 4, units = "in", limitsize = FALSE, dpi = 100)
```

As the scatter plots depict the correlations between different types reliability metrics were very high.

```{r}
cor(rel_df[,c('spearman', 'icc', 'pearson')])
```

```{r echo=FALSE}
rm(p1,p2,p3)
```

Note: Some variables have <0 ICC's. This would be the case if the $MS_{error}$>$MS_{between}$. Data for these variables have no relationship between the two time points.

## Summary of all measure reliabilities

Summarized bootstrapped reliabilities

```{r message=FALSE, warning=FALSE}
boot_df %>%
  group_by(dv) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            spearman_median = quantile(spearman, probs = 0.5),
            spearman_2.5 = quantile(spearman, probs = 0.025),
            spearman_97.5 = quantile(spearman, probs = 0.975)) %>%
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5', 'spearman_median', 'spearman_2.5', 'spearman_97.5'), digits=3)
```

```{r echo=FALSE}
measure_labels <- read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/measure_labels.csv')
measure_labels = measure_labels %>% select(-measure_description)
# Check if there are any missing variables
# retest_report_vars[(retest_report_vars %in% measure_labels$dv == FALSE)]
# measure_labels$dv[(measure_labels$dv %in% retest_report_vars == FALSE)]
```

```{r warning=FALSE, message=FALSE}
# Df wrangling for plotting
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  left_join(boot_df[,c("dv", "icc", "spearman")], by = 'dv') 

tmp = tmp %>%
  separate(dv, c("task_group", "var"), sep="\\.",remove=FALSE,extra="merge") %>%
  mutate(task_group = factor(task_group, levels = task_group[order(task)])) %>%
  separate(var, c("var"), sep="\\.",remove=TRUE,extra="drop") %>%
  mutate(task_group = gsub("_", " ", task_group),
         var = gsub("_", " ", var)) %>%
  arrange(task_group, var)

tmp = tmp %>%
  left_join(rel_df[,c("dv", "icc")], by = "dv") %>%
  rename(icc = icc.x, point_est = icc.y)

#Manual correction
tmp = tmp %>%
  mutate(task = ifelse(task_group == 'holt laury survey', "task", as.character(task))) %>%
  mutate(task_group = gsub("survey", "", task_group),
         task_group = gsub("task", "", task_group),
         task_group = str_to_title(task_group)) %>%
  mutate(task_group = ifelse(task_group == "Psychological Refractory Period Two Choices", "Psychological Refractory Period", ifelse(task_group == "Angling Risk Always Sunny", "Angling Risk", ifelse(task_group == "Two Stage", "Two Step", ifelse(task_group == "Threebytwo", "Task Switching", ifelse(task_group == "Adaptive N Back", "Adaptive N-back", ifelse(task_group == "Go Nogo", "Go/No-go",  ifelse(task_group == "Ravens", "Raven's", task_group)))))))) %>%
  mutate(task_group = ifelse(task_group == "Bis Bas ", "BIS-BAS", ifelse(task_group == "Bis11 ", "BIS-11", ifelse(task_group == "Dospert Eb ", "DOSPERT EB", ifelse(task_group == "Dospert Rp ", "DOSPOERT RP", ifelse(task_group == "Dospert Rt ", "DOSPERT RT", ifelse(task_group == "Erq ", "ERQ", ifelse(task_group == "Upps Impulsivity ", "UPPS-P", task_group))))))))
```

Extract trial number info to add to boot plot instead of adding another table

```{r}
trial_num_info = tmp %>%
  group_by(task_group, task) %>%
  summarise(mean_num_all_trials = round(mean(num_all_trials)),
            num_measures = length(unique(dv)))

tmp = tmp %>%
  left_join(trial_num_info, by="task_group")
```

Boot plot for tasks with trial info

```{r warning=FALSE, message=FALSE}
p4_t <- tmp %>%
  filter(task == 'task') %>%
ggplot(aes(x = factor(task_group, levels=rev(unique(task_group))), y = icc)) + 
  geom_violin(fill='#00BFC4')+
  theme_bw() + 
  theme(axis.text = element_text(size=30))+
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1), position = "right")+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15))+
  geom_hline(yintercept = 0, color = "red", size = 1)+
  coord_flip()

task_trial_num = trial_num_info %>% 
  filter(task == "task") %>% 
  ungroup() %>% 
  select(-task)

task_trial_num_table = task_trial_num %>%
  mutate(y_axis = rev(ggplot_build(p4_t)$layout$panel_params[[1]]$y.major)) %>%
  gather(key, value, -y_axis, -task_group) %>%
  ggplot(aes(key, factor(y_axis)))+
  geom_text(aes(label=value), size=14)+
  xlab("")+
  ylab("")+
  theme(axis.ticks=element_blank(),
        axis.text.y = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.text.x = element_text(size=30))+
  scale_x_discrete(position = "top",
                   breaks=c("mean_num_all_trials", "num_measures"),
                   labels=c("Mean Num Trials", "Num Measures"))

boot_task_plot = arrangeGrob(p4_t, task_trial_num_table, nrow=1, widths = c(3,2))

ggsave('Task_Boot_w_trialinfo.jpg', plot = boot_task_plot, device = "jpeg", path = "../output/figures/", width = 20, height = 48, units = "in", limitsize = FALSE, dpi = 100)
  
```

Boot plot for surveys with trial info

```{r}
p5_t <- tmp %>%
  filter(task == 'survey') %>%
ggplot(aes(x = factor(task_group, levels=rev(unique(task_group))), y = icc)) + 
  geom_violin(fill='#F8766D')+
  theme_bw() + 
  theme(axis.text = element_text(size=30))+
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1), position = "right")+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10))+
  geom_hline(yintercept = 0, color = "red", size = 1)+
  coord_flip()

survey_trial_num = trial_num_info %>% 
  filter(task == "survey") %>% 
  ungroup() %>% 
  select(-task)

survey_trial_num_table = survey_trial_num %>%
  mutate(y_axis = rev(ggplot_build(p5_t)$layout$panel_params[[1]]$y.major)) %>%
  gather(key, value, -y_axis, -task_group) %>%
  ggplot(aes(key, factor(y_axis)))+
  geom_text(aes(label=value), size=14)+
  xlab("")+
  ylab("")+
  theme(axis.ticks=element_blank(),
        axis.text.y = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.text.x = element_text(size=30))+
  scale_x_discrete(position = "top",
                   breaks=c("mean_num_all_trials", "num_measures"),
                   labels=c("Mean Num Trials", "Num Measures"))

boot_survey_plot = arrangeGrob(p5_t, survey_trial_num_table, nrow=1, widths = c(3,2))

ggsave('Survey_Boot_w_trialinfo.jpg', plot = boot_survey_plot, device = "jpeg", path = "../output/figures/", width = 20, height = 48, units = "in", limitsize = FALSE, dpi = 100)
```

Both task level boot plots with trial info together

```{r}
boot_both_w_trial = arrangeGrob(boot_task_plot, boot_survey_plot, nrow=1)

ggsave('Boot_Both_w_trialinfo.jpg', plot = boot_both_w_trial, device = "jpeg", path = "../output/figures/", width = 38, height = 48, units = "in", limitsize = FALSE, dpi = 100)
```

Variable level summary of bootstrapped reliabilities.

```{r warning=FALSE, message=FALSE} 
p4 <- tmp %>%
  filter(task == 'task',
         raw_fit == 'raw') %>%
ggplot(aes(y = var, x = icc)) + 
  geom_point(color = '#00BFC4')+
  geom_point(aes(y = var, x = point_est), color = "black")+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y", labeller = label_wrap_gen(width=20)) +
  theme(panel.spacing = unit(0.75, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180, size=36),
        axis.text.y = element_text(size=20),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80")) + 
  xlab("")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_y_discrete(labels = function(x) str_wrap(x, width = 10))+
  geom_vline(xintercept = 0, color = "red", size = 1)

p5 <- tmp %>%
  filter(task == 'survey') %>%
ggplot(aes(y = var, x = icc)) + 
  geom_point(color = '#F8766D')+
  geom_point(aes(y = var, x = point_est), color = "black")+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y", labeller = label_wrap_gen(width=20)) +
  theme(panel.spacing = unit(0.75, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180, size=36),
        axis.text.y = element_text(size=20),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey80")) + 
  xlab("")+
  ylab("")+
  scale_x_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_y_discrete(labels = function(x) str_wrap(x, width = 10))+
  geom_vline(xintercept = 0, color = "red", size = 1)
  
p6 <- arrangeGrob(p4, p5,nrow=1)

ggsave('Bootstrap_Raw_Var_Plot.jpg', plot = p6, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 36, height = 72, units = "in", limitsize = FALSE, dpi=50)

rm(p4, p5, p6)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/Bootstrap_Raw_Var_Plot.jpg')
```

Example of the overlaying procedure.

```{r}
p1<- tmp %>%
  filter(grepl('selection_optimization', dv)) %>%
ggplot(aes(x = var, y = icc)) + 
  geom_violin(fill='#F8766D')+
  theme_bw() + 
  theme(axis.text.y = element_text(size=30))+
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15))+
  geom_hline(yintercept = 0, color = "red", size = 1)+
  coord_flip()

p2<- tmp %>%
  filter(grepl('selection_optimization', dv)) %>%
ggplot(aes(x = factor(task_group, levels=rev(unique(task_group))), y = icc, group=dv)) + 
  geom_violin(fill='#F8766D', position = 'identity')+
  theme_bw() + 
  theme(axis.text.y = element_text(size=30))+
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15))+
  geom_hline(yintercept = 0, color = "red", size = 1)+
  coord_flip()

p3<- tmp %>%
  filter(grepl('selection_optimization', dv)) %>%
ggplot(aes(x = factor(task_group, levels=rev(unique(task_group))), y = icc)) + 
  geom_violin(fill='#F8766D', position = 'identity')+
  theme_bw() + 
  theme(axis.text.y = element_text(size=30))+
  xlab("")+
  ylab("")+
  scale_y_continuous(limits = c(-0.25,1), breaks=c(-0.25, 0, 0.25, 0.5, 0.75, 1))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15))+
  geom_hline(yintercept = 0, color = "red", size = 1)+
  coord_flip()

p4 <- arrangeGrob(p1,p2, p3,nrow=1)

ggsave('Bootstrap_Example_Plot_t.jpg', plot = p4, device = "jpeg", path = "../output/figures/", width = 41, height = 5, units = "in", limitsize = FALSE, dpi = 100)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/Bootstrap_Example_Plot_t.jpg')
```

```{r}
rm(p1, p2, p3, p4)
```

## Survey vs Tasks

Comparison of survey measures to cognitive task measures in the bootstrapped results. Multilevel model with random intercepts for each measure and fixed effect of survey versus cognitive measure. 

```{r}
boot_df = boot_df %>%
    mutate(task = ifelse(grepl("survey",dv), "survey","task"),
           task = ifelse(grepl("holt",dv), "task", task))

boot_df %>%
  group_by(task) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            spearman_median = quantile(spearman, probs = 0.5),
            spearman_2.5 = quantile(spearman, probs = 0.025),
            spearman_97.5 = quantile(spearman, probs = 0.975),
            num_vars = n()/1000) %>%
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5', 'spearman_median', 'spearman_2.5', 'spearman_97.5'), digits=3)
```


```{r eval=FALSE}
#summary(lmerTest::lmer(icc ~  task + (1|dv), boot_df))
# m = MCMCglmm(icc ~  task, random = ~dv, data=boot_df, nitt = 1300, burnin = 300)
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/icc_by_task_model.rds')
summary(m)
```

### Variance breakdown

The quantiative explanation for the difference in reliability estimates between surveys and tasks, as recently detailed by Hedge et al. (2017), lies in the difference in sources of variance between these measures. Specifically, the ICC is calculated as the ratio of variance between subjects variance to all sources of variance. Thus, measures with high between subjects variance would have high test-retest reliability. Intuitively, measures with high between subjects variance are also better suited for individual difference analyses as they would capture the differences between the subjects in a sample.

Here we first plot the percentage of variance explained by the three sources of variance for the point estimates of measure reliabilities. The plot only includes raw measures (no DDM parameters) and the measures are ranked by percentage of between subject variability for each task/survey (i.e. the best to worst individual difference measure for each task/survey). Then we compare statistically whether the percentage of variance explained by these sources differ between tasks and surveys.

```{r warning=FALSE, message=FALSE}
tmp = rel_df %>%
  mutate(var_subs_pct = var_subs/(var_subs+var_ind+var_resid)*100,
         var_ind_pct = var_ind/(var_subs+var_ind+var_resid)*100, 
         var_resid_pct = var_resid/(var_subs+var_ind+var_resid)*100) %>%
  select(dv, task, var_subs_pct, var_ind_pct, var_resid_pct) %>%
  mutate(dv = factor(dv, levels = dv[order(task)])) %>%
  separate(dv, c("task_group", "var"), sep="\\.",remove=FALSE,extra="merge") %>%
  mutate(task_group = factor(task_group, levels = task_group[order(task)])) %>%
  arrange(task_group, var_subs_pct) %>%
  mutate(rank = row_number()) %>%
  arrange(task, task_group, rank) %>%
  gather(key, value, -dv, -task_group, -var, -task, -rank) %>%
  ungroup()%>%
  mutate(task_group = gsub("_", " ", task_group),
         var = gsub("_", " ", var)) %>%
  mutate(task_group = ifelse(task_group == "psychological refractory period two choices", "psychological refractory period", ifelse(task_group == "angling risk task always sunny", "angling risk task",task_group))) %>%
  mutate(task_group = gsub("survey", "", task_group)) %>%
  filter(task=="task",
         !grepl("EZ|hddm", dv))%>%
  arrange(task_group, rank)
labels = tmp %>%
  distinct(dv, .keep_all=T)

p1 <- tmp %>%
  ggplot(aes(x=factor(rank), y=value, fill=factor(key, levels = c("var_resid_pct", "var_ind_pct", "var_subs_pct"))))+
  geom_bar(stat='identity', alpha = 0.75, color='#00BFC4')+
  scale_x_discrete(breaks = labels$rank,
                       labels = labels$var)+
  coord_flip()+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey85"),
        legend.position = 'bottom')+
  theme(legend.title = element_blank())+
  scale_fill_manual(breaks = c("var_subs_pct", "var_ind_pct", "var_resid_pct"),
                      labels = c("Variance between individuals",
                                 "Variance between sessions",
                                 "Error variance"),
                  values=c("grey65", "grey45", "grey25"))+
  ylab("")+
  xlab("")

tmp = rel_df %>%
  mutate(var_subs_pct = var_subs/(var_subs+var_ind+var_resid)*100,
         var_ind_pct = var_ind/(var_subs+var_ind+var_resid)*100, 
         var_resid_pct = var_resid/(var_subs+var_ind+var_resid)*100) %>%
  select(dv, task, var_subs_pct, var_ind_pct, var_resid_pct) %>%
  mutate(dv = factor(dv, levels = dv[order(task)])) %>%
  separate(dv, c("task_group", "var"), sep="\\.",remove=FALSE,extra="merge") %>%
  mutate(task_group = factor(task_group, levels = task_group[order(task)])) %>%
  arrange(task_group, var_subs_pct) %>%
  mutate(rank = row_number()) %>%
  arrange(task, task_group, rank) %>%
  gather(key, value, -dv, -task_group, -var, -task, -rank) %>%
  ungroup()%>%
  mutate(task_group = gsub("_", " ", task_group),
         var = gsub("_", " ", var)) %>%
  mutate(task_group = ifelse(task_group == "psychological refractory period two choices", "psychological refractory period", ifelse(task_group == "angling risk task always sunny", "angling risk task",task_group))) %>%
  mutate(task_group = gsub("survey", "", task_group)) %>%
  filter(task=="survey")%>%
  arrange(task_group, rank)
labels = tmp %>%
  distinct(dv, .keep_all=T)

p2 <- tmp %>%
  ggplot(aes(x=factor(rank), y=value, fill=factor(key, levels = c("var_resid_pct", "var_ind_pct", "var_subs_pct"))))+
  geom_bar(stat='identity', alpha = 0.75)+
  geom_bar(stat='identity', color='#F8766D', show.legend=FALSE)+
  scale_x_discrete(breaks = labels$rank,
                       labels = labels$var)+
  coord_flip()+
  facet_grid(task_group~., switch = "y", scales = "free_y", space = "free_y") +
  theme(panel.spacing = unit(0.5, "lines"), 
        strip.placement = "outside",
        strip.text.y = element_text(angle=180),
        panel.background = element_rect(fill = NA),
        panel.grid.major = element_line(colour = "grey85"),
        legend.position = 'bottom')+
  theme(legend.title = element_blank())+
  scale_fill_manual(breaks = c("var_subs_pct", "var_ind_pct", "var_resid_pct"),
                      labels = c("Variance between individuals",
                                 "Variance between sessions",
                                 "Error variance"),
                  values=c("grey65", "grey45", "grey25"))+
  ylab("")+
  xlab("")

mylegend<-g_legend(p2)

p3 <- arrangeGrob(arrangeGrob(p1 +theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(10, 1))

ggsave('Variance_Breakdown_Plot.jpg', plot = p3, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures", width = 24, height = 20, units = "in", dpi = 100)
rm(tmp, labels, p1, p2 , p3)
```

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/Variance_Breakdown_Plot.jpg')
```

Comparing types of variance for survey vs task measures: Survey measures have higher between subject variability  

Note: This analysis includes DDM variables too.

Running separate models for different sources of variance because interactive model with variance type*task seemed too complicated.

First we find that task measures have a smaller percentage of their overall variance explained by variability between subjects compared to survey measures.

```{r}
tmp = boot_df %>%
  mutate(var_subs_pct = var_subs/(var_subs+var_ind+var_resid)*100,
         var_ind_pct = var_ind/(var_subs+var_ind+var_resid)*100, 
         var_resid_pct = var_resid/(var_subs+var_ind+var_resid)*100) %>%
  select(dv, task, var_subs_pct, var_ind_pct, var_resid_pct) 

# summary(lmerTest::lmer(var_subs_pct~task+(1|dv),tmp%>%select(-var_ind_pct,-var_resid_pct)))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/var_subs_pct_by_task_model.rds')
summary(m)
```

We also find that a significantly larger percentage of their variance is explained by between session variability. Larger between session variability suggests systematic differences between the two sessions. Such systematic effects can be due to e.g. learning effects as explored later.

```{r}
# summary(lmerTest::lmer(var_ind_pct~task+(1|dv),tmp%>%select(-var_subs_pct,-var_resid_pct)))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/var_ind_pct_by_task_model.rds')
summary(m)
```

```{r}
# summary(lmerTest::lmer(var_resid_pct~task+(1|dv),tmp%>%select(-var_subs_pct,-var_ind_pct)))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/var_resid_pct_by_task_model.rds')
summary(m)
```

```{r}
tmp_save = tmp%>%
  gather(key, value, -dv, -task) %>%
  group_by(task, key) %>%
  summarise(median = median(value),
            sd = sd(value)) %>%
  mutate(key = ifelse(key == 'var_ind_pct', 'Between session variance', ifelse(key == 'var_subs_pct', 'Between subjects variance', ifelse(key == 'var_resid_pct', 'Residual variance',NA)))) %>%
  rename(Median = median, SD = sd) %>%
  arrange(task, key)

tmp_save
```

```{r}
sjt.df(tmp_save %>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/var_breakdown_summary.doc")
```

Summarizing for clearer presentation. This graph is currently using the bootstrapped reliabilities and is therefore messier than if just using the point estimates.

```{r}
tmp %>%
  gather(key, value, -dv, -task) %>%
  group_by(task, key) %>%
  summarise(mean_pct = mean(value),
            sd_pct = sd(value, na.rm=T),
            n = n()) %>%
  mutate(cvl = qt(0.025, n-1),
         cvu = qt(0.975, n-1),
         cil = mean_pct+(sd_pct*cvl)/sqrt(n),
         ciu = mean_pct+(sd_pct*cvu)/sqrt(n),
         sem_pct = sd_pct/sqrt(n)) %>%
  ggplot(aes(factor(key, levels = c("var_subs_pct", "var_ind_pct", "var_resid_pct"),
                    labels = c("Between subject variance",
                               "Within subject variance",
                               "Error variance")), mean_pct))+
  # geom_point(position=position_dodge(width = 0.25), size = 3, aes(color=task))+
  geom_bar(position=position_dodge(width = 0.5), width=0.5, aes(fill=task), stat='identity', alpha=0.5)+
  # geom_errorbar(aes(ymin=mean_pct-sd_pct, ymax=mean_pct+sd_pct), position=position_dodge(width = 0.25), width=0, size=2)+
  geom_errorbar(aes(ymin=cil, ymax=ciu, color=task), position=position_dodge(width = 0.5), width=0.1)+
  theme_bw()+
  xlab('')+
  ylab('Percent of total variance')+
  theme(legend.title = element_blank(),
        legend.text = element_text(size=14),
        # legend.position = 'bottom',
        axis.text = element_text(size=12),
        axis.title.y = element_text(size=12))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15))+
  ylim(0,100)

ggsave('Variance_Breakdown_BarPlot.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures", width = 6, height = 5, units = "in", dpi = 300)
```

## Systematic effects between time points

### Anova time effects

The type of ICC we have chosen does not take within subject (between session/systematic) variance in to account. This is why Weir recommends checking whether there is a significant change based on time and examining the SEMs. These systematic effects could be meaningful and important to account for for some measures (e.g. task measures that show learning effects).

Had we chosen another kind of ICC taking this source of variance into account (e.g. 2,1 or 2,k) they could have suggested that tasks have lower reliability.

Doing a simple t-test on the difference score alone would not be a very rigourous way of testing whether any change is meaningful because two distributions from both time points with error would be compared to each other. Fortunately there are ways to take the error for both measurements in to account. 

To check whether a measure shows systematic differences between the two time points in a meaningful number of bootstrapped samples we can: check if the effect of time is significant in each bootstrapped sample and filter variables that have more than 5% of the boostrapped samples showing significant time effects.

Another way might be to compute confidence intervals using SEMs as described in the second half of Weir (2005) and check what percent of participants have scores that fall out of this range.

First we ask: Which variables have significant time effects in more than 5% of the bootstrapped samples?

23/74 survey measures
133/372 task measures

```{r}
boot_df %>%
  select(dv, p_time, task) %>%
  mutate(time_effect_sig = ifelse(p_time<0.05,1,0)) %>%
  group_by(dv)%>%
  summarise(pct_sig_time_effect = sum(time_effect_sig)/10,
            task = unique(task))%>%
  filter(pct_sig_time_effect>5) %>%
  arrange(task,-pct_sig_time_effect) %>%
  ungroup()%>%
  group_by(task) %>%
  summarise(count=n()) %>%
  mutate(total_num_vars = c(74, 372),
         prop = count/total_num_vars) %>%
  ggplot(aes(task, prop, fill=task))+
  geom_bar(stat="identity", alpha=0.5, width=0.5)+
  theme(legend.position = "none", 
        axis.text.x = element_text(size=14),
        axis.title.y = element_text(size=14))+
  ylab("Proportiono of bootstrap samples")+
  xlab("")+
  ylim(0,1)

ggsave('PropTimeEffects.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 3, height = 5, units = "in", limitsize = FALSE, dpi = 300)

```

Are these significantly different between surveys and tasks? No.

```{r}
chisq.test(matrix(data = c(23,74-23,133, 372-133), nrow=2))
```

```{r}
boot_df %>%
  select(dv, p_time, task, pearson) %>%
  mutate(time_effect_sig = ifelse(p_time<0.05,1,0)) %>%
  group_by(dv)%>%
  summarise(pct_sig_time_effect = sum(time_effect_sig)/10,
            task = unique(task),
            mean_pearson = mean(pearson))%>%
  filter(pct_sig_time_effect>5) %>%
  arrange(task,-pct_sig_time_effect) %>%
  
```

### SEM CI calculation

Step 1: T = Grand mean + ICC * (Subject score - Grand mean)
Step 2: SEP = SD of both measurements * sqrt(1-ICC^2)

Here I calculate the proportion of subjects that move more than one standard error of prediction (SEP) in t2 compared to t1 for each measure. 

This is odd to think about because the larger the ICC of a measure the smaller the SEP. So very small differences between the two time points can be categorized as 'meaningful' based on the tiny SEP.

What you are interested in is not necessarily whether individuals change at all between the two time points though. You want to know if this change is systematic in one direction.

Here I calculate the proportion of people showing 'meaningful' changes in one direction or the other. To integrate both of these direction I subtracted one from the other and filtered the variables that have more than 5% of the participants showing meaningful change in one direction over the other (so if a variable has 10 participants showing difference in one and 10 in the other direction this would cancel out but a variables with 15 people showing a positive and 5 negative change would remain).


```{r}
get_ind_ci = function(dv_var){
  matched = match_t1_t2(dv_var)
  grand_mean = mean(matched$score)
  grand_sd = sd(matched$score)
  dv_icc = get_icc(dv_var)
  sep = grand_sd * sqrt(1-(dv_icc^2))
  matched = matched %>% 
    spread(time, score) %>%
    rename("t1"="1", "t2"="2") %>%
    mutate(true_score = grand_mean+(dv_icc*(t1-grand_mean)),
           ci_up = true_score+(1.96*sep),
           ci_low = true_score-(1.96*sep),
           t2_above_ci = ifelse(t2>ci_up,1,0),
           t2_below_ci = ifelse(t2<ci_low,1, 0))
    return(matched)
}

get_prop_out_ci = function(dv_var){
  get_ind_ci(dv_var) %>%
    summarise(prop_above_ci = sum(t2_above_ci)/n(),
              prop_below_ci = sum(t2_below_ci/n()))
}

#Create df of point estimate reliabilities
ind_ci_df <- data.frame(prop_above_ci = rep(NA, length(numeric_cols)),
                        prop_below_ci = rep(NA, length(numeric_cols)))

row.names(ind_ci_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
    ind_ci_df[numeric_cols[i], 'prop_above_ci'] <- get_prop_out_ci(numeric_cols[i])$prop_above_ci 
    ind_ci_df[numeric_cols[i], 'prop_below_ci'] <- get_prop_out_ci(numeric_cols[i])$prop_below_ci 
}

ind_ci_df$dv = row.names(ind_ci_df)
row.names(ind_ci_df) = seq(1:nrow(ind_ci_df))
ind_ci_df$task = 'task'
ind_ci_df[grep('survey', ind_ci_df$dv), 'task'] = 'survey'
ind_ci_df[grep('holt', ind_ci_df$dv), 'task'] = "task"
ind_ci_df = ind_ci_df %>%
  select(dv, task, prop_above_ci, prop_below_ci)

mean_diff_df = ind_ci_df %>%
  mutate(prop_one_direction = prop_above_ci-prop_below_ci) %>%
  filter(abs(prop_one_direction)>0.05) %>%
  arrange(-abs(prop_one_direction))

mean_diff_df
```

This doesn't make it easier to interpret whether it is a performance improvement or decline. In fact 'performance' isn't even necessarily the correct term here. Using the valence assignments of the measures look at whether it translates to an increase or decrease in self-regulation. Of the 66 this makes sense for 54 variables.

```{r}
valence_df = read.csv(paste0(retest_data_path, 'DV_valence.csv'), header = F)
valence_df = valence_df %>% rename(dv = V1, valence = V2)
valence_df$task = 'task'
valence_df[grep('survey', valence_df$dv), 'task'] = 'survey'
valence_df[grep('holt', valence_df$dv), 'task'] = "task"

valence_df = valence_df %>% filter(dv %in% retest_report_vars)
with(valence_df, table(task, valence))
#Comparable level of valences for task and survey measures
chisq.test(with(valence_df, table(task, valence)))
```

```{r warning=FALSE, message=FALSE}

mean_diff_df %>%
  left_join(valence_df, by = c("dv", "task")) %>%
  mutate(sc_increase = ifelse(prop_one_direction>0&valence==1,1,ifelse(prop_one_direction<0&valence==-1,1,0)),
         sc_decrease = ifelse(prop_one_direction>0&valence==-1,1,ifelse(prop_one_direction<0&valence==1,1,0))) %>%
  group_by(task) %>%
summarise(sum_sc_increase = sum(sc_increase,na.rm=T),
          sum_sc_decrease = sum(sc_decrease,na.rm=T)) %>%
  gather(key, value, -task) %>%
  mutate(total_vars = c(74, 372, 74, 372),
         prop = value/total_vars) %>%
  ggplot(aes(key, prop, group=task, fill=task))+
  geom_bar(stat = "identity", position = position_dodge(), alpha = 0.5)+
  theme(legend.title = element_blank(),
        axis.title.y = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("Proportion of measures")+
  ylim(0, 1)+
  scale_x_discrete(breaks = c("sum_sc_decrease", "sum_sc_increase"),
                   labels = c("Self-control decrease", "Self-control increase"))

ggsave('PropTimeEffectsValence.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 7, height = 5, units = "in", limitsize = FALSE, dpi = 300)
```

Variables that show more than 5% difference in a direction do not depend on whether they translate to a self control increase or decrease.

```{r}
chisq.test(matrix(data = c(23,54-23,31,54-31), nrow=2))
```

Do they differ depending on whether they are task or survey variables? No.

```{r}
chisq.test(matrix(data = c(5,74-5,60,372-60), nrow=2))
```

```{r}
rm(mean_diff_df)
```

## Task Reliabilities

Here we summarize the results on a task level to make it more digestable and easier to make contact with the literature.  

We reduce the list of task measures to a list of one per task by averaging only the measures that are extracted and used from these tasks in the literature. We call these the 'meaningful measures.'

```{r}
meaningful_vars = read.table('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/meaningful_vars.txt')
meaningful_vars = as.character(meaningful_vars$V1)

tmp = as.character(unique(lit_review$dv)[which(unique(lit_review$dv) %in% meaningful_vars == FALSE)])
tmp = tmp[-grep('survey', tmp)]

meaningful_vars = c(meaningful_vars, tmp)
meaningful_vars = sort(meaningful_vars)
```

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2) %>%
  group_by(task_name) %>%
  summarise(median_icc = median(icc),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_measures = n()/1000,
            mean_num_trials = round(mean(num_all_trials)))%>%
  arrange(-median_icc)

tmp %>%
  datatable() %>%
  formatRound(columns=c('median_icc','icc_2.5','icc_97.5'), digits=3)
```


```{r}
tmp = tmp%>%
  mutate(task_name = gsub("_", " ", task_name),
         task_name = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", task_name, perl=TRUE))

names(tmp) = gsub("_", " ", names(tmp))
names(tmp) = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", names(tmp), perl=TRUE)

sjt.df(tmp %>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/task_rel_table.doc")
```

### Number of trials

Does number of items in a task have a significant effect on the average ICC of meaningful measures for all trials from a task? No.

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2) 

# summary(lm(icc ~ num_all_trials, data = tmp))
# summary(lmerTest::lmer(icc ~ num_all_trials + (1|dv), data = tmp))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/icc_by_num_trials_model.rds')
summary(m)
```

```{r warning=FALSE, message=FALSE}
tmp %>%
  ggplot(aes(num_all_trials, icc))+
  geom_point()+
  geom_smooth(method="lm")+
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")
```

#### Trial number dependence intrameasure 

The above analysis was looking at the effect of number of trials across tasks. But some tasks might be bad for individual difference measurement regardless of how many trials there are in them whereas for others fewer trials might be yielding a sufficiently reliable measure.

For tasks for which dependent variables are estimated using many trials one can ask: Does the same measure get less reliable if fewer trials are used to estimate its reliability?

This won't make sense for all tasks. For example to estimate a risk aversion parameter you need all trials for Holt and Laury. For Kirby and Bickel you have specific conditions looking at fewer trials. The Cognitive Reflection Task might be more appropriate to analyze each item seaprately. The writing task does not have trial numbers. For all others it might be interesting to investigate.

These kinds of analyses are too task-specific and in-depth for a paper that is trying to give a global sense of the differences between self-regulation measures in their suitablity for individual difference analyses based on their stability across time. Such analyses would provide a detailed examination of how to extract the most reliable/best individual difference measure from tasks with a set of mediocre variables to begin with. Though we do not provide such a comprehensive analysis of this sort in this paper we provide a single example of this approach and hope the open access we provide to the data spurs further work.

For this example we look at the retest reliability of dependent measures from the threebytwo with >400 trials. Here is a graph of how the point estimates of the retest reliability changes for each of the dependent measures using different numbers of trials to estimate them. 

Post process dv's from cluster to calculate reliabilities
```{r}
t1_dvs = read.csv(paste0(retest_data_path, 't1_shift_dvs.csv'))
t2_dvs = read.csv(paste0(retest_data_path, 't2_shift_dvs.csv'))

t1_dvs = t1_dvs %>% select(-X)
t2_dvs = t2_dvs %>% select(-X)
```

```{r eval=FALSE}
hr_merge = merge(t1_dvs, t2_dvs, by = c("worker_id", "level_1"))

hr_merge = hr_merge %>%
  gather(key, value, -worker_id, -level_1) %>%
  separate(key, c("dv", "time"), sep="\\.") %>%
  mutate(time = ifelse(time == "x", 1, 2))%>%
  rename(sub_id = worker_id,
         breaks = level_1)

t1_dvs = hr_merge %>%
  filter(time == 1) %>%
  select(-time) %>%
  spread(dv, value)

t2_dvs = hr_merge %>%
  filter(time == 2) %>%
  select(-time) %>%
  spread(dv, value)

# calculate point estimates for reliability of each of the variables for each break
# get_icc for each break of tmp_t1_dvs and tmp_t2_dvs

trial_num_rel_df = data.frame(breaks=rep(NA, length(unique(t1_dvs$breaks))),
                              acc_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              avg_rt_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              conceptual_responses_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              fail_to_maintain_set_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              learning_rate_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              learning_to_learn_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              nonperseverative_errors_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              perseverative_errors_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              perseverative_responses_icc=rep(NA, length(unique(t1_dvs$breaks))),
                              total_errors_icc=rep(NA, length(unique(t1_dvs$breaks))))

for(i in 1:length(unique(t1_dvs$breaks))){
  cur_break = unique(t1_dvs$breaks)[i]
  tmp_t1_dvs = t1_dvs %>% filter(breaks == cur_break)
  tmp_t2_dvs = t2_dvs %>% filter(breaks == cur_break)
  trial_num_rel_df$breaks[i] = cur_break
  trial_num_rel_df$acc_icc[i] = get_icc("acc", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$avg_rt_icc[i] = get_icc("avg_rt", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$conceptual_responses_icc[i] = get_icc("conceptual_responses", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$fail_to_maintain_set_icc[i] = get_icc("fail_to_maintain_set", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$learning_rate_icc[i] = get_icc("learning_rate", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$learning_to_learn_icc[i] = get_icc("learning_to_learn", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$nonperseverative_errors_icc[i] = get_icc("nonperseverative_errors", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$perseverative_errors_icc[i] = get_icc("perseverative_errors", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$perseverative_responses_icc[i] = get_icc("perseverative_responses", tmp_t1_dvs, tmp_t2_dvs)
  trial_num_rel_df$total_errors_icc[i] = get_icc("total_errors", tmp_t1_dvs, tmp_t2_dvs)
}
rm(i, cur_break, tmp_t1_dvs, tmp_t2_dvs)

trial_num_rel_df$breaks = as.numeric(trial_num_rel_df$breaks)

# write.csv(trial_num_rel_df, paste0(retest_data_path, 'trial_num_rel_df_shift.csv'))
```

Takeaways from this graph:
- Reliability estimates stabilize after about an eigthth of the trials (note that this might not be true for other model parameter estimates; here we are only looking at raw response time and accuracy measures)
- Only three measures that are not contrast measures have reliabilities >0
- All the other measures are basically completely unreliable (including the two 'meaningful variables' in the literature: the cue switch cost RT's)

```{r warning=FALSE, message=FALSE}
trial_num_rel_df = read.csv(paste0(retest_data_path, 'trial_num_rel_df_shift.csv'))

trial_num_rel_df %>%
  gather(key, value, -breaks) %>%
  ggplot(aes((breaks+1)*10, value, shape=key))+
  geom_point()+
  geom_line(alpha = 0.5)+
  
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')+
  scale_shape_manual(values = c(0:9),
                     breaks = c("acc_icc", "avg_rt_icc", "conceptual_responses_icc", "fail_to_maintain_set_icc", "learning_rate_icc", "learning_to_learn_icc", "nonperseverative_errors_icc", "perseverative_errors_icc", "perseverative_responses_icc", "total_errors_icc"),
                     labels = c("Accuracy", "Average Response Time", "Conceptual Responses", "Failure to Maintain Set", "Learning Rate", "Learning to Learn",  "Nonperseverative Errors", "Perseverative Errors", "Perseverative Responses", "Total Errors"))

ggsave('Shift_Task_Intrameasure_Trialnum_Dependendence.jpg', device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures", width = 10, height = 5, units = "in", dpi = 100)
```

### Raw vs DDM

Checking DDM results in the bootstrapped estimates. Variables using all trials are significantly more reliable compared to difference scores. Raw measures don't differ from DDM parameters. Which DDM is better depends on whether all trials are used.

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1, 
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv')

tmp_save = tmp %>%
  drop_na() %>% #try removing this in final release
  group_by(contrast, raw_fit, rt_acc) %>%
  summarise(icc_median = quantile(icc, probs = 0.5),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_vars = n()/1000) 

tmp_save %>% 
  datatable() %>%
  formatRound(columns=c('icc_median', 'icc_2.5', 'icc_97.5'), digits=3)
```

```{r}
tmp_save = tmp_save %>%
  ungroup() %>%
  mutate(contrast = as.character(contrast),
         raw_fit = as.character(raw_fit),
         rt_acc = as.character(rt_acc)) %>%
  arrange(-icc_median)

sjt.df(tmp_save %>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/ddm_rel_table.doc")
```

Comparing contrast vs non-contrast: overall has higher reliability than difference.

```{r}
# summary(lmerTest::lmer(icc ~ contrast + (1|dv) ,tmp))
summary(MCMCglmm(icc ~ contrast, random = ~ dv, data=tmp))
```

Comparing raw vs ddm in overall estimates: EZ is significantly better than HDDM and comparable to raw estimates.

```{r}
# summary(lmerTest::lmer(icc ~ raw_fit + (1|dv) ,tmp %>% filter(contrast == "non-contrast")))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/icc_by_rawfit_noncon_model.rds')
summary(m)
```

Comparing raw vs ddm in difference scores: EZ is significantly worse than HDDM and comparable to raw estimates.

```{r}
# summary(lmerTest::lmer(icc ~ raw_fit + (1|dv) ,tmp %>% filter(contrast == "contrast")))
m = readRDS('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/icc_by_rawfit_con_model.rds')
summary(m)
```

```{r}
ddm_boot_plot = tmp %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~factor(contrast, levels=c("non-contrast", "contrast"), labels=c("Non-contrast", "Contrast")))+
  theme_bw()+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
        legend.text = element_text(size=12),
        strip.text = element_text(size=12),
        axis.text = element_text(size = 12),
        text = element_text(size=12))+
  guides(fill = guide_legend(ncol = 2, byrow=F))+
  scale_fill_brewer(palette="Greys",
                    breaks=c( "Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))

mylegend<-g_legend(ddm_boot_plot)

grob_name <- names(mylegend$grobs)[1]

#manually fix the legend
#move non-decision down
#key
mylegend$grobs[grob_name][[1]]$layout[11,c(1:4)] <- c(4,8,4,8)
mylegend$grobs[grob_name][[1]]$layout[12,c(1:4)] <- c(4,8,4,8)
#text
mylegend$grobs[grob_name][[1]]$layout[17,c(1:4)] <- c(4,10,4,10)
#move threshold down
#key
mylegend$grobs[grob_name][[1]]$layout[9,c(1:4)] <- c(3,8,3,8)
mylegend$grobs[grob_name][[1]]$layout[10,c(1:4)] <- c(3,8,3,8)
#text
mylegend$grobs[grob_name][[1]]$layout[16,c(1:4)] <- c(3,10,3,10)
#move drift rate right and up
#key
mylegend$grobs[grob_name][[1]]$layout[7,c(1:4)] <- c(2,8,2,8)
mylegend$grobs[grob_name][[1]]$layout[8,c(1:4)] <- c(2,8,2,8)
#text
mylegend$grobs[grob_name][[1]]$layout[15,c(1:4)] <- c(2,10,2,10)

ddm_boot_legend = arrangeGrob(ddm_boot_plot +theme(legend.position="none"),
             mylegend, nrow=2, heights=c(10, 1))

ggsave('Bootstrap_DDM_Comp_Legend.jpg', plot = ddm_boot_legend, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 14, height = 10, units = "in", limitsize = FALSE)
```

```{r}
tmp %>%
  group_by(contrast) %>%
  summarise(mean_icc = mean(icc),
            sd_icc = sd(icc))
```

```{r}
tmp2 = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1, 
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(rel_df[,c("dv", "icc")], by = 'dv')

ddm_point_plot = tmp2 %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~factor(contrast, levels=c("non-contrast", "contrast"), labels=c("Non-contrast", "Contrast")))+
  theme_bw()+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom',
        legend.text = element_text(size=16),
        strip.text = element_text(size=16),
        axis.text = element_text(size = 16),
        text = element_text(size=16))+
  guides(fill = guide_legend(ncol = 2))+
  scale_fill_brewer(palette="Greys",
                    breaks=c( "Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))+
  ylim(-0.4, 1)
  
mylegend<-g_legend(ddm_point_plot)

grob_name <- names(mylegend$grobs)[1]

#manually fix the legend
#move non-decision down
#key
mylegend$grobs[grob_name][[1]]$layout[11,c(1:4)] <- c(4,8,4,8)
mylegend$grobs[grob_name][[1]]$layout[12,c(1:4)] <- c(4,8,4,8)
#text
mylegend$grobs[grob_name][[1]]$layout[17,c(1:4)] <- c(4,10,4,10)
#move threshold down
#key
mylegend$grobs[grob_name][[1]]$layout[9,c(1:4)] <- c(3,8,3,8)
mylegend$grobs[grob_name][[1]]$layout[10,c(1:4)] <- c(3,8,3,8)
#text
mylegend$grobs[grob_name][[1]]$layout[16,c(1:4)] <- c(3,10,3,10)
#move drift rate right and up
#key
mylegend$grobs[grob_name][[1]]$layout[7,c(1:4)] <- c(2,8,2,8)
mylegend$grobs[grob_name][[1]]$layout[8,c(1:4)] <- c(2,8,2,8)
#text
mylegend$grobs[grob_name][[1]]$layout[15,c(1:4)] <- c(2,10,2,10)

ddm_point_legend = arrangeGrob(ddm_point_plot +theme(legend.position="none"),
             mylegend, nrow=2, heights=c(10, 1))


ggsave('PointEst_DDM_Comp.jpg', plot=ddm_point_legend, device = "jpeg", path = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/figures/", width = 15, height = 7, units = "in", limitsize = FALSE)
```

## Survey reliabilities

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'survey') %>%
  left_join(boot_df[,c("dv", "icc")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2) 

tmp_save = tmp %>%
  group_by(task_name) %>%
  summarise(median_icc = median(icc),
            icc_2.5 = quantile(icc, probs = 0.025),
            icc_97.5 = quantile(icc, probs = 0.975),
            num_measures = n()/1000,
            mean_num_trials = round(mean(num_all_trials)))%>%
  arrange(-median_icc)

tmp_save %>%
  datatable() %>%
  formatRound(columns=c('median_icc', 'icc_2.5','icc_97.5'), digits=3)
```

```{r}
tmp_save = tmp_save%>%
  mutate(task_name = gsub("_", " ", task_name),
         task_name = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", task_name, perl=TRUE),
         task_name = gsub("Survey", "", task_name))

names(tmp_save) = gsub("_", " ", names(tmp_save))
names(tmp_save) = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", names(tmp_save), perl=TRUE)

sjt.df(tmp_save %>% mutate_if(is.numeric, funs(round(., 3))), describe=F, hide.progress = TRUE, show.rownames = FALSE, file = "/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/output/tables/survey_rel_table.doc")
```

### Number of items

Does number of items in a survey have a significant effect on the average ICC of survey measures? No. 

```{r warning=FALSE, message=FALSE}
summary(lmerTest::lmer(icc ~ num_all_trials + (1|dv), data = tmp))
```

```{r warning=FALSE, message=FALSE}
tmp %>%
  ggplot(aes(num_all_trials, icc))+
  geom_point()+
  geom_smooth(method="lm")+
  theme_bw()+
  xlab("Number of trials")+
  ylab("ICC")
```
