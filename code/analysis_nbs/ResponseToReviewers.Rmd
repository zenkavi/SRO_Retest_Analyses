---
title: 'SRO Retest Response to Reviewers'
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
from_gh=FALSE
script_loader = function(file, path, remote=from_gh, branch="master"){
  if(remote){
    library(RCurl)
    gh_path = paste('https://raw.githubusercontent.com/zenkavi',
                     strsplit(path, '/')[[1]][1],
                     branch,
                     paste(strsplit(path, '/')[[1]][-1],'/',sep = '', collapse=""),
                    sep="/")
    eval(parse(text = getURL(paste0(gh_path, file), ssl.verifypeer = FALSE)))
  }else{
    source(paste0('/Users/zeynepenkavi/Dropbox/PoldrackLab/',path, file))
  }
}

library(tidyverse)
library(jsonlite)
library(MCMCglmm)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")

to_keep = c('from_gh',"script_loader", "cbbPalette", "ggplot")
```

#R1

##R1.5
>If the common assumption that threshold and non-decision time do not vary between conditions in tasks with intermixed trials is true, we could interpret the contrasts between those parameters generated from the EZ diffusion model to simply be noise. If that were the case, we shouldn't expect those contrasts to be reliable. 

TODO: 
### Are EZ threshold and non-decision time contrast estimates are >0?

Yes. The assumption that thresholds and non-decision times are the same across conditions does not hold in our data as the distributions of these parameters are systematically different than 0 for both time point.

```{r}
script_loader(file='ddm_subject_data.R', path='SRO_DDM_Analyses/code/workspace_scripts/')
```

```{r}
ez_contrast_vars = measure_labels %>%
  filter(raw_fit == "EZ" & overall_difference == "contrast") %>%
  select(dv)
```

```{r}
get_tval = function(x, val=0){
  return((mean(x, na.rm=T) - val)/(sd(x, na.rm=T)/sqrt(length(x))))
}

get_mean = function(x){
  return(mean(x, na.rm=T))
}
```

Means and and T values comparing the means to 0 for T1 data.

```{r}
test_data_522 %>%
  select(ez_contrast_vars$dv) %>%
  gather(key, value) %>%
  group_by(key) %>%
  summarise(tvals = get_tval(value), 
            means = get_mean(value))
```

Means and and T values comparing the means to 0 for T2 data.

```{r}
retest_data %>%
  select(ez_contrast_vars$dv) %>%
  gather(key, value) %>%
  group_by(key) %>%
  summarise(tvals = get_tval(value), 
            means = get_mean(value))
```

```{r message=FALSE, warning=FALSE}
retest_data %>%
  select(ez_contrast_vars$dv) %>%
  mutate(time = "Time 1") %>%
  bind_rows(test_data %>%
              select(ez_contrast_vars$dv) %>%
              mutate(time = "Time 2")) %>%
  gather(key, value, -time) %>%
  rename(dv = key) %>%
  left_join(measure_labels %>% select(dv, rt_acc), by = 'dv') %>%
  ggplot(aes(value))+
  geom_density(aes(group=dv, col=rt_acc), linetype="dotted")+
  geom_density(aes(col=rt_acc), size=2.5)+
  xlim(-1.5,1.5)+
  geom_vline(xintercept=0, linetype="dashed", size=1.5)+
  theme(legend.title=element_blank())+
  xlab("")+
  ylab("")+
  facet_wrap(~time)
```

```{r}
retest_data %>%
  select(ez_contrast_vars$dv) %>%
  mutate(time = "Time 1") %>%
  bind_rows(test_data %>%
              select(ez_contrast_vars$dv) %>%
              mutate(time = "Time 2")) %>%
  gather(key, value, -time) %>%
  rename(dv = key) %>%
  left_join(measure_labels %>% select(dv, rt_acc), by = 'dv') %>%
  group_by(rt_acc, time) %>%
  summarise(tvals=get_tval(value),
            means=get_mean(value))
```

### Are their reliabilities >0?

Although the thresholds and non-decision times for contrasts are >0 still have low reliabilities.

```{r  message=FALSE, warning=FALSE}
rt_ddm_vars = measure_labels %>%
  filter(overall_difference %in% c("non-contrast", "contrast")) %>%
  select(dv)

rt_ddm_vars = rt_ddm_vars[,1]

script_loader(file="make_rel_df.R", path="SRO_Retest_Analyses/code/helper_functions/")
```

```{r message=FALSE, warning=FALSE}
rel_df = make_rel_df(test_data, retest_data, metrics = c("icc3.k", 'icc2.1'), vars = rt_ddm_vars)

rel_df %>%
  left_join(measure_labels %>% select("rt_acc", "raw_fit", "dv", "overall_difference"), by = "dv") %>%
  mutate(overall_difference = factor(overall_difference, levels = c("non-contrast", "contrast"), labels = c("Non-contrast", "Contrast")),
         raw_fit = factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels = c("Raw", "EZ", "HDDM")),
         rt_acc = factor(rt_acc, levels = c("drift rate", "threshold", "non-decision", "rt", "accuracy"), labels = c("Drift Rate", "Threshold", "Non-decision", "RT", "Accuracy"))) %>%
  ggplot(aes(raw_fit, icc2.1, fill=rt_acc))+
  geom_boxplot()+
  facet_wrap(~overall_difference)+
  geom_hline(yintercept=0, linetype="dashed")+
  guides(fill = guide_legend(nrow=3, byrow=FALSE))+
  theme(legend.title = element_blank())
```

```{r}
rel_df %>%
  left_join(measure_labels %>% select("rt_acc", "raw_fit", "dv", "overall_difference"), by = "dv") %>%
  mutate(overall_difference = factor(overall_difference, levels = c("non-contrast", "contrast"), labels = c("Non-contrast", "Contrast")),
         raw_fit = factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels = c("Raw", "EZ", "HDDM")),
         rt_acc = factor(rt_acc, levels = c("drift rate", "threshold", "non-decision", "rt", "accuracy"), labels = c("Drift Rate", "Threshold", "Non-decision", "RT", "Accuracy"))) %>%
  group_by(overall_difference, raw_fit, rt_acc) %>%
  summarise(mean_icc2.1 = mean(icc2.1),
            tvals_icc2.1 = get_tval(icc2.1))
```

*Response:* We thank the reviewer for providing a potential explanation for the low reliability of the EZ threshold and non-decision time variables for contrasts. In our data the EZ thresholds and non-decision times for contrasts are systematically different than 0 suggesting that they capture a systematic difference instead of the common assumption of noise. Still these contrast measures are much less reliable than non-contrast measures. Similar to the constrasts in raw measures of response times and accuracies, which are less reliable than their non-contrast counterparts, measures do not have to be noise to be unreliable. 
Because the assumption fails we chose not to present this possibility in the manuscript. The details of our analyses on this question can however be found [on:  https://zenkavi.github.io/SRO_Retest_Analyses/output/reports/ResponseToRevisers.nb.html#R1](https://zenkavi.github.io/SRO_Retest_Analyses/output/reports/ResponseToRevisers.nb.html#R1)

```{r echo=FALSE}
rm(list=setdiff(ls(), to_keep))
```

##R1.6
>In our paper, we report ICC(2,1)...

TODO: All results with ICC using ICC(2,1) instead of ICC(3,k)  
- [DONE] recode `get_retest_stats` to report all kinds of ICCs
- [DONE] make new `rel_df` (with both types of ICC for retest paper vars)
- [DONE] make new `boot_df` (with both types of ICC for retest paper vars)
- [DONE] rerun MCMC models with ICC(2,1)
- [DONE] run SRO_Retest_Analyses.Rmd with ICC(2,1) and confirm results don't change
- [DONE] replace fig S3 with new metric scatterplot including icc(2,1)
- [DONE] replace all ICC(3,k)'s in main text 

```{r message=FALSE}
script_loader(file='subject_data.R', path="SRO_Retest_Analyses/code/workspace_scripts/")

script_loader(file='make_rel_df.R', path="SRO_Retest_Analyses/code/helper_functions/")

script_loader(file='measure_labels_data.R', path="SRO_Retest_Analyses/code/workspace_scripts/")
```

```{r}
rel_df = make_rel_df(test_data, retest_data, metrics = c("icc3.k", 'icc2.1', 'pearson', 'spearman'))
```

```{r message=FALSE}
rel_df %>%
  gather(key, value, -dv, -icc2.1)%>%
  left_join(measure_labels %>% select(dv, task),by="dv") %>%
  ggplot(aes(icc2.1, value))+
  geom_point(aes(col=task))+
  facet_wrap(~key)+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlim(-0.5, 1)+
  ylim(-0.5, 1)+
  ylab("")+
  theme(legend.title = element_blank(),
        aspect.ratio = 1)
```

##R1.7
>It is stated on pg. 9 that the ICC ranges from -1 to 1. Though statistical packages report negative ICCs, it is sometimes suggested that they should be treated as zero (e.g. Bartko, 1976), as a proportion of variance cannot theoretically be negative. I am unsure what the best approach is for the current purposes, but I note it for consideration. As there don't appear to be a large number of negative ICCs, I suspect it wouldn't affect the averages much. 

TODO:
- report how many negative ICC's there are
- run all analyses that include ICC's with 
  - replacing negative values with 0's 
  - removing them

There are 21 variables with negative point estimate ICC's. This is less than 5% of the measures.

```{r}
rel_df %>%
  mutate(neg_icc = ifelse(icc2.1<0,1,0)) %>%
  summarise(sum_neg = sum(neg_icc))
```

### Replacing negative ICC's with 0's

```{r}
rel_df_rep=rel_df %>%
  left_join(measure_labels %>% select(dv, task), by="dv") %>%
  select(dv, task, icc2.1) %>%
  mutate(icc2.1=ifelse(icc2.1<0,0,icc2.1))
```

Reliability by task vs survey
```{r}
summary(MCMCglmm(icc2.1 ~ task, data=rel_df_rep, verbose=F))
```

Reliability depending on number of trials
```{r warning=FALSE, message=FALSE}
script_loader('meaningful_vars_data.R', path="SRO_Retest_Analyses/code/workspace_scripts/")

tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(rel_df_rep[,c("dv", "icc2.1")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2)

summary(MCMCglmm(icc2.1 ~ num_all_trials, data=tmp, verbose=F))
```

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1,
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(rel_df_rep[,c("dv", "icc2.1")], by = 'dv')
```

Comparing contrast vs non-contrast: overall has higher reliability than difference.

```{r}
summary(MCMCglmm(icc2.1 ~ contrast, data=tmp, verbose=FALSE))
```

Comparing raw vs ddm in overall estimates

```{r message=FALSE, warning=FALSE}
summary(MCMCglmm(icc2.1 ~ raw_fit, data=tmp %>% filter(contrast == "non-contrast"), verbose=FALSE))
```

Comparing raw vs ddm in difference scores

```{r message=FALSE, warning=FALSE}
summary(MCMCglmm(icc2.1 ~ raw_fit, data=tmp %>% filter(contrast == "contrast"), verbose=FALSE))
```

### Removing negative ICC's

```{r}
rel_df_ex = rel_df %>%
  left_join(measure_labels %>% select(dv, task), by="dv") %>%
  select(dv, task, icc2.1) %>%
  filter(icc2.1>0)
```

Reliability by task vs survey
```{r}
summary(MCMCglmm(icc2.1 ~ task, data=rel_df_ex, verbose=F))
```

Reliability depending on number of trials
```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task',
         dv %in% meaningful_vars) %>%
  left_join(rel_df_ex[,c("dv", "icc2.1")], by = 'dv') %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2)

summary(MCMCglmm(icc2.1 ~ num_all_trials, data=tmp, verbose=F))
```

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv),
         contrast = ifelse(overall_difference == "difference", "contrast", "non-contrast")) %>%
  filter(ddm_task == 1,
         rt_acc != 'other') %>%
  drop_na() %>%
  left_join(rel_df_ex[,c("dv", "icc2.1")], by = 'dv')
```

Comparing contrast vs non-contrast: overall has higher reliability than difference.

```{r}
summary(MCMCglmm(icc2.1 ~ contrast, data=tmp, verbose=FALSE))
```

Comparing raw vs ddm in overall estimates

```{r}
summary(MCMCglmm(icc2.1 ~ raw_fit, data=tmp %>% filter(contrast == "non-contrast"), verbose=FALSE))
```

Comparing raw vs ddm in difference scores

```{r}
summary(MCMCglmm(icc2.1 ~ raw_fit, data=tmp %>% filter(contrast == "contrast"), verbose=FALSE))
```

##R1.8
>I think it is a strength of the paper overall that the authors consider the importance of trial numbers, though I'm curious why they chose the number of trials they did for their tasks (e.g. if they came from particular studies or an average)? The role of trial numbers was also something we were interested in (see our Supplementary material D: https://link.springer.com/article/10.3758/s13428-017-0935-1#SupplementaryMaterial), and is discussed by Rouder & Haaf (2018). 

TODO:
- [DONE] extract trial numbers for papers that went in to the lit search and compare to ours
- [PARTIALLY DONE] code pipeline to get reliability with k number of trials

```{r warning=FALSE, message=FALSE}
tmp1 = lit_review %>%
  filter(task == "task") %>%
  select(dv, total_trial_numbers) %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.') %>%
  select(-extra_1, -extra_2) %>%
  group_by(task_name) %>%
  summarise(mean_num_trials_lit = mean(total_trial_numbers, na.rm=T))

tmp2 = measure_labels %>%
  filter(task == "task") %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.') %>%
  select(-extra_1, -extra_2) %>%
  group_by(task_name) %>%
  summarise(mean_num_trials_emp = mean(num_all_trials))

tmp2 %>%
  left_join(tmp1, by="task_name")
```

```{r}
rm(rel_df)

script_loader(file='make_rel_df.R', path='SRO_Retest_Analyses/code/helper_functions/')

input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/trial_num/ordered/'

for(i in c(0.25, 0.5, 0.75)){
  df1 = read.csv(paste0(input_path, 'complete_', as.character(i) ,'_variables_exhaustive.csv'))
  df1 = df1 %>% rename(sub_id = X)  
  
  df2 = read.csv(paste0(input_path, 'retest_', as.character(i) ,'_variables_exhaustive.csv'))
  df2 = df2 %>% rename(sub_id = X) %>% 
    filter(sub_id %in% df1$sub_id) 
  
  numeric_cols = get_numeric_cols(df1, df2)
  
  subs = df1$sub_id
  
  df1 = df1[,numeric_cols]
  df2 = df2[,numeric_cols]
  
  df1 = df1 %>%
    select_if(function(col) !is.na(sd(col))) %>%
    select_if(function(col) sd(col) != 0)
  
  # df2 = df2 %>%
  #   select_if(function(col) !is.na(sd(col))) %>%
  #   select_if(function(col) sd(col) != 0)
  
  df2 = df2[,c(names(df1))]
  
  df1$sub_id = subs
  df2$sub_id = subs
  
  cur_rel_df = make_rel_df(t1_df=df1, t2_df=df2, metrics = c("icc2.1", "icc3.k", "var_breakdown"))
  cur_rel_df$proptrials = i
  if(!exists('rel_df')){
    rel_df = cur_rel_df
  } else{
    rel_df = rbind(rel_df, cur_rel_df)
  }
}
rm(cur_rel_df)
```

```{r message=FALSE, warning=FALSE}
rel_df %>% 
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  filter(task_name != "columbia_card_task_cold") %>%
  mutate(task_name = factor(task_name, levels = c("columbia_card_task_hot","go_nogo", "hierarchical_rule", "keep_track","kirby", "ravens"),
                            labels = c("Columbia Card Task (24)","Go/No-go (200)","Hierarchical Rule (300)", "Keep Track (9)","Kirby (27)", "Ravens (18)"))) %>%
  ggplot(aes(factor(proptrials), icc2.1, col=task_name, shape=task_name))+
  geom_point(aes(group=dv))+
  geom_line(aes(group=dv))+
  xlab("Proportion of trials")+
  ylab("ICC 2.1")+
  theme(legend.title=element_blank(),
        panel.grid=element_blank())+
  guides(color = guide_legend(nrow=2))+
  scale_color_manual(values=c('#762a83','#af8dc3','#e7d4e8','#d9f0d3','#7fbf7b','#1b7837'))
```

```{r}
summary(MCMCglmm(icc2.1 ~ factor(proptrials), random=~dv, data = rel_df, verbose=FALSE))
```

#R2

##R2.1
>The researchers were careful in their sampling procedure to try to minimize differences between participants who completed (vs. did not complete) the retest phase of the test-retest. Some simple statistical comparisons between the completers (N = 157) and non-completers/non-responders (N = 242-157 = 85) would help characterize this subset of the sample better and provide an additional data quality check. 

TODO: How many variables are significantly different than each other between the subjects who have been invited and completed the retest battery and those who have not after FDR correcting for multiple comparisons?  

Answer: None.

```{r warning=FALSE, message=FALSE}
mturk_worker_data = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_11-27-2017/Local/User_717570_workers.csv')

worker_lookup = read_json('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_11-27-2017/Local/worker_lookup.json')

worker_lookup = data.frame(worker_lookup) %>%
  gather(sub_id, worker_id)

retest_workers = mturk_worker_data %>%
  select(Worker.ID, CURRENT.RetestWorker, CURRENT.RetestWorkerB2, CURRENT.RetestWorkerB3, CURRENT.RetestWorkerB4, CURRENT.RetestWorkerB5) %>%
  replace(., is.na(.), 0) %>%
  mutate(retest_worker = ifelse(CURRENT.RetestWorker+ CURRENT.RetestWorkerB2+CURRENT.RetestWorkerB3+CURRENT.RetestWorkerB4+ CURRENT.RetestWorkerB5>0,1,0)) %>%
  filter(retest_worker==1) %>%
  select(Worker.ID) %>%
  rename(worker_id = Worker.ID) %>%
  left_join(worker_lookup, by = "worker_id") %>%
  select(-worker_id)

test_data_522 = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/variables_exhaustive.csv')

test_data_522 = test_data_522 %>%
  rename(sub_id = X)

test_data_522_subs = test_data_522$sub_id

test_data_522 <- test_data_522[,names(test_data_522) %in% retest_report_vars]

test_data_522$sub_id = test_data_522_subs

non_completers = retest_workers %>%
  filter(sub_id %in% retest_data$sub_id==FALSE)

completers = retest_data %>%
  select(sub_id)

non_completer_data = test_data_522 %>%
  filter(sub_id %in% non_completers$sub_id)

completer_data = test_data_522 %>%
  filter(sub_id %in% completers$sub_id)

out_df = data.frame(dv=NA, p_val=NA)

for(i in retest_report_vars[-1]){
  x = non_completer_data %>% select(i)
  y = completer_data %>% select(i)
  tmp = data.frame(dv = i, p_val = t.test(x,y)$p.value)
  out_df = rbind(out_df, tmp)
}
out_df = out_df[-1,]

sum(p.adjust(out_df$p_val, method = "fdr") < 0.05)
```

