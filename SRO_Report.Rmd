---
title: 'Self Regulation Ontology Retest Data Report'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(lme4)
library(GGally)
library(jsonlite)
library(psych)
library(rmarkdown)
library(psych)
library(stringr)
library(plotly)
library(jsonlite)
library(DT)
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
render_this <- function(){rmarkdown::render('SRO_Report.Rmd', output_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Report', html_notebook(toc = T, toc_float = T, code_folding = 'hide'))}
```

# Introduction

# Methods

## Data collection

This sample consists of data for 150 subjects of the original sample of 522 that has completed the initial battery of 37 cognitive tasks, 23 surveys and 3 surveys on demographics. Details of the original sample as well as quality control (qc) procedures are described elsewhere. Invited participants were chosen randomly and only subsets of them were invited for a given batch (instead of opening the battery to all qualified subjects) with the intention to avoid a potential oversampling and bias towards high self regulators.

```{r warning=FALSE, message=FALSE}
workers = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/Local/User_717570_workers.csv')
workers = workers %>% 
  group_by(Worker.ID) %>%
  mutate(Retest_worker=ifelse(sum(CURRENT.RetestWorker,CURRENT.RetestWorkerB2,CURRENT.RetestWorkerB3,CURRENT.RetestWorkerB4,CURRENT.RetestWorkerB5,na.rm=T)>0,1,0)) %>%
  ungroup()

worker_counts <- fromJSON('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/Local/retest_worker_counts.json')

worker_counts = as.data.frame(unlist(worker_counts))
names(worker_counts) = "task_count"
```

In total `r sum(workers$Retest_worker)` participants were invited, `r nrow(worker_counts)` began the battery, `r sum(worker_counts$task_count >= 62)` completed the battery and 150 provided data that passed qc for both time points. Our target sample size was determined in advance of data collection and data collection continued until this number of participants with data that survived qc was reached.

```{r warning=FALSE, message=FALSE}
disc_comp_date = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/Local/discovery_completion_dates.csv', header=FALSE)
val_comp_date = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/Local/validation_completion_dates.csv', header=FALSE)
test_comp_date = rbind(disc_comp_date, val_comp_date)
rm(disc_comp_date, val_comp_date)
retest_comp_date = read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/Local/retest_completion_dates.csv', header=FALSE)
comp_dates = merge(retest_comp_date, test_comp_date, by="V1")
names(comp_dates) <- c("sub_id", "retest_comp", "test_comp")
comp_dates$retest_comp = as.Date(comp_dates$retest_comp)
comp_dates$test_comp = as.Date(comp_dates$test_comp)
comp_dates$days_btw = with(comp_dates, retest_comp-test_comp)
```

Data collection took place on average `r round(mean(as.numeric(comp_dates$days_btw)))` number of days after the completion of the initial battery with a range of `r round(range(as.numeric(comp_dates$days_btw)))[1]` to `r round(range(as.numeric(comp_dates$days_btw)))[2]` days.

## Literature

### Tasks

### Surveys

## Loading and matching datasets

This report uses the variables that were designated as "meaningful" before. This is not the smallest subset where certain variables are dropped because they are correlated with other variables (within a task) and includes both accuracies as well as both kinds of DDM variables (EZ and HDDM). It will not include variables that were considered not of theoretical interest in the analyses of time 1 data. The variables that are not included will be listed for reference as well.

### Load time 1 data
```{r}
test_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables.csv')

test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_noDDM.csv')

test_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_EZ.csv')

test_data <- merge(test_data, test_data2)
test_data <- merge(test_data, test_data3)
rm(test_data2, test_data3)
```

For reference here are the variables that are **not** included in the analyses of the remainder of this report because they were not of theoretical interest in factor structure analyses of this data so far. These include drift diffusion and other model parameters for specific conditions within a task; survey variables that are not part of the dependant variables for that survey in the literature and demographics (these are saved for prediction analyses).

```{r}
test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/variables_exhaustive.csv')

test_data2$X <- as.character(test_data2$X)

names(test_data2)[1] <- 'sub_id'

df <- data.frame(names(test_data2)[which(names(test_data2) %in% names(test_data) == FALSE)])
names(df) = c('vars')

datatable(df)
```

```{r echo=FALSE}
rm(test_data2, df)
```

### Load time 2 data 
```{r}
retest_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/meaningful_variables.csv')

retest_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/meaningful_variables_noDDM.csv')

retest_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/meaningful_variables_EZ.csv')

retest_data <- merge(retest_data, retest_data2)
retest_data <- merge(retest_data, retest_data3)
rm(retest_data2, retest_data3)
```

Extract t2 participants with good t1 data: We only invited those with good data but due to a technical error a handful of subjects who weren't checked for data quality completed the retest battery as well. One of those subjects gave good t2 data but didn't have good t1 data and therefore is removed from further analyses.

```{r}
#Process sub_id columns in the data dataframes
retest_data$X <- as.character(retest_data$X)
test_data$X <- as.character(test_data$X)

names(retest_data)[1] <- 'sub_id'
names(test_data)[1] <- 'sub_id'

retest_data <- retest_data[retest_data$sub_id %in% test_data$sub_id,]
```

### Clean t1 data

The cleaning functions here are translated from the python pipeline to mimick the same procedure that was applied to the `meaningful_variables` in creating `meaningful_variables_clean`.

The original cleaning procedures include three steps:  
1. Removing outliers for each variable where outliers are defined as those who are greater than 2.5 IQR from median. **This step is skipped here because removing outliers from either dataset could lead to an unnecessary increase in missing data. Since we are interested in the average distance between two measures of a metric for a given subject instead of the distance between all these metrics this seemed appropriate.**  
2. Removing correlated variables *within* a task. In creating `meaningful_variables_clean` if variables for a given task correlated >.85 only one was kept. **This step is skipped here because we want to look at the reliability of as many variables as we can.**  
3. Log transforming variables with skew. In cleaning t1 data variables that had a |skew| > 1 were log transformed and those for which skew could not be removed successfully were dropped. **Here we transform all variables that have a |skew|>1 in t1 data but variables that are not successfully transformed will be listed and not dropped.**

```{r}
remove_outliers = function(data_column, quantile_range = 2.5){
  
  q_25 = quantile(data_column, na.rm=T)[2]
  q_50 = quantile(data_column, na.rm=T)[3]
  q_75 = quantile(data_column, na.rm=T)[4]
  
  lowlimit = q_50 - quantile_range*(q_75 - q_25)
  highlimit = q_50 + quantile_range*(q_75 - q_25)
  
  data_column = ifelse(data_column<lowlimit, NA, ifelse(data_column>highlimit, NA, data_column))
  
  return(data_column)
}

"%w/o%" <- function(x, y) x[!x %in% y]

pos_log <- function(column){
  col_min = min(column, na.rm=T)
  a = 1-col_min
  column = column+a
  return(log(column))
}

neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}

transform_remove_skew = function(data, columns, threshold = 1, drop=FALSE){
  
  tmp = as.data.frame(apply(data[,columns],2,skew))
  names(tmp) = c("skew")
  tmp$dv = row.names(tmp)
  tmp = tmp %>% 
    filter(abs(skew)>threshold)
  
  skewed_variables = tmp$dv
  skew_subset = data[, skewed_variables]
  positive_subset = data[,tmp$dv[tmp$skew>0]]
  negative_subset = data[,tmp$dv[tmp$skew<0]]
  
  # transform variables
  # log transform for positive skew
  # positive_subset = log(positive_subset)
  positive_subset = pos_log(positive_subset) #slight divergence from original code
  successful_transforms = as.data.frame(apply(positive_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<threshold)
  successful_transforms = successful_transforms$dv
  successful_transforms = positive_subset[,successful_transforms]
  dropped_vars = names(positive_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(positive_subset)) ,' data positively skewed data were transformed:'))
  cat('\n')
  cat(names(positive_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(positive_subset)))]
  
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' positively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(positive_subset) = paste0(names(positive_subset), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' positively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, positive_subset)
  }
  
  
  # reflected log transform for negative skew      
  negative_subset = as.data.frame(apply(negative_subset, 2, neg_log))
  successful_transforms = as.data.frame(apply(negative_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<1)
  successful_transforms = successful_transforms$dv
  successful_transforms = negative_subset[,successful_transforms]
  dropped_vars = names(negative_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(negative_subset)) ,' data negatively skewed data were transformed:'))
  cat('\n')
  cat(names(negative_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(negative_subset)))]
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' negatively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(negative_subset) = paste0(names(negative_subset), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' negatively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, negative_subset)
  }
  
  return(data)
}
```

Clean t1 data on full sample without dropping any columns. Columns that are not transformed successfully are listed.

```{r warning=FALSE}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

# clean_test_data = cbind(sub_id = test_data$sub_id, as.data.frame(apply(test_data[, -which(names(test_data) %in% c("sub_id"))], 2, remove_outliers)))

# clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
clean_test_data = transform_remove_skew(test_data, numeric_cols)
```

```{r echo=FALSE}
rm(i)
```

### Clean t2 data

Instead of applying the same functions applied to t1 data we check which variables are transformed in what way and transform the t2 data the same way. Again we are not removing outliers in the t2 or dropping variables if the skew is not removed successfully after transforming.

Get list of transformed variables and the sign of their skew
```{r}
logTr_vars = data.frame(grep('logTr', names(clean_test_data), value=T))
names(logTr_vars) = c('var')
logTr_vars$sign = ifelse(grepl(".ReflogTr",logTr_vars$var), 'negative', 'positive')
logTr_vars$var = gsub(".logTr|.ReflogTr", "", logTr_vars$var)
```

Transform the t2 columns the same way t1 columns were transformed.

```{r warning=FALSE}
clean_retest_data = retest_data

for(i in 1:length(names(clean_retest_data))){
  tmp_col = names(clean_retest_data)[i]
  if(tmp_col %in% logTr_vars$var){
    sign = logTr_vars$sign[which(tmp_col == logTr_vars$var)]
      if(sign == 'positive'){
        # clean_retest_data[,tmp_col] = log(clean_retest_data[,tmp_col])
        clean_retest_data[,tmp_col] = pos_log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.logTr')
        cat(paste0(tmp_col ,' was positively skewed and transformed.'))
        cat('\n')
      }
      else if(sign == 'negative'){
        clean_retest_data[,tmp_col] = neg_log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.ReflogTr')
        cat(paste0(tmp_col ,' was negatively skewed and transformed.'))
        cat('\n')
      }
    }
}
```

```{r echo = FALSE}
rm(i, sign, tmp_col)
```

Check transformations are correct
```{r}
retest_logTr_vars = data.frame(grep('logTr', names(clean_retest_data), value=T))
names(retest_logTr_vars) = c('var')
retest_logTr_vars$sign = ifelse(grepl(".ReflogTr",retest_logTr_vars$var), 'negative', 'positive')
retest_logTr_vars$var = gsub(".logTr|.ReflogTr", "", retest_logTr_vars$var)
retest_logTr_vars = retest_logTr_vars %>% arrange(var)
logTr_vars = logTr_vars %>% arrange(var)
retest_logTr_vars == logTr_vars
```


```{r echo=FALSE}
rm(logTr_vars, retest_logTr_vars)
```

```{r}
# Switch df names
raw_retest_data = retest_data
raw_test_data = test_data
test_data = clean_test_data
retest_data = clean_retest_data
rm(clean_test_data, clean_retest_data)
```

### Extract t-1 data for t-2 subjects
```{r}
retest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id,]

rest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id == FALSE,]

#Arrange datasets of same size by sub_id
retest_data = retest_data %>% arrange(sub_id)
retest_subs_test_data = retest_subs_test_data %>% arrange(sub_id)

#CHECK IF EVERYTHING IS ORDERED RIGHT
as.character(retest_subs_test_data$sub_id) == as.character(retest_data$sub_id)
```

### Replace HDDM parameters in t1 data

Since HDDM parameters depend on the sample on which they are fit we refit the model on t1 data for the subjects that have t2 data. Here we replace the HDDM parameters in the current t1 dataset with these refitted values and transform them as they would have been transformed for the full sample to mimick the procedure we applied to rest of the t2 data. 

Transform hddm refits as they had been transformed for the full sample t1 data.
```{r}
hddm_refits <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-21-2017/hddm_refits_exhaustive.csv')

hddm_refits$X <- as.character(hddm_refits$X)

names(hddm_refits)[1] <- 'sub_id'

hddm_logTr_vars = data.frame(grep('(?=.*hddm)(?=.*logTr)', names(retest_subs_test_data), value=T, perl=T))
names(hddm_logTr_vars) = c('var')
hddm_logTr_vars$sign = ifelse(grepl(".ReflogTr",hddm_logTr_vars$var), 'negative', 'positive')
hddm_logTr_vars$var = gsub(".logTr|.ReflogTr", "", hddm_logTr_vars$var)

transformed_hddm_refits = hddm_refits

for(i in 1:length(names(transformed_hddm_refits))){
  tmp_col = names(transformed_hddm_refits)[i]
  if(tmp_col %in% hddm_logTr_vars$var){
    sign = hddm_logTr_vars$sign[which(tmp_col == hddm_logTr_vars$var)]
      if(sign == 'positive'){
        # clean_retest_data[,tmp_col] = log(clean_retest_data[,tmp_col])
        transformed_hddm_refits[,tmp_col] = pos_log(transformed_hddm_refits[,tmp_col])
        names(transformed_hddm_refits)[which(names(transformed_hddm_refits) == tmp_col)] = paste0(tmp_col, '.logTr')
        cat(paste0(tmp_col ,' was positively skewed and transformed.'))
        cat('\n')
      }
      else if(sign == 'negative'){
        transformed_hddm_refits[,tmp_col] = neg_log(transformed_hddm_refits[,tmp_col])
        names(transformed_hddm_refits)[which(names(transformed_hddm_refits) == tmp_col)] = paste0(tmp_col, '.ReflogTr')
        cat(paste0(tmp_col ,' was negatively skewed and transformed.'))
        cat('\n')
      }
    }
}
```

Check if transformation of hddm refits worked
```{r}
transform_hddm_logTr_vars = data.frame(grep('logTr', names(transformed_hddm_refits), value=T))
names(transform_hddm_logTr_vars) = c('var')
transform_hddm_logTr_vars$sign = ifelse(grepl(".ReflogTr",transform_hddm_logTr_vars$var), 'negative', 'positive')
transform_hddm_logTr_vars$var = gsub(".logTr|.ReflogTr", "", transform_hddm_logTr_vars$var)
transform_hddm_logTr_vars = transform_hddm_logTr_vars %>% arrange(var)
hddm_logTr_vars = hddm_logTr_vars %>% arrange(var)
transform_hddm_logTr_vars == hddm_logTr_vars
```

Replace t1 hddm parameters for retest subjects

```{r}
replace_hddm_vars = data.frame(grep('(?=.*hddm)(?=.*logTr)', names(retest_subs_test_data), value=T, perl=T))
names(replace_hddm_vars) = c('var')

hddm_fix_retest_subs_test_data = retest_subs_test_data

for(i in 1:length(names(hddm_fix_retest_subs_test_data))){
  if(names(hddm_fix_retest_subs_test_data)[i] %in% replace_hddm_vars$var){
    hddm_fix_retest_subs_test_data[,names(hddm_fix_retest_subs_test_data)[i]] <- transformed_hddm_refits[,names(hddm_fix_retest_subs_test_data)[i]]
  }
}

retest_subs_test_data <- hddm_fix_retest_subs_test_data
```

```{r}
rm(hddm_refits, transformed_hddm_refits, hddm_logTr_vars, transform_hddm_logTr_vars, replace_hddm_vars, hddm_fix_retest_subs_test_data, sign, i)
```

# Results

## Correlations for all cleaned meaningful variables
Spearman only

```{r}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

cor_df <- data.frame(spearman = rep(NA, length(numeric_cols)))

row.names(cor_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  cor_df[numeric_cols[i], 'spearman'] <- cor(retest_data[,numeric_cols[i]],retest_subs_test_data[,numeric_cols[i]], method = 'spearman', use = "pairwise.complete.obs")
}

# Add column to tasks vs. surveys in correlation table
cor_df$dv = row.names(cor_df)
row.names(cor_df) = seq(1:nrow(cor_df))
cor_df$task = 'task'
cor_df[grep('survey', cor_df$dv), 'task'] = 'survey'
```


Mean Spearman correlation and the number of variables for both the surveys and the tasks
```{r}
datatable(cor_df %>%
  group_by(task) %>%
  summarise(mean_spearman = mean(spearman),
            num_vars = n()))
```

### Distributions of correlations by task vs. survey

Spearman correlations for surveys are noticably higher than task reliabilities.

```{r echo=FALSE}
#Convenience functions to make prettier interactive histograms
get_labels = function(plot_df, ref_df, bin_col, group_col){
  
  plot_df$bin = as.numeric(factor(plot_df$x))
  ref_df$bin = NA
  
  for(i in 1:nrow(ref_df)){
    for(j in 1:nrow(plot_df)){
      if(ref_df[i, bin_col] <= plot_df$xmax[j] & ref_df[i, bin_col]>=plot_df$xmin[j] & all(ref_df[i, group_col] == plot_df[j, group_col])){
        ref_df$bin[i] <- plot_df$bin[j]
      }
    }
  }
  
  ref_df = ref_df %>% 
    group_by_("bin", group_col) %>%
    mutate(label=as.character(toJSON(unique(dv))))
  
  label_df = ref_df[!duplicated(ref_df[,c('label')]),c(group_col, 'bin', 'label')]
  
  out_df = left_join(plot_df,label_df, by= c("bin", group_col))
  
  return(out_df)
}


line_break <- function(cell){
  out = paste(unlist(strsplit(gsub("\\]","",gsub("\\[","",cell)), ",")), sep="\n", collapse = "</br>")
  return(out)
}

prettify_label = function(df){
  for(i in 1:nrow(df)){
    df$label[i] =line_break(df$label[i])
  }
  return(df)
}
```

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(cor_df %>%
                        ggplot(aes(spearman, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,cor_df, "spearman", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('Spearman')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Rank order of correlations 

##### For tasks
```{r}
datatable(cor_df %>%
  filter(task == 'task') %>%
  select(dv, spearman) %>%
  arrange(-spearman))
```

##### For surveys

- Demographics are excluded from this list of variables (because they are saved for predictive analyses).  
- Holt and Laury test-retest is the worst for *surveys*. But note that this is not a survey that has been studied extensively psychometrically like others. This is more similar to our tasks (one that is used more often by economists). In previous data I have worked with retest reliablities for this task was ~.35 so the results here are in line with this.  
- Otherwise the lowest survey reliabilities are 0.5.
```{r}
datatable(cor_df %>%
  filter(task == 'survey') %>%
  select(dv, spearman) %>%
  arrange(-spearman))
```

## ICC for all cleaned meaningful variables

Based on [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) ICC(3,k) does not take in to account within subject differences between two time points (i.e. the fixed effect of time/systematic error). Thus, it is well approximated by Pearson's r and subject to similar criticisms. [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) suggests reporting at least this systematic error effect size if one chooses to report with ICC(3,k). Based on his conclusions here I report:  
- ICC(3,k): As Dave clarified this ranges from 1 to -1/(number of repeated measures -1) so in our case this range would be [-1, 1]; larger values would mean that the two scores of a subject for a given measure are more similar to each other than they are to scores of other people  
- partial $\eta^2$ for time ($SS_{time}/SS_{within}$): effect size of time   
- SEM ($\sqrt(MS_{error})$): standard error of measurement; the smaller the better

```{r}
match_t1_t2 <- function(dv_var, t1_df = retest_subs_test_data, t2_df = retest_data, merge_var = 'sub_id', format = "long"){
  df = merge(t1_df[,c(merge_var, dv_var)], t2_df[,c(merge_var, dv_var)], by = merge_var)
  
  df = df %>% 
    na.omit()%>%
    gather(dv, score, -sub_id) %>%
    mutate(time = ifelse(grepl('\\.x', dv), 1, ifelse(grepl('\\.y', dv), 2, NA))) %>%
    separate(dv, c("dv", "drop"), sep='\\.([^.]*)$') %>%
    select(-drop)
  
  
  if(format == 'wide'){
    df = df%>% spread(time, score) 
  }
  
  return(df)
}

get_icc <- function(dv_var, t1_df = retest_subs_test_data, t2_df = retest_data, merge_var = 'sub_id', match=TRUE, matched_df){
  if(match==TRUE){
    df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var, format='wide')
  }
  else{
    df = matched_df
  }
  df = df %>% select(-dv, -sub_id)
  icc = ICC(df)
  icc_3k = icc$results['Average_fixed_raters', 'ICC']
  return(icc_3k)
  }

get_eta <- function(dv_var, t1_df = retest_subs_test_data, t2_df = retest_data, merge_var = 'sub_id', match=TRUE, matched_df){
  if(match==TRUE){
    df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  }
  else{
    df = matched_df
  }
  mod = summary(aov(score~Error(sub_id)+time, df))
  ss_time = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq1',]
  ss_error = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq2',]
  eta = ss_time/(ss_time+ss_error)
  return(eta)
  }

get_sem <- function(dv_var, t1_df = retest_subs_test_data, t2_df = retest_data, merge_var = 'sub_id', match=TRUE, matched_df){
  if(match==TRUE){
    df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  }
  else{
    df = matched_df
  }
  mod = summary(aov(score~Error(sub_id)+time, df))
  ms_error = as.data.frame(unlist(mod$`Error: Within`))['Mean Sq2',]
  sem = sqrt(ms_error)
  return(sem)
}
```

```{r}
#Calculate ICC df
icc_df <- data.frame(icc = rep(NA, length(numeric_cols)), 
                     eta_sq = rep(NA, length(numeric_cols)),
                     sem = rep(NA, length(numeric_cols)))

row.names(icc_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  icc_df[numeric_cols[i], 'icc'] <- get_icc(numeric_cols[i])
  icc_df[numeric_cols[i], 'eta_sq'] <- get_eta(numeric_cols[i])
  icc_df[numeric_cols[i], 'sem'] <- get_sem(numeric_cols[i])
}

icc_df$dv = row.names(icc_df)
row.names(icc_df) = seq(1:nrow(icc_df))
icc_df$task = 'task'
icc_df[grep('survey', icc_df$dv), 'task'] = 'survey'
icc_df = icc_df %>%
  select(dv, task, icc, eta_sq, sem)
row.names(icc_df) = NULL
```

Median reliability and the number of variables for both the surveys and the tasks:
As seen with Spearman $\rho$'s as well the ICC's are higher for surveys.  
Systematic differences between the two measurements ($\eta^2$) are higher for tasks than surveys (potentially indicating learning effects?).  
Unexpectedly, median standard error of measurement is higher for surveys but the distribution below suggests that this is a side effect of having too many task measures with very low SEMs.
```{r}
datatable(icc_df %>%
  group_by(task) %>%
  summarise(median_icc = median(icc),
            median_eta_sq = median(eta_sq),
            median_sem = median(sem),
            num_vars = n()))
```

#### Rank order of ICCs, partial $\eta^2$ and SEM

Note that these tables can be sorted for any of the columns.

###### For tasks

```{r}
datatable(icc_df %>%
  filter(task == 'task') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-icc) %>%
  select(dv, -task, icc, eta_sq, sem, spearman))
```

###### For surveys

```{r}
datatable(icc_df %>%
  filter(task == 'survey') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-icc) %>%
  select(dv, -task, icc, eta_sq, sem, spearman))
```

### Distributions of ICC's by task vs. survey

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(icc, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "icc", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('ICC')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

### Distributions of partial $\eta^2$ by task vs. survey

Tooltip might not work well for this graph for the highest bar because the label is too large. But this bar indicates measures where the effect of time is very small anyway. We might want to pay closer attention to those not in this lowest bar since those are the variables where there is a non-negligible effect of time, indicating either a learning effect or other kind of systematic change that we did not hypothesize to observe with any of these measures.

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(eta_sq, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "eta_sq", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('Partial Eta Squared')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Effect of time

So what variables show a non-negligible effect of time and in what direction? (sorted by increasing partial $\eta^2$)

```{r warning=FALSE, message=FALSE}
time_effect_vars = ggplot_build(icc_df %>%
               ggplot(aes(eta_sq, fill=task)) +
               geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
  mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
  get_labels(.,icc_df, "eta_sq", "task") %>%
  prettify_label(.) %>%
  filter(bin != 1 & bin !=2 & label != "NA") %>%
  select(label, task) %>%
  separate(label,into=c(as.character(1:27)),sep = "</br>") %>%
  gather(key, value, -task) %>%
  select(value, task) %>%
  na.omit()

time_effect_vars$value = gsub("\"", "", time_effect_vars$value)
datatable(time_effect_vars)
```

Box plots showing the distributions of difference scores between the measurements at two time points for each measure can be found under the [Completion times](#diff_plot) section.

### Distributions of SEM's by task vs. survey

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(sem, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "sem", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('SEM')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

### Negative icc's?
```{r}
neg_icc_vars = icc_df %>%
  arrange(icc) %>% 
  filter(icc<0) %>%
  select(dv)
```

Some variables (`r neg_icc_vars$dv`) have <0 ICC's. This would be the case if the $MS_{error}$>$MS_{between}$. 

Plot data for measures with negative ICC.  
x-axis is time 1 data and y-axis is time 2 data. The scatter plots show that the scores for measures with negative ICC's have no relationship across time points.
```{r, fig.height=12, fig.width=12}
tmp = data.frame()

for(i in 1:nrow(neg_icc_vars)){
  tmp = rbind(tmp, match_t1_t2(neg_icc_vars$dv[i]))
}
rm(neg_icc_vars)

tmp %>%
  spread(time, score) %>%
  # mutate(dv_wrap = str_wrap(dv, width = 30)) %>%
  ggplot(aes(`1`, `2`))+
  geom_point()+
  theme_bw()+
  facet_wrap(~dv, scales='free', labeller = label_wrap_gen())

```

This point is clarified further by compare these scatter plots to those of variables with highest ICC. These variables have strong and clear positive linear relationships.

```{r fig.height=12, fig.width=12}
high_icc_vars = icc_df %>%
  arrange(-icc)  %>% 
  filter(icc>0.92) %>%
  select(dv)

tmp = data.frame()

for(i in 1:nrow(high_icc_vars)){
  tmp = rbind(tmp, match_t1_t2(high_icc_vars$dv[i]))
}
rm(high_icc_vars)

tmp %>%
  spread(time, score) %>%
  ggplot(aes(`1`, `2`))+
  geom_point()+
  theme_bw()+
  facet_wrap(~dv, scales='free')

```

```{r echo=FALSE}
rm(tmp)
```

### Relationship between ICC and Spearman
These two measures give mostly similar answers regarding the reliabilities of the measures except for 

```{r}
ggplotly(icc_df %>%
           left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
           ggplot(aes(spearman, icc, col=task, label=dv, label2=eta_sq))+
           geom_point()+
           theme_bw()+
           theme(legend.title = element_blank())+
           geom_abline(intercept = 0, slope=1))
```

What are the variables clustered around 0 ICC?

```{r}
datatable(icc_df %>%
  filter(icc>(-0.1), icc<0.1) %>%
  arrange(icc))
```

### Relationship between ICC and $\eta^2$

```{r}
ggplotly(icc_df %>%
  ggplot(aes(eta_sq, icc, col=task, label=dv))+
  geom_point()+
  theme_bw()+
  theme(legend.title = element_blank()))
```

## Reliability by measure types

The results presented so far aim to be comprehensive but are difficult to parse through and get a clear sense of what is going on (other than a general impression that surveys tend to have higher reliabilities than tasks). In this and the following section we try to deal with this in two ways to make the results more digestable with two kinds of categorizations. 

### Tasks

We aim to make contact with the literature to make the results more accesible to readers who have experience with our tasks but do not want to   
1. take away from the novelty of the main papers coming out of this project outlining the factor structure that this data unveils   
2. necessarily make prescriptive recommendations or theoretical arguments on why certain measures of certain processes are more reliable than others.  

With this in mind we reduce the list of over 270 task measures to a list of one per task by averaging only the raw measures from all the trials in a task. We chose to reduce the information in this manner to avoid any bias stemming from differential amount of interest and procedures applied to certain tasks over others (e.g. a task can have over 10 measures because it has multiple conditions and we have chosen to fit DDM's for specific conditions while another might only have 2 due to our relative inexperience and lack of interest in it). We check whether the number of trials in a task has a significant effect on these average reliabilities of raw measures as well. 

We plan to supplement/potentially replace this with an additional labeling procedure intending to capture the underlying putative cognitive processes but are deferring this until at least for another week. This exercise might not even prove helpful but at least a per task summary seemed a reasonable first pass.  

```{r echo=FALSE}
measure_labels <- read.csv('/Users/zeynepenkavi/Downloads/retest_measure_labels.csv')
measure_labels = measure_labels %>% select(-Patrick_hates)

trim <- function (x) gsub("^\\s+|\\s+$", "", x)
```

We filter out the DDM parameters and measures for specific contrasts. Note that this does leave some tasks with measures that are model fits and/or for specific conditions (because at least the current datasets do not include measures that are based on all the trials **and** are raw though I could dive in to variables_exhaustive for such measures. For example the average relialibility for Kirby is based on three discount rates for specific conditions.). Here's the order of tasks by mean reliability sorted for ICC and then Spearman's $\rho$.

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task') %>%
  select(-measure_description, -notes, -measure_type, -task, -ddm_task) %>%
  left_join(icc_df[,c("dv", "icc")], by='dv') %>%
  left_join(cor_df[,c("dv", "spearman")], by='dv') %>%
  separate(variable_type, c("label_1", "label_2", "label_3"), sep = ",") %>%
  mutate(label_1 = trim(label_1),
         label_2=trim(label_2),
         label_3=trim(label_3))%>%
  filter(label_1 %in% c('difference', 'EZ', 'hddm') ==FALSE & label_2 %in% c('difference', 'EZ', 'hddm') == FALSE) %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.',remove=FALSE) %>%
  select(-extra_1, -extra_2) %>%
  group_by(task_name) %>%
  summarise(mean_icc = mean(icc),
            mean_spearman = mean(spearman),
            num_measures = n(),
            num_trials = unique(num_all_trials)) %>%
  arrange(-mean_icc, -mean_spearman)

datatable(tmp)
```

Does number of items in a task have a significant effect on the average ICC of (mostly) raw measures for all trials from a task? No.

```{r warning=FALSE, message=FALSE}
summary(lm(mean_icc ~ num_trials, data = tmp))
```

Does number of items in a task have a significant effect on the average Spearman $\rho$ of (mostly) raw measures for all trials from a task? No.

```{r warning=FALSE, message=FALSE}
summary(lm(mean_spearman ~ num_trials, data = tmp))
```

Does number of measures that went in to this reliability estimate have a significant effect on the average ICC of (mostly) raw measures for all trials from a task? No.

```{r warning=FALSE, message=FALSE}
summary(lm(mean_icc ~ num_measures, data = tmp))
```

Does number of measures that went in to this reliability estimate have a significant effect on the average ICC of (mostly) raw measures for all trials from a task?  
The regression suggests that the more measures that went in to the averaging of Spearman's $\rho$'s the lower the mean Spearman $\rho$ for that task. This might suggest that this way of summarizing data might not be a good idea in general or at least that we shouldn't do it for the Spearman correlations.

```{r warning=FALSE, message=FALSE}
summary(lm(mean_spearman ~ num_measures, data = tmp))
```

In addition to average relilibilities for each task we can look at relilibities of measures organized by the putative cognitive process they are intended to measure. Note however that these labels are not determined in a data driven manner.

```{r warning=FALSE, message=FALSE}
datatable(measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task') %>%
  select(-measure_description, -notes, -variable_type, -task, -ddm_task) %>%
  left_join(icc_df[,c("dv", "icc")], by='dv') %>%
  left_join(cor_df[,c("dv", "spearman")], by='dv') %>%
  group_by(measure_type) %>%
  summarise(mean_icc = mean(icc),
            mean_spearman = mean(spearman),
            num_measures = n()) %>%
  arrange(-mean_icc, -mean_spearman))
```

Here is what the distributions look like.  
- Alerting: This labels consists only of ANT alerting contrast (no cue - double cue trials). 4/6 are drift diffusion parameters.    
- Cue encoding: This label consists only of three by two task switch cost contrast (stay (where cue and task remains the same) - switch (where task is the same but cue changes) trials). Again most are drift diffusion parameters.  
- Delay discounting: This label consists of the discount rates for the Bickel titrator (3), Kirby items (3) and discount titrate (model fit to random set of stimuli). **Does the discrepancy between ICC and Spearman for these measures, given the specific interest in these tasks in the literature warrant a more detailed look in to these?**  
- Information use: This label includes Columbia Card Task (both versions) and information sampling task.  
- Inhibition: This label includes go no-go, motor selective stop signal, stimulus selective stop signal and regular stop signal (excluding specific constrasts).  
- Intelligence: This label includes ravens and tower of london.  
- Learning: This label includes ART, CCT, two stage decision, dietary decision task, hierarchical rule task, information sampling task, probabilistic selection task, shift task, two stage task.   
- Neutral writing: This label consists only of the writing task.  
- Orienting: This labels consists only of ANT orienting contrast (center - spatial cue trials). 4/6 are drift diffusion parameters. 
- Positive writing: This label consists only of the writing task.  
- Proactive control: This labels consists of one DPX contrast (AY - BY trials), motor selective stop signal and regular stop signal.   
- Reactive control: This labels consists of one DPX contrast (BX - BY trials) and motor selective stop signal.  
- Resisting proactive interference: This label consists of one contrast in directed forgetting and recent probes tasks (negative (where the stimulus to respond to is in forgetting set) and control (where the stimulus to respond to is new) trials).  
- Response conflict: This label consists of ANT conflict contrast (congruent-incongruent), DPX, local global conflict contrast, shape matching, simon, stroop.  
- Response selection: This label consists of choice reaction time task.  
- Risk Taking: This label consists of ART and CCT.
- Selective attention: This label consists of local global task and ANT (all trials).  
- Set shifting: This label consists of local global, PRT and three by two.    
- Speed: This label consists of raw response times in information sampling task, simple reaction time, tower of london.  
- Working memory: This label consists of adaptive n back, digit span, directed forgetting, keep track, recent probes, spatial span.

```{r warning=FALSE, message=FALSE, fig.width=12, fig.height=10}
measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task') %>%
  select(-measure_description, -notes, -variable_type, -task, -ddm_task) %>%
  left_join(icc_df[,c("dv", "icc")], by='dv') %>%
  left_join(cor_df[,c("dv", "spearman")], by='dv') %>%
  select(measure_type, icc, spearman) %>%
  gather(key, value, -measure_type) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(position='identity', alpha=0.5)+
  theme_bw()+
  facet_wrap(~measure_type)+
  theme(legend.title = element_blank())
```

### Surveys

To be filled with factor labels after processing either ASU factor analyses or Russ' MIRT?

## Reliability by variable types

In this section we label the measures based on the procedure behind the creation of a variable. This moves further away from the literature and thinking about our measures in terms of specific tasks, surveys or cognitive processes and is a more novel way of presenting our data. 

Potentially prescriptive recommendations coming out of this section would intend to build intuitions about different treatments to data that have higher reliabilities especially when researchers plan on making their own tasks and collecting different kinds of measures.
 
### Tasks

#### DDM parameters

```{r warning=FALSE, message=FALSE}
tmp = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  filter(task == 'task') %>%
  select(-measure_description, -notes, -measure_type, -task) %>%
  left_join(icc_df[,c("dv", "icc")], by='dv') %>%
  left_join(cor_df[,c("dv", "spearman")], by='dv') %>%
  separate(variable_type, c("label_1", "label_2", "label_3"), sep = ",") %>%
  mutate(label_1 = trim(label_1),
         label_2=trim(label_2),
         label_3=trim(label_3))%>%
  filter(ddm_task == 1) %>%
  separate(dv, c('task_name', 'extra_1', 'extra_2'), sep = '\\.', remove=FALSE) %>%
  select(-extra_1, -extra_2) %>%
  drop_na()
```

In addition to raw response time and accuracy measures we get from certain tasks we have also chosen to fit drift diffusion models to them. The tasks we have chosen to fit these models are: `unique(tmp$task_name)`. This list does not include all cognitive tasks in the battery.

Table of median reliabilities for the two kinds of DDM models' (hddm and EZ) parameters compared to raw response times and accuracies depending on whether they were fit to all data or whether they are contrast of two conditions sorted by mean ICC and mean Spearman's $\rho$.

- All parameters that were fit to all trials compared to contrasts have higher relialibilities.   
- Raw variables are not the variables with highest ICC's overall. Raw response times do, however, have the highest Spearman's $\rho$'s on average. 
- 2 of 3 hddm parameters also have higher mean reliabilities than EZ parameter estimates.  

```{r warning=FALSE, message=FALSE}
datatable(tmp %>%
  group_by(label_1,label_2, label_3) %>%
  summarise(mean_icc = mean(icc),
            mean_spearman = mean(spearman),
            n_vars = n()) %>%
  arrange(-mean_icc, -mean_spearman))
```

This is what the distributions of look like. Note that there is one box in the difference columns for HDDM because this model only allows this to be different across conditions.

```{r warning=FALSE, message=FALSE}
tmp = tmp %>%
  select(dv, icc, spearman, label_1, label_2, label_3) %>%
  gather(key, value, -dv, -label_1, -label_2, -label_3) 

tmp %>% ggplot(aes(label_2, value, fill=label_3))+
  geom_boxplot()+
  theme_bw()+
  facet_wrap(key~label_1)+
  labs(fill="Parameter")
```

```{r message=FALSE, echo=FALSE, eval=FALSE}
#attempt to make the above interactive. didn't really work
ggplotly(ggplot_build(tmp %>%
                        ggplot(aes(value, color=label_2, fill=label_3))+
                        geom_histogram(position='identity', alpha = 0.5)+
                        theme_bw()+
                        facet_wrap(key~label_1)+
                        labs(color="Model", fill="Parameter"))$data[[1]] %>%
           mutate(key = ifelse(PANEL %in%  c(1,2), "icc", "spearman"),
                  label_1 = ifelse(PANEL %in%  c(1,3), "all", "difference"),
                  label_2 = ifelse(colour == "#F8766D", "EZ", ifelse(colour == "#00BA38", "hddm", "raw")),
                  label_3 = ifelse(fill == "#A3A500", "drift rate", ifelse(fill == "#00BF7D", "non-decision", ifelse(fill == "#E76BF3", "threshold", ifelse(fill == "#F8766D", "accuracy", "rt"))))) %>%
           get_labels(.,tmp, "value", c("label_1", "label_2", "label_3", "key")) %>%
           prettify_label(.) %>%
           ggplot(aes(x, y, fill = label_3, color=label_2, label=label))+
           geom_bar(stat='identity',position='identity', alpha=0.5)+
           facet_wrap(key~label_1)+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```
### Surveys

Would this be a comparison of factor reliability versus item reliability?

## Completion times

Do people differ in how much their scores change depending on how many days it has been since they completed the initial battery?  

Make data frame with difference between two scores for each measure for each subject. Since the scores for different measures are on different scales for comparability the difference scores are scaled (i.e. divided by their root mean square) but not centered so a value of 0 for the difference scores would indicate a lack of a difference between the scores of a subject.

```{r warning=FALSE}
t1_2_difference = data.frame()

for(i in 1:length(numeric_cols)){
  tmp = match_t1_t2(numeric_cols[i],format='wide')
  tmp = tmp %>% 
    # mutate(difference = scale(`2` - `1`))
  mutate(difference = scale(`2` - `1`, center=F))
  t1_2_difference = rbind(t1_2_difference, tmp)
}

t1_2_difference$difference = as.data.frame(t1_2_difference$difference)$V1
```

```{r echo=FALSE}
rm(tmp, i)
```

Add completion dates to this data frame.

```{r warning=FALSE}
t1_2_difference = merge(t1_2_difference, comp_dates[,c('sub_id', 'days_btw')], by='sub_id')
```

```{r echo=FALSE}
rm(test_comp_date, retest_comp_date, comp_dates)
```

The effect of number of days in between in the full model where the difference scores are regressed on a fixed effect for measure and days between the two scores and random intercepts for each subject (*Should this model include the interaction between the fixed effects?*). Since there are over 300 measures the output of the full model is not presented here. Instead below are the coefficient for the fixed effect of days between completion times and its t value.

```{r}
mod = lmer(difference ~ dv + days_btw + (1|sub_id), data = t1_2_difference)

data.frame(estimate=coef(summary(mod))["days_btw","Estimate"], tval=coef(summary(mod))["days_btw","t value"])
```

For visualization purposes I summarized the difference scores per person by looking at the average difference and plot that against the number of days between completion.

```{r warning=FALSE, message=FALSE}
tmp = t1_2_difference %>%
  group_by(sub_id) %>%
  summarise(mean_diff = mean(difference, na.rm=T),
            days_btw = unique(days_btw))

ggplotly(tmp %>%
  ggplot(aes(days_btw, mean_diff))+
  geom_point()+
  theme_bw()+
  xlab("Days between completion")+
  ylab("Mean standardized difference between two time points")+
  geom_smooth(method="lm"))
```

To confirm: the slope of this line is not significant. That is, there doesn't seem to be a systematic difference between the two measurements depending on the number of days between the two measurements.

```{r warning=FALSE, message=FALSE}
summary(lm(mean_diff ~ days_btw, data=tmp))
```

<a id="diff_plot"></a>
For reference here are the box plots showing the distribution of difference scores for each task.

```{r warning=FALSE, message=FALSE, fig.height=36}
t1_2_difference %>%
           ggplot(aes(reorder(dv, difference, FUN=median), difference))+
           geom_hline(yintercept = 0,color='red',size=1)+
           geom_boxplot()+
           theme_bw()+
           xlab("")+
           ylab("Standardized difference")+
           coord_flip()
```


Bootstrapping

```{r eval=FALSE, echo=FALSE}
sample_workers = function(N = 150, repl= TRUE, df=retest_data, worker_col = "sub_id"){
  return(sample(df[,worker_col], N, replace = repl))
}

extract_sample_data = function(sample, var_name, df, worker_col = "sub_id"){
  return(df[df[,worker_col] %in% sample, var_name])
}

sample_workers()
extract_sample_data(df=retest_data)
extract_sample_data(df=retest_subs_test_data)
```
