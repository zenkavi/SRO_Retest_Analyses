---
title: 'Self Regulation Ontology Retest Data Report'
output:
html_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(lme4)
library(GGally)
library(jsonlite)
library(lavaan)
library(semTools)
library(psych)
library(GPArotation)
library(rmarkdown)
library(psych)
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
render_this <- function(){rmarkdown::render('SRO_Report.Rmd', html_notebook(toc = T, toc_float = T))}
```

# Introduction

# Methods

## Loading and matching datasets

This report uses the variables that were designated as "meaningful" before. This is not the smallest subset where certain variables are dropped because they are correlated with other variables (within a task) and includes both accuracies as well as both kinds of DDM variables (EZ and HDDM). It will not include variables that were considered not of theoretical interest in the analyses of time 1 data. The variables that are not included will be listed for reference as well.

### Load time 1 data
```{r}
test_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables.csv')

test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_noDDM.csv')

test_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_EZ.csv')

test_data <- merge(test_data, test_data2)
test_data <- merge(test_data, test_data3)
rm(test_data2, test_data3)
```

For reference here are the variables that are **not** included in the analyses of the remainder of this report because they were not of theoretical interest in factor structure analyses of this data so far. These include drift diffusion and other model parameters for specific conditions within a task; survey variables that are not part of the dependant variables for that survey in the literature and demographics (these are saved for prediction analyses).

```{r}
test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/variables_exhaustive.csv')

test_data2$X <- as.character(test_data2$X)

names(test_data2)[1] <- 'sub_id'

df <- data.frame(names(test_data2)[which(names(test_data2) %in% names(test_data) == FALSE)])
names(df) = c('vars')

df
```

```{r echo=FALSE}
rm(test_data2, df)
```

### Load time 2 data 
```{r}
retest_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables.csv')

retest_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables_noDDM.csv')

retest_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables_EZ.csv')

retest_data <- merge(retest_data, retest_data2)
retest_data <- merge(retest_data, retest_data3)
rm(retest_data2, retest_data3)
```

Extract t2 participants with good t1 data: We only invited those with good data but due to a technical error a handful of subjects who weren't checked for data quality completed the battery as well. One of those subjects gave good t2 data but didn't have good t1 data and therefore is removed from further analyses.

```{r}
#Process sub_id columns in the data dataframes
retest_data$X <- as.character(retest_data$X)
test_data$X <- as.character(test_data$X)

names(retest_data)[1] <- 'sub_id'
names(test_data)[1] <- 'sub_id'

retest_data <- retest_data[retest_data$sub_id %in% test_data$sub_id,]
```

### Clean t1 data

The cleaning functions here are translated from the python pipeline to mimick the same procedure that was applied to the `meaningful_variables` in creating `meaningful_variables_clean`.

The original cleaning procedures include three steps:  
1. Removing outliers for each variables where outliers are defined as those who are greater than 2.5 IQR from median. **This step is skipped here because removing outliers from either dataset could lead to an unnecessary increase in missing data. Since we are interested in how close two measures of the metric for a given subject are to each other instead of how close the measurements for all these metrics are to each other this seemed appropriate.**  
2. Removing correlated variables *within* a task. In creating `meaningful_variables_clean` if variables for a given task correlated >.85 only one was kept. **This step is skipped here because we want to look at the reliability of as many variables as we can.**
3. Log transforming variables with skew. In cleaning t1 data variables that had a skew > 1 were log transformed and those for which skew could not be removed successfully were dropped. **Here we transform all variables that have a skew>1 in t1 data but variables that are not successfully transformed will be listed but not dropped.**

```{r}
remove_outliers = function(data_column, quantile_range = 2.5){
  
  q_25 = quantile(data_column, na.rm=T)[2]
  q_50 = quantile(data_column, na.rm=T)[3]
  q_75 = quantile(data_column, na.rm=T)[4]
  
  lowlimit = q_50 - quantile_range*(q_75 - q_25)
  highlimit = q_50 + quantile_range*(q_75 - q_25)
  
  data_column = ifelse(data_column<lowlimit, NA, ifelse(data_column>highlimit, NA, data_column))
  
  return(data_column)
}

"%w/o%" <- function(x, y) x[!x %in% y]

neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}

transform_remove_skew = function(data, columns, threshold = 1, drop=FALSE){
  
  tmp = as.data.frame(apply(data[,columns],2,skew))
  names(tmp) = c("skew")
  tmp$dv = row.names(tmp)
  tmp = tmp %>% 
    filter(abs(skew)>threshold)
  
  skewed_variables = tmp$dv
  skew_subset = data[, skewed_variables]
  positive_subset = data[,tmp$dv[tmp$skew>0]]
  negative_subset = data[,tmp$dv[tmp$skew<0]]
  
  # transform variables
  # log transform for positive skew
  positive_subset = log(positive_subset)
  successful_transforms = as.data.frame(apply(positive_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<threshold)
  successful_transforms = successful_transforms$dv
  successful_transforms = positive_subset[,successful_transforms]
  dropped_vars = names(positive_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(positive_subset)) ,' data positively skewed data were transformed:'))
  cat('\n')
  cat(names(positive_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(positive_subset)))]
  
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' positively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(positive_subset) = paste0(names(positive_subset), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' positively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, positive_subset)
  }
  
  
  # reflected log transform for negative skew      
  negative_subset = as.data.frame(apply(negative_subset, 2, neg_log))
  successful_transforms = as.data.frame(apply(negative_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<1)
  successful_transforms = successful_transforms$dv
  successful_transforms = negative_subset[,successful_transforms]
  dropped_vars = names(negative_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(negative_subset)) ,' data negatively skewed data were transformed:'))
  cat('\n')
  cat(names(negative_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(negative_subset)))]
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' negatively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(negative_subset) = paste0(names(negative_subset), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' negatively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, negative_subset)
  }
  
  return(data)
}
```

Clean test data on full sample without dropping any columns. Columns that are not transformed successfully are listed.

```{r warning=FALSE}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

# clean_test_data = cbind(sub_id = test_data$sub_id, as.data.frame(apply(test_data[, -which(names(test_data) %in% c("sub_id"))], 2, remove_outliers)))

# clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
clean_test_data = transform_remove_skew(test_data, numeric_cols)
```

```{r echo=FALSE}
rm(i)
```

### Clean t2 data

Instead of applying the same functions applied to t1 data we check which variables are transformed in what way and transform the t2 data the same way.

**Question: Should I be removing outliers from the retest data? Or keep them if they were within the ranges in the test data even if they aren't in the retest data. Right now I'm removing them depending on t2 distributions**

Remove outliers from retest data the way they have been from test data
```{r}
clean_retest_data = cbind(sub_id = retest_data$sub_id, as.data.frame(apply(retest_data[, -which(names(retest_data) %in% c("sub_id"))], 2, remove_outliers)))
```

Get list of transformed variables and the sign of their skew
```{r}
logTr_vars = data.frame(grep('logTr', names(clean_test_data), value=T))
names(logTr_vars) = c('var')
logTr_vars$sign = ifelse(grepl(".ReflogTr",logTr_vars$var), 'negative', 'positive')
logTr_vars$var = gsub(".logTr|.ReflogTr", "", logTr_vars$var)
```

Transform the t2 columns the same way t1 columns were transformed.

```{r}
for(i in 1:length(names(clean_retest_data))){
  tmp_col = names(clean_retest_data)[i]
  if(tmp_col %in% logTr_vars$var){
    sign = logTr_vars$sign[which(tmp_col == logTr_vars$var)]
      if(sign == 'positive'){
        clean_retest_data[,tmp_col] = log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.logTr')
        cat(paste0(tmp_col ,' was positively skewed and transformed.'))
        cat('\n')
      }
      else if(sign == 'negative'){
        clean_retest_data[,tmp_col] = neg_log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.ReflogTr')
        cat(paste0(tmp_col ,' was negatively skewed and transformed.'))
        cat('\n')
      }
    }
}
```

```{r echo = FALSE}
rm(i, sign, tmp_col)
```

Check transformations are correct
```{r}
retest_logTr_vars = data.frame(grep('logTr', names(clean_retest_data), value=T))
names(retest_logTr_vars) = c('var')
retest_logTr_vars$sign = ifelse(grepl(".ReflogTr",retest_logTr_vars$var), 'negative', 'positive')
retest_logTr_vars$var = gsub(".logTr|.ReflogTr", "", retest_logTr_vars$var)
retest_logTr_vars = retest_logTr_vars %>% arrange(var)
logTr_vars = logTr_vars %>% arrange(var)
retest_logTr_vars == logTr_vars
```


```{r echo=F}
rm(logTr_vars, retest_logTr_vars)
```

Switch df names

```{r}
raw_retest_data = retest_data
raw_test_data = test_data
test_data = clean_test_data
retest_data = clean_retest_data
rm(clean_test_data, clean_retest_data)
```

### Extract t-1 data for t-2 subjects
```{r}
retest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id,]

rest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id == FALSE,]

#Arrange datasets of same size by sub_id
retest_data = retest_data %>% arrange(sub_id)
retest_subs_test_data = retest_subs_test_data %>% arrange(sub_id)

#CHECK IF EVERYTHING IS ORDERED RIGHT
as.character(retest_subs_test_data$sub_id) == as.character(retest_data$sub_id)
```

# Results

## Correlations for all cleaned meaningful variables
Spearman only

```{r}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

cor_df <- data.frame(spearman = rep(NA, length(numeric_cols)))

row.names(cor_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  cor_df[numeric_cols[i], 'spearman'] <- cor(retest_data[,numeric_cols[i]],retest_subs_test_data[,numeric_cols[i]], method = 'spearman', use = "pairwise.complete.obs")
}
```

Distribution of Spearman correlations
```{r warning=FALSE, message=FALSE}
cor_df %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  ggtitle('Distribution of Spearman correlations for cleaned meaningful variables')
```

Add column to tasks vs. surveys in correlation table

```{r}
cor_df$dv = row.names(cor_df)
row.names(cor_df) = seq(1:nrow(cor_df))
cor_df$task = 'task'
cor_df[grep('survey', cor_df$dv), 'task'] = 'survey'
```

### Distributions of correlations by task vs. survey

Survey reliabilities are noticably higher than task reliabilities.
```{r warning=FALSE, message=FALSE}
cor_df %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~task)
```

Mean reliability and the number of variables for both the surveys and the tasks
```{r}
cor_df %>%
  group_by(task) %>%
  summarise(mean_spearman = mean(spearman),
            num_vars = n())
```

#### Rank order of correlations 

##### For tasks
```{r}
cor_df %>%
  filter(task == 'task') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

##### For surveys

- Demographics are excluded from this list of variables.  
- Holt and Laury test-retest is the worst for *surveys*. But note that this is not a survey that has been studied extensively psychometrically like others. This is more similar to out tasks (one that is used more often by economists). In previous data I have worked retest reliablities for this task was ~.35 so the results here are in line with this.  
- Otherwise the lowest survey reliabilities are 0.5.
```{r}
cor_df %>%
  filter(task == 'survey') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

## ICC for all cleaned meaningful variables

Based on [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) ICC(3,k) does not take in to account within subject differences between two time points (i.e. the fixed effect of time/systematic error). Thus, it is well approximated by Pearson's r and subject to similar criticisms. [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) suggests reporting at least this effect size if one chooses to report with ICC(3,k). Based on his conclusions here I report:  
- ICC(3,k): ranges between 0-1; the larger the better  
- partial $\eta^2$ for time ($SS_{time}/SS_{within}$): effect size of time 
- SEM ($sqrt(MS_{error})$): standard error of measurement; the smaller the better

```{r}
match_t1_t2 <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = merge(t1_df[,c(merge_var, dv_var)], t2_df[,c(merge_var, dv_var)], by = merge_var)
  df = df %>% 
    na.omit()%>%
    gather(dv, score, -sub_id) %>%
    mutate(time = ifelse(grepl('\\.x', dv), 1, ifelse(grepl('\\.y', dv), 2, NA))) %>%
    separate(dv, c("dv", "drop"), sep='\\.([^.]*)$') %>%
    select(-drop)
   return(df)
}

get_icc <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  df = df %>% spread(time, score) %>% select(-dv, -sub_id)
  icc = ICC(df)
  icc_3k = icc$results['Average_fixed_raters', 'ICC']
  return(icc_3k)
  }

get_eta <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  mod = summary(aov(score~Error(sub_id)+time, df))
  ss_time = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq1',]
  ss_error = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq2',]
  eta = ss_time/(ss_time+ss_error)
  return(eta)
  }

get_sem <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  mod = summary(aov(score~Error(sub_id)+time, df))
  ms_error = as.data.frame(unlist(mod$`Error: Within`))['Mean Sq2',]
  sem = sqrt(ms_error)
  return(sem)
}
```


```{r}
icc_df <- data.frame(icc = rep(NA, length(numeric_cols)), 
                     eta_sq = rep(NA, length(numeric_cols)),
                     sem = rep(NA, length(numeric_cols)))

row.names(icc_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  icc_df[numeric_cols[i], 'icc'] <- get_icc(numeric_cols[i])
  icc_df[numeric_cols[i], 'eta_sq'] <- get_eta(numeric_cols[i])
  icc_df[numeric_cols[i], 'sem'] <- get_sem(numeric_cols[i])
}
```

Add column to tasks vs. surveys in icc table

```{r}
icc_df$dv = row.names(icc_df)
row.names(icc_df) = seq(1:nrow(icc_df))
icc_df$task = 'task'
icc_df[grep('survey', icc_df$dv), 'task'] = 'survey'
icc_df
```

### Distributions of ICC's by task vs. survey

Survey reliabilities are noticably higher than task reliabilities.
```{r warning=FALSE, message=FALSE}
icc_df %>%
  gather(key,value,-dv, -task) %>%
  ggplot(aes(value, fill=task))+
  geom_histogram(position = 'stack', alpha=0.5)+
  theme_bw()+
  facet_wrap(~key,scales='free')
```

Median reliability and the number of variables for both the surveys and the tasks
```{r}
icc_df %>%
  group_by(task) %>%
  summarise(median_icc = median(icc),
            median_eta_sq = median(eta_sq),
            median_sem = median(sem),
            num_vars = n())
```

----------------------------------------------------------------------------------------------------------------

## Different task variable types

Mean reliabilities for different types of dependant variables

### Drift diffusion variables: raw vars vs. EZ vs. hddm

**DO THIS ON VARIABLES EXHAUSTIVE GETTING ALL RT AND ACC'S**

```{r message=FALSE, warning=FALSE, fig.height=12, fig.width=9}
cor_df%>%
  filter(grepl('EZ|hddm|_rt|_acc', dv)) %>%
  mutate(var_type = ifelse(grepl('EZ',dv), 'EZ', ifelse(grepl('hddm',dv), 'hddm', 'raw')),
         var = ifelse(grepl('_rt', dv), 'rt', ifelse(grepl('_acc', dv), 'acc', ifelse(grepl('_non_decision', dv), 'non_decision', ifelse(grepl('_thresh', dv), 'thresh', ifelse(grepl('_drift', dv), 'drift', NA)))))) %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(var_type~var)
```

```{r}
cor_df%>%
  filter(grepl('EZ|hddm|_rt|_acc', dv)) %>%
  mutate(var_type = ifelse(grepl('EZ',dv), 'EZ', ifelse(grepl('hddm',dv), 'hddm', 'raw')),
         var = ifelse(grepl('_rt', dv), 'rt', ifelse(grepl('_acc', dv), 'acc', ifelse(grepl('_non_decision', dv), 'non_decision', ifelse(grepl('_thresh', dv), 'thresh', ifelse(grepl('_drift', dv), 'drift', NA)))))) %>%
  group_by(var_type, var) %>%
  summarise(mean_spearman = mean(spearman, na.rm=T),
            sem_spear = sem(spearman),
            num_vars = sum(ifelse(is.na(spearman)==FALSE,1,0))) %>%
  arrange(var_type)
```

## Replicating the factor structure

### Confirm comparability of retest and rest sample

First do a t-test on all measures of T1 for subjects with and without retest to confirm that these two samples do not differ from each other significantly (FDR corrected)
```{r}
ttest_df = data.frame(t_stat = rep(NA, length(matching_dv_columns)),
                      p_val_raw = rep(NA, length(matching_dv_columns)))

row.names(ttest_df) <- matching_dv_columns

for(i in 1:length(matching_dv_columns)){
  ttest_df[matching_dv_columns[i], 't_stat'] <- t.test(retest_subs_test_data[,matching_dv_columns[i]], rest_subs_test_data[,matching_dv_columns[i]])$statistic
  
  ttest_df[matching_dv_columns[i], 'p_val_raw'] <- t.test(retest_subs_test_data[,matching_dv_columns[i]], rest_subs_test_data[,matching_dv_columns[i]])$p.value
}

ttest_df$dv = row.names(ttest_df)
row.names(ttest_df) = seq(1:nrow(ttest_df))
ttest_df$task = 'task'
ttest_df[grep('survey', ttest_df$dv), 'task'] = 'survey'

#correct p-values 
ttest_df$p_val_fdr = p.adjust(ttest_df$p_val_raw, method='fdr')
```

```{r echo=FALSE}
rm(i)
```

Distribution of t statistics and p values

```{r warning=FALSE, message=FALSE}
ttest_df %>%
  gather(key, value, -dv, -task) %>%
  ggplot(aes(value)) +
  geom_histogram()+
  theme_bw()+
  facet_wrap(key~task, scales='free')
```

**Which variables are significantly different between the two samples (those with and without retest data for T1)?**

13 variables have raw p values <0.05 BUT FDR corrected p values for these are not <0.05.

```{r}
ttest_df %>%
  filter(p_val_raw<0.05) %>%
  select(dv, task, t_stat, p_val_raw, p_val_fdr)
```

