---
title: 'Self Regulation Ontology Retest Data Report'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(lme4)
library(GGally)
library(jsonlite)
library(psych)
library(rmarkdown)
library(psych)
library(stringr)
library(plotly)
library(jsonlite)
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
render_this <- function(){rmarkdown::render('SRO_Report.Rmd', output_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Report', html_notebook(toc = T, toc_float = T, code_folding = 'hide'))}
```

# Introduction

# Methods

## Loading and matching datasets

This report uses the variables that were designated as "meaningful" before. This is not the smallest subset where certain variables are dropped because they are correlated with other variables (within a task) and includes both accuracies as well as both kinds of DDM variables (EZ and HDDM). It will not include variables that were considered not of theoretical interest in the analyses of time 1 data. The variables that are not included will be listed for reference as well.

### Load time 1 data
```{r}
test_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables.csv')

test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_noDDM.csv')

test_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_EZ.csv')

test_data <- merge(test_data, test_data2)
test_data <- merge(test_data, test_data3)
rm(test_data2, test_data3)
```

For reference here are the variables that are **not** included in the analyses of the remainder of this report because they were not of theoretical interest in factor structure analyses of this data so far. These include drift diffusion and other model parameters for specific conditions within a task; survey variables that are not part of the dependant variables for that survey in the literature and demographics (these are saved for prediction analyses).

```{r}
test_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/variables_exhaustive.csv')

test_data2$X <- as.character(test_data2$X)

names(test_data2)[1] <- 'sub_id'

df <- data.frame(names(test_data2)[which(names(test_data2) %in% names(test_data) == FALSE)])
names(df) = c('vars')

df
```

```{r echo=FALSE}
rm(test_data2, df)
```

### Load time 2 data 
```{r}
retest_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables.csv')

retest_data2 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables_noDDM.csv')

retest_data3 <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/meaningful_variables_EZ.csv')

retest_data <- merge(retest_data, retest_data2)
retest_data <- merge(retest_data, retest_data3)
rm(retest_data2, retest_data3)
```

Extract t2 participants with good t1 data: We only invited those with good data but due to a technical error a handful of subjects who weren't checked for data quality completed the retest battery as well. One of those subjects gave good t2 data but didn't have good t1 data and therefore is removed from further analyses.

```{r}
#Process sub_id columns in the data dataframes
retest_data$X <- as.character(retest_data$X)
test_data$X <- as.character(test_data$X)

names(retest_data)[1] <- 'sub_id'
names(test_data)[1] <- 'sub_id'

retest_data <- retest_data[retest_data$sub_id %in% test_data$sub_id,]
```

### Clean t1 data

The cleaning functions here are translated from the python pipeline to mimick the same procedure that was applied to the `meaningful_variables` in creating `meaningful_variables_clean`.

The original cleaning procedures include three steps:  
1. Removing outliers for each variable where outliers are defined as those who are greater than 2.5 IQR from median. **This step is skipped here because removing outliers from either dataset could lead to an unnecessary increase in missing data. Since we are interested in the average distance between two measures of a metric for a given subject instead of the distance between all these metrics this seemed appropriate.**  
2. Removing correlated variables *within* a task. In creating `meaningful_variables_clean` if variables for a given task correlated >.85 only one was kept. **This step is skipped here because we want to look at the reliability of as many variables as we can.**  
3. Log transforming variables with skew. In cleaning t1 data variables that had a |skew| > 1 were log transformed and those for which skew could not be removed successfully were dropped. **Here we transform all variables that have a |skew|>1 in t1 data but variables that are not successfully transformed will be listed and not dropped.**

```{r}
remove_outliers = function(data_column, quantile_range = 2.5){
  
  q_25 = quantile(data_column, na.rm=T)[2]
  q_50 = quantile(data_column, na.rm=T)[3]
  q_75 = quantile(data_column, na.rm=T)[4]
  
  lowlimit = q_50 - quantile_range*(q_75 - q_25)
  highlimit = q_50 + quantile_range*(q_75 - q_25)
  
  data_column = ifelse(data_column<lowlimit, NA, ifelse(data_column>highlimit, NA, data_column))
  
  return(data_column)
}

"%w/o%" <- function(x, y) x[!x %in% y]

pos_log <- function(column){
  col_min = min(column, na.rm=T)
  a = 1-col_min
  column = column+a
  return(log(column))
}

neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}

transform_remove_skew = function(data, columns, threshold = 1, drop=FALSE){
  
  tmp = as.data.frame(apply(data[,columns],2,skew))
  names(tmp) = c("skew")
  tmp$dv = row.names(tmp)
  tmp = tmp %>% 
    filter(abs(skew)>threshold)
  
  skewed_variables = tmp$dv
  skew_subset = data[, skewed_variables]
  positive_subset = data[,tmp$dv[tmp$skew>0]]
  negative_subset = data[,tmp$dv[tmp$skew<0]]
  
  # transform variables
  # log transform for positive skew
  # positive_subset = log(positive_subset)
  positive_subset = pos_log(positive_subset) #slight divergence from original code
  successful_transforms = as.data.frame(apply(positive_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<threshold)
  successful_transforms = successful_transforms$dv
  successful_transforms = positive_subset[,successful_transforms]
  dropped_vars = names(positive_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(positive_subset)) ,' data positively skewed data were transformed:'))
  cat('\n')
  cat(names(positive_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(positive_subset)))]
  
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' positively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(positive_subset) = paste0(names(positive_subset), '.logTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' positively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, positive_subset)
  }
  
  
  # reflected log transform for negative skew      
  negative_subset = as.data.frame(apply(negative_subset, 2, neg_log))
  successful_transforms = as.data.frame(apply(negative_subset, 2, skew))
  names(successful_transforms) = c('skew')
  successful_transforms$dv = row.names(successful_transforms)
  successful_transforms = successful_transforms %>% filter(abs(skew)<1)
  successful_transforms = successful_transforms$dv
  successful_transforms = negative_subset[,successful_transforms]
  dropped_vars = names(negative_subset) %w/o% names(successful_transforms)
  
  cat(rep('*', 40))
  cat('\n')
  cat(paste0(length(names(negative_subset)) ,' data negatively skewed data were transformed:'))
  cat('\n')
  cat(names(negative_subset), sep = '\n')
  cat(rep('*', 40))
  cat('\n')
  
  # replace transformed variables
  data = data[,-c(which(names(data) %in% names(negative_subset)))]
  if(drop == TRUE){
    names(successful_transforms) = paste0(names(successful_transforms), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0('Dropping ', length(dropped_vars) ,' negatively skewed data that could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, successful_transforms)
  }
  else{
    names(negative_subset) = paste0(names(negative_subset), '.ReflogTr')
    cat(rep('*', 40))
    cat('\n')
    cat(paste0(length(dropped_vars) ,' negatively skewed data could not be transformed successfully:'))
    cat('\n')
    cat(dropped_vars, sep = '\n')
    cat(rep('*', 40))
    cat('\n')
    data = cbind(data, negative_subset)
  }
  
  return(data)
}
```

Clean t1 data on full sample without dropping any columns. Columns that are not transformed successfully are listed.

```{r warning=FALSE}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

# clean_test_data = cbind(sub_id = test_data$sub_id, as.data.frame(apply(test_data[, -which(names(test_data) %in% c("sub_id"))], 2, remove_outliers)))

# clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
clean_test_data = transform_remove_skew(test_data, numeric_cols)
```

```{r echo=FALSE}
rm(i)
```

### Clean t2 data

Instead of applying the same functions applied to t1 data we check which variables are transformed in what way and transform the t2 data the same way. Again we are not removing outliers in the t2 or dropping variables if the skew is not removed successfully after transforming.

Get list of transformed variables and the sign of their skew
```{r}
logTr_vars = data.frame(grep('logTr', names(clean_test_data), value=T))
names(logTr_vars) = c('var')
logTr_vars$sign = ifelse(grepl(".ReflogTr",logTr_vars$var), 'negative', 'positive')
logTr_vars$var = gsub(".logTr|.ReflogTr", "", logTr_vars$var)
```

Transform the t2 columns the same way t1 columns were transformed.

```{r warning=FALSE}
clean_retest_data = retest_data

for(i in 1:length(names(clean_retest_data))){
  tmp_col = names(clean_retest_data)[i]
  if(tmp_col %in% logTr_vars$var){
    sign = logTr_vars$sign[which(tmp_col == logTr_vars$var)]
      if(sign == 'positive'){
        # clean_retest_data[,tmp_col] = log(clean_retest_data[,tmp_col])
        clean_retest_data[,tmp_col] = pos_log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.logTr')
        cat(paste0(tmp_col ,' was positively skewed and transformed.'))
        cat('\n')
      }
      else if(sign == 'negative'){
        clean_retest_data[,tmp_col] = neg_log(clean_retest_data[,tmp_col])
        names(clean_retest_data)[which(names(clean_retest_data) == tmp_col)] = paste0(tmp_col, '.ReflogTr')
        cat(paste0(tmp_col ,' was negatively skewed and transformed.'))
        cat('\n')
      }
    }
}
```

```{r echo = FALSE}
rm(i, sign, tmp_col)
```

Check transformations are correct
```{r}
retest_logTr_vars = data.frame(grep('logTr', names(clean_retest_data), value=T))
names(retest_logTr_vars) = c('var')
retest_logTr_vars$sign = ifelse(grepl(".ReflogTr",retest_logTr_vars$var), 'negative', 'positive')
retest_logTr_vars$var = gsub(".logTr|.ReflogTr", "", retest_logTr_vars$var)
retest_logTr_vars = retest_logTr_vars %>% arrange(var)
logTr_vars = logTr_vars %>% arrange(var)
retest_logTr_vars == logTr_vars
```


```{r echo=FALSE}
rm(logTr_vars, retest_logTr_vars)
```

```{r}
# Switch df names
raw_retest_data = retest_data
raw_test_data = test_data
test_data = clean_test_data
retest_data = clean_retest_data
rm(clean_test_data, clean_retest_data)
```

### Extract t-1 data for t-2 subjects
```{r}
retest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id,]

rest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id == FALSE,]

#Arrange datasets of same size by sub_id
retest_data = retest_data %>% arrange(sub_id)
retest_subs_test_data = retest_subs_test_data %>% arrange(sub_id)

#CHECK IF EVERYTHING IS ORDERED RIGHT
as.character(retest_subs_test_data$sub_id) == as.character(retest_data$sub_id)
```

# Results

## Correlations for all cleaned meaningful variables
Spearman only

```{r}
numeric_cols = c()

for(i in 1:length(names(test_data))){
  if(is.numeric(test_data[,i])){
    numeric_cols <- c(numeric_cols, names(test_data)[i])
  }
}

cor_df <- data.frame(spearman = rep(NA, length(numeric_cols)))

row.names(cor_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  cor_df[numeric_cols[i], 'spearman'] <- cor(retest_data[,numeric_cols[i]],retest_subs_test_data[,numeric_cols[i]], method = 'spearman', use = "pairwise.complete.obs")
}

# Add column to tasks vs. surveys in correlation table
cor_df$dv = row.names(cor_df)
row.names(cor_df) = seq(1:nrow(cor_df))
cor_df$task = 'task'
cor_df[grep('survey', cor_df$dv), 'task'] = 'survey'
```


Mean Spearman correlation and the number of variables for both the surveys and the tasks
```{r}
cor_df %>%
  group_by(task) %>%
  summarise(mean_spearman = mean(spearman),
            num_vars = n())
```

### Distributions of correlations by task vs. survey

Spearman correlations for surveys are noticably higher than task reliabilities.

```{r echo=FALSE}
get_labels = function(plot_df, ref_df, bin_col, group_col){
  
  plot_df$bin = as.numeric(factor(plot_df$x))
  ref_df$bin = NA
  
  for(i in 1:nrow(ref_df)){
    for(j in 1:nrow(plot_df)){
      if(ref_df[i, bin_col] <= plot_df$xmax[j] & ref_df[i, bin_col]>=plot_df$xmin[j] & ref_df[i, group_col] == plot_df[j, group_col]){
        ref_df$bin[i] <- plot_df$bin[j]
      }
    }
  }
  
  ref_df = ref_df %>% 
    group_by(bin, task) %>%
    mutate(label=as.character(toJSON(unique(dv))))
  
  label_df = ref_df[!duplicated(ref_df[,c('label')]),c('task', 'bin', 'label')]
  
  out_df = left_join(plot_df,label_df, by= c("bin", "task"))
  
  return(out_df)
}


line_break <- function(cell){
  out = paste(unlist(strsplit(gsub("\\]","",gsub("\\[","",cell)), ",")), sep="\n", collapse = "</br>")
  return(out)
}

prettify_label = function(df){
  for(i in 1:nrow(df)){
    df$label[i] =line_break(df$label[i])
  }
  return(df)
}
```

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(cor_df %>%
                        ggplot(aes(spearman, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,cor_df, "spearman", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('Spearman')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Rank order of correlations 

##### For tasks
```{r}
cor_df %>%
  filter(task == 'task') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

##### For surveys

- Demographics are excluded from this list of variables (because they are saved for predictive analyses).  
- Holt and Laury test-retest is the worst for *surveys*. But note that this is not a survey that has been studied extensively psychometrically like others. This is more similar to our tasks (one that is used more often by economists). In previous data I have worked with retest reliablities for this task was ~.35 so the results here are in line with this.  
- Otherwise the lowest survey reliabilities are 0.5.
```{r}
cor_df %>%
  filter(task == 'survey') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

## ICC for all cleaned meaningful variables

Based on [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) ICC(3,k) does not take in to account within subject differences between two time points (i.e. the fixed effect of time/systematic error). Thus, it is well approximated by Pearson's r and subject to similar criticisms. [Weir (2005)](https://pdfs.semanticscholar.org/d99a/790cce43f7f20d742f9d379b79de4f767740.pdf) suggests reporting at least this systematic error effect size if one chooses to report with ICC(3,k). Based on his conclusions here I report:  
- ICC(3,k): As Dave clarified this ranges from 1 to -1/(number of repeated measures -1) so in our case this range would be [-1, 1]; larger values would mean that the two scores of a subject for a given measure are more similar to each other than they are to scores of other people  
- partial $\eta^2$ for time ($SS_{time}/SS_{within}$): effect size of time   
- SEM ($\sqrt(MS_{error})$): standard error of measurement; the smaller the better

```{r}
match_t1_t2 <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = merge(t1_df[,c(merge_var, dv_var)], t2_df[,c(merge_var, dv_var)], by = merge_var)
  df = df %>% 
    na.omit()%>%
    gather(dv, score, -sub_id) %>%
    mutate(time = ifelse(grepl('\\.x', dv), 1, ifelse(grepl('\\.y', dv), 2, NA))) %>%
    separate(dv, c("dv", "drop"), sep='\\.([^.]*)$') %>%
    select(-drop)
   return(df)
}

get_icc <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  df = df %>% spread(time, score) %>% select(-dv, -sub_id)
  icc = ICC(df)
  icc_3k = icc$results['Average_fixed_raters', 'ICC']
  return(icc_3k)
  }

get_eta <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  mod = summary(aov(score~Error(sub_id)+time, df))
  ss_time = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq1',]
  ss_error = as.data.frame(unlist(mod$`Error: Within`))['Sum Sq2',]
  eta = ss_time/(ss_time+ss_error)
  return(eta)
  }

get_sem <- function(dv_var, t1_df = test_data, t2_df = retest_data, merge_var = 'sub_id'){
  df = match_t1_t2(dv_var, t1_df = t1_df, t2_df = t2_df, merge_var = merge_var)
  mod = summary(aov(score~Error(sub_id)+time, df))
  ms_error = as.data.frame(unlist(mod$`Error: Within`))['Mean Sq2',]
  sem = sqrt(ms_error)
  return(sem)
}
```

```{r}
#Calculate ICC df
icc_df <- data.frame(icc = rep(NA, length(numeric_cols)), 
                     eta_sq = rep(NA, length(numeric_cols)),
                     sem = rep(NA, length(numeric_cols)))

row.names(icc_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  icc_df[numeric_cols[i], 'icc'] <- get_icc(numeric_cols[i])
  icc_df[numeric_cols[i], 'eta_sq'] <- get_eta(numeric_cols[i])
  icc_df[numeric_cols[i], 'sem'] <- get_sem(numeric_cols[i])
}

icc_df$dv = row.names(icc_df)
row.names(icc_df) = seq(1:nrow(icc_df))
icc_df$task = 'task'
icc_df[grep('survey', icc_df$dv), 'task'] = 'survey'
icc_df = icc_df %>%
  select(dv, task, icc, eta_sq, sem)
row.names(icc_df) = NULL
```

Median reliability and the number of variables for both the surveys and the tasks:
As implied by Spearman the ICC's are higher for surveys.  
Systematic differences between the two measurements are higher for tasks than surveys (potentially indicating learning effects).  
Unexpectedly, median standard error of measurement is higher for surveys but the distribution below suggests that this is a side effect of having too many task measures with very low SEMs.
```{r}
icc_df %>%
  group_by(task) %>%
  summarise(median_icc = median(icc),
            median_eta_sq = median(eta_sq),
            median_sem = median(sem),
            num_vars = n())
```

### Distributions of ICC's by task vs. survey

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(icc, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "icc", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('ICC')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Rank order of ICCs

###### For tasks

```{r}
icc_df %>%
  filter(task == 'task') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-icc) %>%
  select(dv, -task, icc, eta_sq, sem, spearman)
```

###### For surveys

```{r}
icc_df %>%
  filter(task == 'survey') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-icc) %>%
  select(dv, -task, icc, eta_sq, sem, spearman)
```

### Distributions of partial $\eta^2$ by task vs. survey

Tooltip might not work well for this graph for the highest bar because the label is too large. But this bar indicates measures where the effect of time is very small anyway. We might want to pay closer attention to those not in this lowest bar since those are the variables where there is a non-negligible effect of time, indicating either a learning effect or other kind of systematic change that we did not hypothesize to observe with any of these measures.

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(eta_sq, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "eta_sq", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('Partial Eta Squared')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Rank order of partial $\eta^2$

###### For tasks

```{r}
icc_df %>%
  filter(task == 'task') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-eta_sq) %>%
  select(dv, -task, eta_sq, icc, sem, spearman)
```

###### For surveys

```{r}
icc_df %>%
  filter(task == 'survey') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-eta_sq)%>%
  select(dv, -task, eta_sq, icc, sem, spearman)
```

#### Effect of time

So what variables show a non-negligible effect of time and in what direction?

```{r warning=FALSE, message=FALSE}
ggplot_build(icc_df %>%
               ggplot(aes(eta_sq, fill=task)) +
               geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
  mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
  get_labels(.,icc_df, "eta_sq", "task") %>%
  prettify_label(.) %>%
  filter(bin != 1 & bin !=2 & !is.na(label)) %>%
  select(label)
 

anyNA("NA") 
```

### Distributions of SEM's by task vs. survey

```{r warning=FALSE, message=FALSE}
ggplotly(ggplot_build(icc_df %>%
                        ggplot(aes(sem, fill=task)) +
                        geom_histogram(position = 'identity', alpha=0.5))$data[[1]] %>%
           mutate(task=ifelse(as.character(fill) == "#00BFC4", "task", "survey")) %>%
           get_labels(.,icc_df, "sem", "task") %>%
           prettify_label(.) %>%
           ggplot(aes(x,y, fill=task, label=label))+
           geom_bar(stat='identity', position='identity', alpha=0.5)+
           xlab('SEM')+
           ylab('Count')+
           theme_bw(),
         tooltip = 'label',
         hoverinfo = 'text')
```

#### Rank order of SEMs

###### For tasks

```{r}
icc_df %>%
  filter(task == 'task') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-sem) %>%
  select(dv, -task,  sem, eta_sq, icc, spearman)
```

###### For surveys

```{r}
icc_df %>%
  filter(task == 'survey') %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  arrange(-sem)%>%
  select(dv, -task,  sem, eta_sq, icc, spearman)
```

### Negative icc's?
```{r}
neg_icc_vars = icc_df %>%
  arrange(icc) %>% 
  filter(icc<0) %>%
  select(dv)
```

Some variables (`r neg_icc_vars$dv`) have <0 ICC's. This would be the case if the $MS_{error}$>$MS_{between}$. That is, ICC would be negative if the ...

Plot data for measures with negative ICC.  
x-axis is time 1 data and y-axis is time 2 data. The scatter plots show that the scores for measures with negative ICC's have no relationship across time points.
```{r, fig.height=12, fig.width=12}
tmp = data.frame()

for(i in 1:nrow(neg_icc_vars)){
  tmp = rbind(tmp, match_t1_t2(neg_icc_vars$dv[i]))
}
rm(neg_icc_vars)

tmp %>%
  spread(time, score) %>%
  # mutate(dv_wrap = str_wrap(dv, width = 30)) %>%
  ggplot(aes(`1`, `2`))+
  geom_point()+
  theme_bw()+
  facet_wrap(~dv, scales='free', labeller = label_wrap_gen())

```

This point is clarified further by compare these scatter plots to those of variables with highest ICC. These variables have strong and clear positive linear relationships.

```{r fig.height=12, fig.width=12}
high_icc_vars = icc_df %>%
  arrange(-icc)  %>% 
  filter(icc>0.92) %>%
  select(dv)

tmp = data.frame()

for(i in 1:nrow(high_icc_vars)){
  tmp = rbind(tmp, match_t1_t2(high_icc_vars$dv[i]))
}
rm(high_icc_vars)

tmp %>%
  spread(time, score) %>%
  ggplot(aes(`1`, `2`))+
  geom_point()+
  theme_bw()+
  facet_wrap(~dv, scales='free')

```

```{r echo=FALSE}
rm(tmp)
```

### Relationship between ICC and Spearman
These two measures give mostly similar answers regarding the reliabilities of the measures except for 

```{r}
ggplotly(icc_df %>%
           left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
           ggplot(aes(spearman, icc, col=task, label=dv, label2=eta_sq))+
           geom_point()+
           theme_bw()+
           theme(legend.title = element_blank())+
           geom_abline(intercept = 0, slope=1))
```

What are the variables clustered around 0 ICC?

```{r}
icc_df %>%
  filter(icc>(-0.1), icc<0.1) %>%
  arrange(icc)
```

### Relationship between ICC and $\eta^2$

```{r}
ggplotly(icc_df %>%
  ggplot(aes(eta_sq, icc, col=task, label=dv))+
  geom_point()+
  theme_bw()+
  theme(legend.title = element_blank()))
```

Which tasks show a systematic difference between t1 and t2?
Are these the tasks where the ICC and Spearman imply different conclusions?

```{r}
icc_df %>%
  select(dv, eta_sq) %>%
  arrange(-eta_sq)
```

Ties?

"The group of zero values for the ICC may be that the ICC is more of a measure of consistency than the Spearman/Pearson measures of rank order similarity.  Or it could be the ICC calculation is off in some way. It would be worth focusing on five of these unusual cases (where the ICC is zero but the Pearson/Spearman is large) and calculate the ICC using other formulas.  Generally the Spearman and Pearson differ when there are ties.""

```{r}
ties_df <- data.frame(unique_entries = rep(NA, length(numeric_cols)), 
                     transformed = rep(NA, length(numeric_cols)))

row.names(ties_df) <- numeric_cols

for(i in 1:length(numeric_cols)){
  ties_df[numeric_cols[i], 'unique_entries'] <- get_unique(numeric_cols[i])
  ties_df[numeric_cols[i], 'transformed'] <- get_transformed(numeric_cols[i])
}

icc_df %>%
  left_join(cor_df[,c('spearman','dv')], by = 'dv') %>%
  left_join(ties_df, by = 'dv')

#Function to output: spearman, icc, number of unique values in column, whether it is log transformed

#Gina seems right; these transformed variables have high Spearman but low ICC and they have few unique values (though this isn't )
length(unique(retest_data$impulsive_venture_survey.impulsiveness.logTr))
length(unique(retest_data$adaptive_n_back.mean_load.logTr))
```

# Completion dates

Is a subject more/less consistent depending on how much time has passed in between?

How do you aggregate all this information to decide on what is a good measure?

Do any of these quantifications of reliability relate to number of items (e.g. surveys with fewer items are more or less reliable - easier to remember or noisier?)

--------------------
One reason why it's difficult to get a sense of what is going on in this paper other than just saying that questionnaires have higher reliability than tasks is because there are too many measures. While a discussion on why some measures are more reliable than others might help with this and in building readers' intuitions about better ways to measure these constructs the paper might have a larger impact andbe eeasier to digest with more of an organizational structure, parsing these variables. 
On the one hand this could be a little contrary to our general theory blind approach to these measures but on the other hand it could also help us reinforce the terminology we are converging on with the analyses of the factor structure of these data. Because at the end of the day this paper would be most useful if it can make prescriptive recommendations on which tasks to use and reduce the utilitization of non-reliable measures.