---
title: 'Self Regulation Ontology Retest Data: Meaningful Variables Clean'
output:
html_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(lme4)
library(GGally)
library(jsonlite)
library(lavaan)
library(semTools)
library(psych)
library(GPArotation)
library(rmarkdown)
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
render_this <- function(){rmarkdown::render('SRO_Meaningful_Vars.Rmd', html_notebook(toc = T, toc_float = T))}
options(digits = 4)
```

# Loading and matching datasets

### Load original data (only cleaned meaningful variables)
```{r}
test_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_01-31-2017/meaningful_variables_clean.csv')
```

### Load retest data (all variables; will subset and clean based on cleaned variables for test data)
```{r}
retest_data <- read.csv('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/variables_exhaustive.csv')
```

Function to process lookup table
```{r}
process_lookup <- function(lookup_json){
  lookup_json <- data.frame(unlist(lookup_json))
  lookup_json$sub_id <- as.character(row.names(lookup_json))
  row.names(lookup_json) <- seq(1:nrow(lookup_json))
  names(lookup_json)[1] <- 'worker_id'
  lookup_json$worker_id <- as.character(lookup_json$worker_id)
  lookup_json$complete <- ifelse(lookup_json$worker_id == lookup_json$sub_id, 0, 1)
  lookup_json <- lookup_json %>% arrange(-complete, sub_id)
  return(lookup_json)
}
```

### Extract retest participants from test data

```{r}
test_worker_lookup <- fromJSON('/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_02-11-2017/Local/worker_lookup.json')

#Process full lookup tables for both datasets
test_worker_lookup <- process_lookup(test_worker_lookup)

#Process sub_id columns in the data dataframes
retest_data$X <- as.character(retest_data$X)
test_data$X <- as.character(test_data$X)

names(retest_data)[1] <- 'sub_id'
names(test_data)[1] <- 'sub_id'

#Trim lookup table to only include those we have data for
test_worker_lookup <- test_worker_lookup[test_worker_lookup$sub_id %in% test_data$sub_id,]

#Replace worker_id's in retest_data with original sub_id's 
correct_subids <- function(row, lookup_table){
  sub_id = row$sub_id
  if(sub_id %in% lookup_table$worker_id){
    index = which(sub_id == lookup_table$worker_id)
    sub_id = lookup_table$sub_id[index]
    row$sub_id = sub_id
  }
  return(row)
}

retest_data <- retest_data %>%
  group_by(sub_id) %>%
  do(correct_subids(.,test_worker_lookup))
```

### Checks and matching issues

Are all retest subjects in test_data? No. 
```{r}
sum(retest_data$sub_id %in% test_data$sub_id) == nrow(retest_data)
```

Who is missing?
```{r}
retest_data$sub_id[which(retest_data$sub_id %in% test_data$sub_id == FALSE)]
```

One retest participant is not in test data. Remove that subject from retest_data. This participant should not have completed the retest battery to begin with because they had failed our previous criterion on number of properly completed trials. The first HIT I posted, however, had an issue with the qualifications and before I could terminate it a handful of subjects had accepted and begun the battery so we had to let them finish. So this subject has given us good data for the retest battery but worse data for the test battery.
```{r}
retest_data <- retest_data[retest_data$sub_id %in% test_data$sub_id,]

retest_data
```

Extract test data for retest subjects
```{r}
retest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id,]

rest_subs_test_data <- test_data[test_data$sub_id %in% retest_data$sub_id == FALSE,]

#Arrange datasets of same size by sub_id
retest_data = retest_data %>% arrange(sub_id)
retest_subs_test_data = retest_subs_test_data %>% arrange(sub_id)

#CHECK IF EVERYTHING IS ORDERED RIGHT
retest_subs_test_data$sub_id == retest_data$sub_id
```

```{r echo=FALSE}
rm(test_worker_lookup, correct_subids, process_lookup)
```

Get only cleaned meaningful var columns based on test data from retest data 

```{r}
#Get list of meaningful vars
meaningful_vars = names(test_data)

#Remove transofrmation suffices to be able to use for subsetting of retest data
meaningful_vars = gsub(".logTr|.ReflogTr", "", meaningful_vars)

#Get list of transformed variables and the sign of their skew
logTr_vars = data.frame(grep('logTr', names(test_data), value=T))
names(logTr_vars) = c('var')
logTr_vars$sign = ifelse(grepl(".ReflogTr",logTr_vars$var), 'negative', 'positive')
logTr_vars$var = gsub(".logTr|.ReflogTr", "", logTr_vars$var)

#Subset retest data
retest_data = retest_data[, names(retest_data) %in% meaningful_vars]
```

Clean columns subsetted from retest data the way they have been for test data   
**Question: Should I be removing outliers from the retest data? Or keep them if they were within the ranges in the test data even if they aren't in the retest data**
````{r}
#Function to remove outliers as they have been removed for the test data
remove_outliers = function(data_column, quantile_range = 2.5){
  
  q_25 = quantile(data_column, na.rm=T)[2]
  q_50 = quantile(data_column, na.rm=T)[3]
  q_75 = quantile(data_column, na.rm=T)[4]
  
  lowlimit = q_50 - quantile_range*(q_75 - q_25)
  highlimit = q_50 + quantile_range*(q_75 - q_25)
  
  data_column = ifelse(data_column<lowlimit, NA, ifelse(data_column>highlimit, NA, data_column))
  
  return(data_column)
}

#Function to transform the variables with negative skew
neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}

#Remove outliers from retest data the way they have been from test data
retest_data = cbind(sub_id = retest_data$sub_id, as.data.frame(apply(retest_data[, -which(names(retest_data) %in% c("sub_id"))], 2, remove_outliers)))

#Transform necessary vars and rename those columns
for(i in 1:length(names(retest_data))){
  tmp_col = names(retest_data)[i]
  if(tmp_col %in% logTr_vars$var){
    sign = logTr_vars$sign[which(tmp_col == logTr_vars$var)]
      if(sign == 'positive'){
        retest_data[,tmp_col] = log(retest_data[,tmp_col])
        names(retest_data)[i] = paste0(tmp_col, '.logTr')
      }
      else if(sign == 'negative'){
        retest_data[,tmp_col] = neg_log(retest_data[,tmp_col])
        names(retest_data)[i] = paste0(tmp_col, '.ReflogTr')
      }
    }
}

```

```{r echo = FALSE}
rm(i, sign, tmp_col)
```

Check transformations are correct
```{r}
retest_logTr_vars = data.frame(grep('logTr', names(retest_data), value=T))
names(retest_logTr_vars) = c('var')
retest_logTr_vars$sign = ifelse(grepl(".ReflogTr",retest_logTr_vars$var), 'negative', 'positive')
retest_logTr_vars$var = gsub(".logTr|.ReflogTr", "", logTr_vars$var)
retest_logTr_vars == logTr_vars
```

```{r echo=FALSE}
rm(logTr_vars, retest_logTr_vars, meaningful_vars)
```

# Correlations for all cleaned meaningful variables
Spearman only

```{r}
all_columns <- unique(c(names(retest_data), names(retest_subs_test_data)))

matching_columns <- c()
for(i in 1:length(all_columns)){
  if(all_columns[i] %in% names(retest_data) & all_columns[i] %in% names(retest_subs_test_data)){
    matching_columns <- c(matching_columns, all_columns[i])
  }
}

matching_dv_columns <- c()

for(i in 1:length(matching_columns)){
  if(is.numeric(retest_data[,matching_columns[i]]) & is.numeric(retest_subs_test_data[,matching_columns[i]])){
    matching_dv_columns <- c(matching_dv_columns, matching_columns[i])
  }
}

cor_df <- data.frame(spearman = rep(NA, length(matching_dv_columns)))

row.names(cor_df) <- matching_dv_columns

for(i in 1:length(matching_dv_columns)){
  cor_df[matching_dv_columns[i], 'spearman'] <- cor(retest_data[,matching_dv_columns[i]],retest_subs_test_data[,matching_dv_columns[i]], method = 'spearman', use = "pairwise.complete.obs")
}
```

Distribution of all Pearson and Spearman correlations
```{r warning=FALSE, message=FALSE}
cor_df %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  ggtitle('Distribution of Spearman correlations for cleaned meaningful variables')
```

Add column to tasks vs. surveys in correlation table

```{r}
cor_df$dv = row.names(cor_df)
row.names(cor_df) = seq(1:nrow(cor_df))
cor_df$task = 'task'
cor_df[grep('survey', cor_df$dv), 'task'] = 'survey'
```

## Distributions of correlations by task vs. survey

Survey reliabilities are noticably higher than task reliabilities.
```{r warning=FALSE, message=FALSE}
cor_df %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~task)
```

Mean reliability and the number of variables for both the surveys and the tasks
```{r}
cor_df %>%
  group_by(task) %>%
  summarise(mean_spearman = mean(spearman),
            num_vars = n())
```

### Rank order of correlations 

#### For tasks
```{r}
cor_df %>%
  filter(task == 'task') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

#### For surveys

- Demographics are excluded from this list of variables.  
- Holt and Laury test-retest is the worst for *surveys*. But note that this is not a survey that has been studied extensively psychometrically like others. This is more similar to out tasks (one that is used more often by economists). In previous data I have worked retest reliablities for this task was ~.35 so the results here are in line with this.  
- Otherwise the lowest survey reliabilities are 0.5.
```{r}
cor_df %>%
  filter(task == 'survey') %>%
  select(dv, spearman) %>%
  arrange(-spearman)
```

## Distribution of each variable

Keeping a record of the distributions of the cleaned meaningful variables.

```{r}
# Function to check if the variables that went in to cor_df have normal distributions
diff_from_normal <- function(col){
  return(shapiro.test(col)$p.value < 0.05)
}

# Create and organize new df where distribution of vars data is stored. First for retest data only; then for test data for subjects who have retest data - restricting here since these are the subsets of data that went in to the correlation matrix. Merge and cleanup.

dist_retest_data <- as.data.frame(apply(retest_data[,matching_dv_columns], 2, diff_from_normal))
names(dist_retest_data) <- c('retest_not_normal')
dist_retest_data$dv <- row.names(dist_retest_data)
row.names(dist_retest_data) <- seq(1:nrow(dist_retest_data))
dist_retest_data = dist_retest_data %>% select(dv, retest_not_normal)

dist_retest_subs_test_data <- as.data.frame(apply(retest_subs_test_data[,matching_dv_columns], 2, diff_from_normal))
names(dist_retest_subs_test_data) <- c('test_not_normal')
dist_retest_subs_test_data$dv <- row.names(dist_retest_subs_test_data)
row.names(dist_retest_subs_test_data) <- seq(1:nrow(dist_retest_subs_test_data))
dist_retest_subs_test_data = dist_retest_subs_test_data %>% select(dv, test_not_normal)

dist_data <- merge(dist_retest_data, dist_retest_subs_test_data, by = 'dv')

rm(dist_retest_data, dist_retest_subs_test_data)

# Add whether dv's are tasks or survey to distribution dfs

dist_data = cor_df %>%
  select(dv, task) %>%
  left_join(dist_data, by = 'dv')
```

**What proportion is not normal?**  
A large proportion of the variables are not normal for data from both sources. Larger proportions of survey data are not normal compared to task data. 
```{r}
dist_data %>%
  group_by(task) %>%
  summarise(mean_retest_not_normal  = mean(retest_not_normal),
            mean_test_not_normal = mean(test_not_normal))
```

**Are the same vars that are not normally distributed in the retest data not normal in test data as well?**  
List of variables that have different distributions in the two time points.
```{r}
dist_data %>%
  filter(retest_not_normal != test_not_normal)
```

# Different task variable types

Mean reliabilities for different types of dependant variables

## Drift diffusion variables: raw vars vs. EZ vs. hddm

We should probably take a closer look at model fits instead but this could be a first step.  

The cleaned meaningful variables do not seem to include neither the EZ estimates nor accuracies
so analyses on this [here](https://htmlpreview.github.io/?https://github.com/zenkavi/SRO_Retest_Analyses/blob/master/SRO_Retest_Analyses.nb.html) are more informative. Otherwise just looking at this one might be tempted to conclude that rt's have higher reliabilities but clearly 

```{r message=FALSE, warning=FALSE, fig.height=12, fig.width=9}
cor_df%>%
  filter(grepl('EZ|hddm|_rt|_acc', dv)) %>%
  mutate(var_type = ifelse(grepl('EZ',dv), 'EZ', ifelse(grepl('hddm',dv), 'hddm', 'raw')),
         var = ifelse(grepl('_rt', dv), 'rt', ifelse(grepl('_acc', dv), 'acc', ifelse(grepl('_non_decision', dv), 'non_decision', ifelse(grepl('_thresh', dv), 'thresh', ifelse(grepl('_drift', dv), 'drift', NA)))))) %>%
  ggplot(aes(spearman))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(var_type~var)
```

```{r}
cor_df%>%
  filter(grepl('EZ|hddm|_rt|_acc', dv)) %>%
  mutate(var_type = ifelse(grepl('EZ',dv), 'EZ', ifelse(grepl('hddm',dv), 'hddm', 'raw')),
         var = ifelse(grepl('_rt', dv), 'rt', ifelse(grepl('_acc', dv), 'acc', ifelse(grepl('_non_decision', dv), 'non_decision', ifelse(grepl('_thresh', dv), 'thresh', ifelse(grepl('_drift', dv), 'drift', NA)))))) %>%
  group_by(var_type, var) %>%
  summarise(mean_spearman = mean(spearman, na.rm=T),
            sem_spear = sem(spearman),
            num_vars = sum(ifelse(is.na(spearman)==FALSE,1,0))) %>%
  arrange(var_type)
```

# Replicating the factor structure

## Confirm comparability of retest and rest sample

First do a t-test on all measures of T1 for subjects with and without retest to confirm that these two samples do not differ from each other significantly (FDR corrected)
```{r}
ttest_df = data.frame(t_stat = rep(NA, length(matching_dv_columns)),
                      p_val_raw = rep(NA, length(matching_dv_columns)))

row.names(ttest_df) <- matching_dv_columns

for(i in 1:length(matching_dv_columns)){
  ttest_df[matching_dv_columns[i], 't_stat'] <- t.test(retest_subs_test_data[,matching_dv_columns[i]], rest_subs_test_data[,matching_dv_columns[i]])$statistic
  
  ttest_df[matching_dv_columns[i], 'p_val_raw'] <- t.test(retest_subs_test_data[,matching_dv_columns[i]], rest_subs_test_data[,matching_dv_columns[i]])$p.value
}

ttest_df$dv = row.names(ttest_df)
row.names(ttest_df) = seq(1:nrow(ttest_df))
ttest_df$task = 'task'
ttest_df[grep('survey', ttest_df$dv), 'task'] = 'survey'

#correct p-values 
ttest_df$p_val_fdr = p.adjust(ttest_df$p_val_raw, method='fdr')
```

```{r echo=FALSE}
rm(i)
```

Distribution of t statistics and p values

```{r warning=FALSE, message=FALSE}
ttest_df %>%
  gather(key, value, -dv, -task) %>%
  ggplot(aes(value)) +
  geom_histogram()+
  theme_bw()+
  facet_wrap(key~task, scales='free')
```

**Which variables are significantly different between the two samples (those with and without retest data for T1)?**

13 variables have raw p values <0.05 BUT FDR corrected p values for these are not <0.05.

```{r}
ttest_df %>%
  filter(p_val_raw<0.05) %>%
  select(dv, task, t_stat, p_val_raw, p_val_fdr)
```

## Efa and Cfa

Efa on the people you don't have retest on and then predict how well this holds up for people w retest on t1 and on t2.  

I haven't consulted other factor analyses that have been done on the full dataset in doing this but will be happy to adjust as necessary.

```{r}
retest_data_mngf <- retest_data[,matching_dv_columns]
retest_subs_test_mngf <- retest_subs_test_data[,matching_dv_columns]
rest_subs_test_mngf <- rest_subs_test_data[,matching_dv_columns]

#standardize all vars going in to factor analysis
rest_subs_test_mngf_z <- rest_subs_test_mngf

for(i in 1:length(rest_subs_test_mngf)){
    rest_subs_test_mngf_z[,names(rest_subs_test_mngf)[i]] = scale(rest_subs_test_mngf[,i])
}

rest_subs_test_mngf_z <- as.data.frame(rest_subs_test_mngf_z)
```

Efa on subjects from original dataset withOUT retest data (using meaningful vars only)  
Using the `psych` package I get some sort of solution but these are probably not valid given all the warnings.
```{r}
corMat <- rest_subs_test_mngf_z %>% cor(.,use = 'complete.obs')

solution <- fa(r = corMat, nfactors = 7, rotate = "oblimin", fm = "pa")
```

```{r}
print(solution$loadings, sort=T)
```

```{r}
# Pricipal Components Analysis
# entering raw data and extracting PCs 
# from the correlation matrix 
fit <- principal(rest_subs_test_mngf_z, nfactors=7, rotate="varimax")
fit

fit <- princomp(rest_subs_test_mngf_z, cor=TRUE)
summary(fit) # print variance accounted for 
loadings(fit) # pc loadings 
plot(fit,type="lines") # scree plot 
fit$scores # the principal components
biplot(fit)
```

